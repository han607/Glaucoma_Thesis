{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras.preprocessing as pre\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "\n",
    "# Importing the dataset\n",
    "X_dataset = pd.read_csv('dataset_without_intervals.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_eyeD</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>eye</th>\n",
       "      <th>number _visits</th>\n",
       "      <th>primary_dx</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>...</th>\n",
       "      <th>rnfl_inf</th>\n",
       "      <th>rnfl_tempinf</th>\n",
       "      <th>md</th>\n",
       "      <th>p_md</th>\n",
       "      <th>psd</th>\n",
       "      <th>p_psd</th>\n",
       "      <th>vfi</th>\n",
       "      <th>age</th>\n",
       "      <th>days_from_baseline</th>\n",
       "      <th>visit_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4OD</td>\n",
       "      <td>4</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-03-15</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>0.372392</td>\n",
       "      <td>5</td>\n",
       "      <td>2.041738</td>\n",
       "      <td>3</td>\n",
       "      <td>94.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4OD</td>\n",
       "      <td>4</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-04-04</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>0.457088</td>\n",
       "      <td>3</td>\n",
       "      <td>2.588213</td>\n",
       "      <td>5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4OD</td>\n",
       "      <td>4</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>0.582103</td>\n",
       "      <td>2</td>\n",
       "      <td>1.905461</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>763.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4OD</td>\n",
       "      <td>4</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>0.407380</td>\n",
       "      <td>4</td>\n",
       "      <td>2.142891</td>\n",
       "      <td>4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2009-03-30</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>0.141906</td>\n",
       "      <td>5</td>\n",
       "      <td>3.749730</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-03-15</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>0.020845</td>\n",
       "      <td>5</td>\n",
       "      <td>5.093309</td>\n",
       "      <td>5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-04-04</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>0.128825</td>\n",
       "      <td>5</td>\n",
       "      <td>2.824880</td>\n",
       "      <td>5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>0.201372</td>\n",
       "      <td>5</td>\n",
       "      <td>2.798981</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>0.104232</td>\n",
       "      <td>5</td>\n",
       "      <td>3.273407</td>\n",
       "      <td>5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4OS</td>\n",
       "      <td>4</td>\n",
       "      <td>OS</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-01-07</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>0.106905</td>\n",
       "      <td>1</td>\n",
       "      <td>4.265795</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2913.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>14OD</td>\n",
       "      <td>14</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>5</td>\n",
       "      <td>11.561122</td>\n",
       "      <td>5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>14OD</td>\n",
       "      <td>14</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-09-23</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0.146893</td>\n",
       "      <td>5</td>\n",
       "      <td>19.230917</td>\n",
       "      <td>5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>14OD</td>\n",
       "      <td>14</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0.127350</td>\n",
       "      <td>1</td>\n",
       "      <td>12.502590</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>14OS</td>\n",
       "      <td>14</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-09-25</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>0.696627</td>\n",
       "      <td>0</td>\n",
       "      <td>1.435489</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14OS</td>\n",
       "      <td>14</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-04-02</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0.359749</td>\n",
       "      <td>5</td>\n",
       "      <td>2.218196</td>\n",
       "      <td>4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>14OS</td>\n",
       "      <td>14</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-11-21</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>3</td>\n",
       "      <td>2.027683</td>\n",
       "      <td>3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>14OS</td>\n",
       "      <td>14</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-09-23</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>0.557186</td>\n",
       "      <td>2</td>\n",
       "      <td>1.721869</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>14OS</td>\n",
       "      <td>14</td>\n",
       "      <td>OS</td>\n",
       "      <td>5</td>\n",
       "      <td>GL</td>\n",
       "      <td>1957-01-25</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>0.463447</td>\n",
       "      <td>2</td>\n",
       "      <td>1.794734</td>\n",
       "      <td>2</td>\n",
       "      <td>97.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18OD</td>\n",
       "      <td>18</td>\n",
       "      <td>OD</td>\n",
       "      <td>5</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2009-06-08</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>1.172195</td>\n",
       "      <td>0</td>\n",
       "      <td>1.458814</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>18OD</td>\n",
       "      <td>18</td>\n",
       "      <td>OD</td>\n",
       "      <td>5</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-06-14</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>1.166810</td>\n",
       "      <td>0</td>\n",
       "      <td>1.448772</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>18OD</td>\n",
       "      <td>18</td>\n",
       "      <td>OD</td>\n",
       "      <td>5</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-06-23</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>1.227439</td>\n",
       "      <td>0</td>\n",
       "      <td>1.595879</td>\n",
       "      <td>2</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>18OD</td>\n",
       "      <td>18</td>\n",
       "      <td>OD</td>\n",
       "      <td>5</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>0.944061</td>\n",
       "      <td>0</td>\n",
       "      <td>1.399587</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>18OD</td>\n",
       "      <td>18</td>\n",
       "      <td>OD</td>\n",
       "      <td>5</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>23</td>\n",
       "      <td>1.106624</td>\n",
       "      <td>0</td>\n",
       "      <td>1.336596</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>18OS</td>\n",
       "      <td>18</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-06-14</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>1.148154</td>\n",
       "      <td>0</td>\n",
       "      <td>1.770109</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>18OS</td>\n",
       "      <td>18</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-06-23</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0.901571</td>\n",
       "      <td>0</td>\n",
       "      <td>1.442115</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>18OS</td>\n",
       "      <td>18</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-06-28</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>0.905733</td>\n",
       "      <td>0</td>\n",
       "      <td>1.733804</td>\n",
       "      <td>2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>18OS</td>\n",
       "      <td>18</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1962-09-04</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>1.006932</td>\n",
       "      <td>0</td>\n",
       "      <td>1.419058</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>23OD</td>\n",
       "      <td>23</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-02-23</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-10-03</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0.977237</td>\n",
       "      <td>0</td>\n",
       "      <td>1.273503</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>23OD</td>\n",
       "      <td>23</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-02-23</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-04-16</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>1.253141</td>\n",
       "      <td>0</td>\n",
       "      <td>1.241652</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>23OD</td>\n",
       "      <td>23</td>\n",
       "      <td>OD</td>\n",
       "      <td>4</td>\n",
       "      <td>GL</td>\n",
       "      <td>1946-02-23</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-10-22</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>1.393157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.425608</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>5451</td>\n",
       "      <td>3511OS</td>\n",
       "      <td>3511</td>\n",
       "      <td>OS</td>\n",
       "      <td>3</td>\n",
       "      <td>GS</td>\n",
       "      <td>1940-03-14</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>0.660693</td>\n",
       "      <td>1</td>\n",
       "      <td>1.811340</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5255</th>\n",
       "      <td>5452</td>\n",
       "      <td>3511OS</td>\n",
       "      <td>3511</td>\n",
       "      <td>OS</td>\n",
       "      <td>3</td>\n",
       "      <td>GS</td>\n",
       "      <td>1940-03-14</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>30</td>\n",
       "      <td>0.532108</td>\n",
       "      <td>3</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1840.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5256</th>\n",
       "      <td>5453</td>\n",
       "      <td>3514OD</td>\n",
       "      <td>3514</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>5</td>\n",
       "      <td>4.405549</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>5454</td>\n",
       "      <td>3514OD</td>\n",
       "      <td>3514</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-09-19</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>5</td>\n",
       "      <td>4.111497</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>5455</td>\n",
       "      <td>3514OD</td>\n",
       "      <td>3514</td>\n",
       "      <td>OD</td>\n",
       "      <td>3</td>\n",
       "      <td>GL</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>5</td>\n",
       "      <td>4.055085</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5259</th>\n",
       "      <td>5456</td>\n",
       "      <td>3514OS</td>\n",
       "      <td>3514</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-09-24</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>0.814704</td>\n",
       "      <td>0</td>\n",
       "      <td>1.770109</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>5457</td>\n",
       "      <td>3514OS</td>\n",
       "      <td>3514</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-02-25</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>0.508159</td>\n",
       "      <td>3</td>\n",
       "      <td>1.603245</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5261</th>\n",
       "      <td>5458</td>\n",
       "      <td>3514OS</td>\n",
       "      <td>3514</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-09-19</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.450817</td>\n",
       "      <td>3</td>\n",
       "      <td>1.659587</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5262</th>\n",
       "      <td>5459</td>\n",
       "      <td>3514OS</td>\n",
       "      <td>3514</td>\n",
       "      <td>OS</td>\n",
       "      <td>4</td>\n",
       "      <td>GS</td>\n",
       "      <td>1942-02-14</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-03-22</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>0.504661</td>\n",
       "      <td>3</td>\n",
       "      <td>1.485936</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263</th>\n",
       "      <td>5460</td>\n",
       "      <td>3516OD</td>\n",
       "      <td>3516</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2009-04-28</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>0.788860</td>\n",
       "      <td>0</td>\n",
       "      <td>1.892344</td>\n",
       "      <td>3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5264</th>\n",
       "      <td>5461</td>\n",
       "      <td>3516OD</td>\n",
       "      <td>3516</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-05-11</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>0.803526</td>\n",
       "      <td>0</td>\n",
       "      <td>1.990673</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>5462</td>\n",
       "      <td>3516OS</td>\n",
       "      <td>3516</td>\n",
       "      <td>OS</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2009-04-28</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>1.191242</td>\n",
       "      <td>0</td>\n",
       "      <td>1.506607</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>5463</td>\n",
       "      <td>3516OS</td>\n",
       "      <td>3516</td>\n",
       "      <td>OS</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1944-03-01</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2010-05-11</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>1.191242</td>\n",
       "      <td>0</td>\n",
       "      <td>1.559553</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>5464</td>\n",
       "      <td>3518OD</td>\n",
       "      <td>3518</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GS</td>\n",
       "      <td>1967-03-12</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>0.207970</td>\n",
       "      <td>5</td>\n",
       "      <td>1.324342</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>5465</td>\n",
       "      <td>3518OD</td>\n",
       "      <td>3518</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GS</td>\n",
       "      <td>1967-03-12</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-10-20</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>0.279254</td>\n",
       "      <td>5</td>\n",
       "      <td>1.291219</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5269</th>\n",
       "      <td>5466</td>\n",
       "      <td>3520OD</td>\n",
       "      <td>3520</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1937-03-09</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>0.474242</td>\n",
       "      <td>3</td>\n",
       "      <td>1.555966</td>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>5467</td>\n",
       "      <td>3520OD</td>\n",
       "      <td>3520</td>\n",
       "      <td>OD</td>\n",
       "      <td>2</td>\n",
       "      <td>GL</td>\n",
       "      <td>1937-03-09</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-09-11</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.310456</td>\n",
       "      <td>5</td>\n",
       "      <td>2.108628</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>5468</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-11-04</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>0.628058</td>\n",
       "      <td>1</td>\n",
       "      <td>2.506109</td>\n",
       "      <td>5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>5469</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>0.521195</td>\n",
       "      <td>3</td>\n",
       "      <td>3.548134</td>\n",
       "      <td>5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>5470</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-11-26</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>0.431519</td>\n",
       "      <td>4</td>\n",
       "      <td>2.685344</td>\n",
       "      <td>5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>5471</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.369828</td>\n",
       "      <td>5</td>\n",
       "      <td>3.953666</td>\n",
       "      <td>5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>5472</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>0.415911</td>\n",
       "      <td>4</td>\n",
       "      <td>5.767665</td>\n",
       "      <td>5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>5473</td>\n",
       "      <td>3521OD</td>\n",
       "      <td>3521</td>\n",
       "      <td>OD</td>\n",
       "      <td>6</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0.527230</td>\n",
       "      <td>3</td>\n",
       "      <td>4.027170</td>\n",
       "      <td>5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>5474</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2011-11-04</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>1.104079</td>\n",
       "      <td>0</td>\n",
       "      <td>1.462177</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>5475</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>1.044720</td>\n",
       "      <td>0</td>\n",
       "      <td>1.380384</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>5476</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2012-11-26</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>0.952796</td>\n",
       "      <td>0</td>\n",
       "      <td>1.367729</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>5477</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-06-03</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>0.937562</td>\n",
       "      <td>0</td>\n",
       "      <td>1.412538</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>5478</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2013-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>0.941890</td>\n",
       "      <td>0</td>\n",
       "      <td>1.396368</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>5479</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>0.839460</td>\n",
       "      <td>0</td>\n",
       "      <td>1.402814</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>5480</td>\n",
       "      <td>3521OS</td>\n",
       "      <td>3521</td>\n",
       "      <td>OS</td>\n",
       "      <td>7</td>\n",
       "      <td>GL</td>\n",
       "      <td>1952-12-19</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>1.188502</td>\n",
       "      <td>0</td>\n",
       "      <td>1.318257</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5284 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 subject_eyeD  subject_id eye  number _visits primary_dx  \\\n",
       "0              0          4OD           4  OD               4         GL   \n",
       "1              1          4OD           4  OD               4         GL   \n",
       "2              2          4OD           4  OD               4         GL   \n",
       "3              3          4OD           4  OD               4         GL   \n",
       "4              4          4OS           4  OS               6         GL   \n",
       "5              5          4OS           4  OS               6         GL   \n",
       "6              6          4OS           4  OS               6         GL   \n",
       "7              7          4OS           4  OS               6         GL   \n",
       "8              8          4OS           4  OS               6         GL   \n",
       "9              9          4OS           4  OS               6         GL   \n",
       "10            10         14OD          14  OD               3         GL   \n",
       "11            11         14OD          14  OD               3         GL   \n",
       "12            12         14OD          14  OD               3         GL   \n",
       "13            13         14OS          14  OS               5         GL   \n",
       "14            14         14OS          14  OS               5         GL   \n",
       "15            15         14OS          14  OS               5         GL   \n",
       "16            16         14OS          14  OS               5         GL   \n",
       "17            17         14OS          14  OS               5         GL   \n",
       "18            18         18OD          18  OD               5         GS   \n",
       "19            19         18OD          18  OD               5         GS   \n",
       "20            20         18OD          18  OD               5         GS   \n",
       "21            21         18OD          18  OD               5         GS   \n",
       "22            22         18OD          18  OD               5         GS   \n",
       "23            23         18OS          18  OS               4         GS   \n",
       "24            24         18OS          18  OS               4         GS   \n",
       "25            25         18OS          18  OS               4         GS   \n",
       "26            26         18OS          18  OS               4         GS   \n",
       "27            27         23OD          23  OD               4         GL   \n",
       "28            28         23OD          23  OD               4         GL   \n",
       "29            29         23OD          23  OD               4         GL   \n",
       "...          ...          ...         ...  ..             ...        ...   \n",
       "5254        5451       3511OS        3511  OS               3         GS   \n",
       "5255        5452       3511OS        3511  OS               3         GS   \n",
       "5256        5453       3514OD        3514  OD               3         GL   \n",
       "5257        5454       3514OD        3514  OD               3         GL   \n",
       "5258        5455       3514OD        3514  OD               3         GL   \n",
       "5259        5456       3514OS        3514  OS               4         GS   \n",
       "5260        5457       3514OS        3514  OS               4         GS   \n",
       "5261        5458       3514OS        3514  OS               4         GS   \n",
       "5262        5459       3514OS        3514  OS               4         GS   \n",
       "5263        5460       3516OD        3516  OD               2         GL   \n",
       "5264        5461       3516OD        3516  OD               2         GL   \n",
       "5265        5462       3516OS        3516  OS               2         GL   \n",
       "5266        5463       3516OS        3516  OS               2         GL   \n",
       "5267        5464       3518OD        3518  OD               2         GS   \n",
       "5268        5465       3518OD        3518  OD               2         GS   \n",
       "5269        5466       3520OD        3520  OD               2         GL   \n",
       "5270        5467       3520OD        3520  OD               2         GL   \n",
       "5271        5468       3521OD        3521  OD               6         GL   \n",
       "5272        5469       3521OD        3521  OD               6         GL   \n",
       "5273        5470       3521OD        3521  OD               6         GL   \n",
       "5274        5471       3521OD        3521  OD               6         GL   \n",
       "5275        5472       3521OD        3521  OD               6         GL   \n",
       "5276        5473       3521OD        3521  OD               6         GL   \n",
       "5277        5474       3521OS        3521  OS               7         GL   \n",
       "5278        5475       3521OS        3521  OS               7         GL   \n",
       "5279        5476       3521OS        3521  OS               7         GL   \n",
       "5280        5477       3521OS        3521  OS               7         GL   \n",
       "5281        5478       3521OS        3521  OS               7         GL   \n",
       "5282        5479       3521OS        3521  OS               7         GL   \n",
       "5283        5480       3521OS        3521  OS               7         GL   \n",
       "\n",
       "             dob  gender   race  visit_date      ...      rnfl_inf  \\\n",
       "0     1946-01-07    male  white  2010-03-15      ...            29   \n",
       "1     1946-01-07    male  white  2011-04-04      ...            29   \n",
       "2     1946-01-07    male  white  2012-04-16      ...            28   \n",
       "3     1946-01-07    male  white  2013-01-14      ...            29   \n",
       "4     1946-01-07    male  white  2009-03-30      ...            23   \n",
       "5     1946-01-07    male  white  2010-03-15      ...            23   \n",
       "6     1946-01-07    male  white  2011-04-04      ...            23   \n",
       "7     1946-01-07    male  white  2012-04-16      ...            23   \n",
       "8     1946-01-07    male  white  2013-01-14      ...            23   \n",
       "9     1946-01-07    male  white  2017-03-21      ...            23   \n",
       "10    1957-01-25  female  white  2013-11-21      ...            26   \n",
       "11    1957-01-25  female  white  2014-09-23      ...            30   \n",
       "12    1957-01-25  female  white  2017-01-09      ...            25   \n",
       "13    1957-01-25  female  white  2012-09-25      ...            31   \n",
       "14    1957-01-25  female  white  2013-04-02      ...            31   \n",
       "15    1957-01-25  female  white  2013-11-21      ...            30   \n",
       "16    1957-01-25  female  white  2014-09-23      ...            31   \n",
       "17    1957-01-25  female  white  2017-01-09      ...            33   \n",
       "18    1962-09-04  female  white  2009-06-08      ...            38   \n",
       "19    1962-09-04  female  white  2010-06-14      ...            38   \n",
       "20    1962-09-04  female  white  2011-06-23      ...            37   \n",
       "21    1962-09-04  female  white  2012-06-28      ...            37   \n",
       "22    1962-09-04  female  white  2013-07-11      ...            37   \n",
       "23    1962-09-04  female  white  2010-06-14      ...            34   \n",
       "24    1962-09-04  female  white  2011-06-23      ...            33   \n",
       "25    1962-09-04  female  white  2012-06-28      ...            32   \n",
       "26    1962-09-04  female  white  2013-07-11      ...            32   \n",
       "27    1946-02-23    male  white  2013-10-03      ...            31   \n",
       "28    1946-02-23    male  white  2014-04-16      ...            31   \n",
       "29    1946-02-23    male  white  2014-10-22      ...            32   \n",
       "...          ...     ...    ...         ...      ...           ...   \n",
       "5254  1940-03-14  female  white  2013-03-26      ...            48   \n",
       "5255  1940-03-14  female  white  2014-04-17      ...            48   \n",
       "5256  1942-02-14    male  white  2010-09-24      ...            11   \n",
       "5257  1942-02-14    male  white  2011-09-19      ...             7   \n",
       "5258  1942-02-14    male  white  2012-03-22      ...             9   \n",
       "5259  1942-02-14    male  white  2010-09-24      ...            38   \n",
       "5260  1942-02-14    male  white  2011-02-25      ...            41   \n",
       "5261  1942-02-14    male  white  2011-09-19      ...            40   \n",
       "5262  1942-02-14    male  white  2012-03-22      ...            40   \n",
       "5263  1944-03-01    male  white  2009-04-28      ...            31   \n",
       "5264  1944-03-01    male  white  2010-05-11      ...            31   \n",
       "5265  1944-03-01    male  white  2009-04-28      ...            33   \n",
       "5266  1944-03-01    male  white  2010-05-11      ...            33   \n",
       "5267  1967-03-12    male  white  2013-09-30      ...            27   \n",
       "5268  1967-03-12    male  white  2014-10-20      ...            29   \n",
       "5269  1937-03-09    male  white  2013-05-31      ...            46   \n",
       "5270  1937-03-09    male  white  2014-09-11      ...            40   \n",
       "5271  1952-12-19  female  white  2011-11-04      ...            27   \n",
       "5272  1952-12-19  female  white  2012-05-23      ...            27   \n",
       "5273  1952-12-19  female  white  2012-11-26      ...            28   \n",
       "5274  1952-12-19  female  white  2013-12-16      ...            26   \n",
       "5275  1952-12-19  female  white  2014-09-15      ...            27   \n",
       "5276  1952-12-19  female  white  2015-06-01      ...            26   \n",
       "5277  1952-12-19  female  white  2011-11-04      ...            34   \n",
       "5278  1952-12-19  female  white  2012-05-23      ...            33   \n",
       "5279  1952-12-19  female  white  2012-11-26      ...            33   \n",
       "5280  1952-12-19  female  white  2013-06-03      ...            33   \n",
       "5281  1952-12-19  female  white  2013-12-16      ...            34   \n",
       "5282  1952-12-19  female  white  2014-09-15      ...            33   \n",
       "5283  1952-12-19  female  white  2015-06-01      ...            34   \n",
       "\n",
       "      rnfl_tempinf        md  p_md        psd  p_psd    vfi   age  \\\n",
       "0               20  0.372392     5   2.041738      3   94.0  64.0   \n",
       "1               20  0.457088     3   2.588213      5   94.0  65.0   \n",
       "2               19  0.582103     2   1.905461      3   96.0  66.0   \n",
       "3               20  0.407380     4   2.142891      4   95.0  67.0   \n",
       "4               18  0.141906     5   3.749730      5   83.0  63.0   \n",
       "5               15  0.020845     5   5.093309      5   54.0  64.0   \n",
       "6               14  0.128825     5   2.824880      5   85.0  65.0   \n",
       "7               15  0.201372     5   2.798981      5   83.0  66.0   \n",
       "8               17  0.104232     5   3.273407      5   76.0  67.0   \n",
       "9               15  0.106905     1   4.265795      1   78.0  71.0   \n",
       "10              17  0.143219     5  11.561122      5   82.0  56.0   \n",
       "11              23  0.146893     5  19.230917      5   78.0  57.0   \n",
       "12              16  0.127350     1  12.502590      1   83.0  59.0   \n",
       "13              21  0.696627     0   1.435489      0   99.0  55.0   \n",
       "14              22  0.359749     5   2.218196      4   93.0  56.0   \n",
       "15              21  0.523600     3   2.027683      3   95.0  56.0   \n",
       "16              23  0.557186     2   1.721869      2   96.0  57.0   \n",
       "17              20  0.463447     2   1.794734      2   97.0  59.0   \n",
       "18              24  1.172195     0   1.458814      0  100.0  46.0   \n",
       "19              23  1.166810     0   1.448772      0  100.0  47.0   \n",
       "20              22  1.227439     0   1.595879      2   99.0  48.0   \n",
       "21              23  0.944061     0   1.399587      0  100.0  49.0   \n",
       "22              23  1.106624     0   1.336596      0  100.0  50.0   \n",
       "23              22  1.148154     0   1.770109      3   98.0  47.0   \n",
       "24              22  0.901571     0   1.442115      0   98.0  48.0   \n",
       "25              21  0.905733     0   1.733804      2   98.0  49.0   \n",
       "26              23  1.006932     0   1.419058      0  100.0  50.0   \n",
       "27              22  0.977237     0   1.273503      0  100.0  67.0   \n",
       "28              22  1.253141     0   1.241652      0  100.0  68.0   \n",
       "29              23  1.393157     0   1.425608      0  100.0  68.0   \n",
       "...            ...       ...   ...        ...    ...    ...   ...   \n",
       "5254            29  0.660693     1   1.811340      3   96.0  73.0   \n",
       "5255            30  0.532108     3   1.909853      3   95.0  74.0   \n",
       "5256            14  0.001107     5   4.405549      5   12.0  68.0   \n",
       "5257            12  0.001219     5   4.111497      5   12.0  69.0   \n",
       "5258            14  0.001064     5   4.055085      5   11.0  70.0   \n",
       "5259            26  0.814704     0   1.770109      3   98.0  68.0   \n",
       "5260            27  0.508159     3   1.603245      2   96.0  69.0   \n",
       "5261            27  0.450817     3   1.659587      2   96.0  69.0   \n",
       "5262            26  0.504661     3   1.485936      0   98.0  70.0   \n",
       "5263            22  0.788860     0   1.892344      3   97.0  65.0   \n",
       "5264            21  0.803526     0   1.990673      3   96.0  66.0   \n",
       "5265            20  1.191242     0   1.506607      1  100.0  65.0   \n",
       "5266            21  1.191242     0   1.559553      1   99.0  66.0   \n",
       "5267            20  0.207970     5   1.324342      0  100.0  46.0   \n",
       "5268            21  0.279254     5   1.291219      0  100.0  47.0   \n",
       "5269            28  0.474242     3   1.555966      1   96.0  76.0   \n",
       "5270            25  0.310456     5   2.108628      4   97.0  77.0   \n",
       "5271            16  0.628058     1   2.506109      5   97.0  58.0   \n",
       "5272            17  0.521195     3   3.548134      5   95.0  59.0   \n",
       "5273            18  0.431519     4   2.685344      5   92.0  59.0   \n",
       "5274            16  0.369828     5   3.953666      5   92.0  60.0   \n",
       "5275            18  0.415911     4   5.767665      5   93.0  61.0   \n",
       "5276            16  0.527230     3   4.027170      5   93.0  62.0   \n",
       "5277            24  1.104079     0   1.462177      0   99.0  58.0   \n",
       "5278            25  1.044720     0   1.380384      0   99.0  59.0   \n",
       "5279            23  0.952796     0   1.367729      0   99.0  59.0   \n",
       "5280            24  0.937562     0   1.412538      0   99.0  60.0   \n",
       "5281            25  0.941890     0   1.396368      0   99.0  60.0   \n",
       "5282            24  0.839460     0   1.402814      0   99.0  61.0   \n",
       "5283            25  1.188502     0   1.318257      0  100.0  62.0   \n",
       "\n",
       "      days_from_baseline  visit_number  \n",
       "0                    0.0             1  \n",
       "1                  385.0             2  \n",
       "2                  763.0             3  \n",
       "3                 1036.0             4  \n",
       "4                    0.0             1  \n",
       "5                  350.0             2  \n",
       "6                  735.0             3  \n",
       "7                 1113.0             4  \n",
       "8                 1386.0             5  \n",
       "9                 2913.0             6  \n",
       "10                   0.0             1  \n",
       "11                 306.0             2  \n",
       "12                1145.0             3  \n",
       "13                   0.0             1  \n",
       "14                 189.0             2  \n",
       "15                 422.0             3  \n",
       "16                 728.0             4  \n",
       "17                1567.0             5  \n",
       "18                   0.0             1  \n",
       "19                 371.0             2  \n",
       "20                 745.0             3  \n",
       "21                1116.0             4  \n",
       "22                1494.0             5  \n",
       "23                   0.0             1  \n",
       "24                 374.0             2  \n",
       "25                 745.0             3  \n",
       "26                1123.0             4  \n",
       "27                   0.0             1  \n",
       "28                 195.0             2  \n",
       "29                 384.0             3  \n",
       "...                  ...           ...  \n",
       "5254              1453.0             2  \n",
       "5255              1840.0             3  \n",
       "5256                 0.0             1  \n",
       "5257               360.0             2  \n",
       "5258               545.0             3  \n",
       "5259                 0.0             1  \n",
       "5260               154.0             2  \n",
       "5261               360.0             3  \n",
       "5262               545.0             4  \n",
       "5263                 0.0             1  \n",
       "5264               378.0             2  \n",
       "5265                 0.0             1  \n",
       "5266               378.0             2  \n",
       "5267                 0.0             1  \n",
       "5268               385.0             2  \n",
       "5269                 0.0             1  \n",
       "5270               468.0             2  \n",
       "5271                 0.0             1  \n",
       "5272               201.0             2  \n",
       "5273               388.0             3  \n",
       "5274               773.0             4  \n",
       "5275              1046.0             5  \n",
       "5276              1305.0             6  \n",
       "5277                 0.0             1  \n",
       "5278               201.0             2  \n",
       "5279               388.0             3  \n",
       "5280               577.0             4  \n",
       "5281               773.0             5  \n",
       "5282              1046.0             6  \n",
       "5283              1305.0             7  \n",
       "\n",
       "[5284 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = X_dataset.drop(['Unnamed: 0', 'subject_id', 'eye', 'number _visits',\n",
    "       'dob', 'visit_date', 'baseline_visit', 'days_from_baseline', 'visit_number'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_eyeD', 'primary_dx', 'gender', 'race', 'avg_cd_ratio',\n",
       "       'gca_average', 'gca_tempsup', 'gca_sup', 'gca_nassup', 'gca_nasinf',\n",
       "       'gca_inf', 'gca_tempinf', 'rnfl_average', 'rnfl_tempsup', 'rnfl_sup',\n",
       "       'rnfl_nassup', 'rnfl_nasinf', 'rnfl_inf', 'rnfl_tempinf', 'md', 'p_md',\n",
       "       'psd', 'p_psd', 'vfi', 'age'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GL          3524\n",
      "GS          1735\n",
      "PDS           21\n",
      "PDS/OHTN       4\n",
      "Name: primary_dx, dtype: int64\n",
      "female    3081\n",
      "male      2203\n",
      "Name: gender, dtype: int64\n",
      "white       4002\n",
      "black       1028\n",
      "asian        250\n",
      "hispanic       4\n",
      "Name: race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_dataset['primary_dx'].value_counts())\n",
    "print(X_dataset['gender'].value_counts())\n",
    "print(X_dataset['race'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#label encoding dx: 4 total\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X_dataset.iloc[:, 1] = labelencoder_X_1.fit_transform(X_dataset.iloc[:, 1])\n",
    "\n",
    "#label encoding gender: 2 total\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X_dataset.iloc[:, 2] = labelencoder_X_2.fit_transform(X_dataset.iloc[:, 2])\n",
    "\n",
    "#label encoding race: 4 total\n",
    "labelencoder_X_3 = LabelEncoder()\n",
    "X_dataset.iloc[:, 3] = labelencoder_X_3.fit_transform(X_dataset.iloc[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3524\n",
      "1    1735\n",
      "2      21\n",
      "3       4\n",
      "Name: primary_dx, dtype: int64\n",
      "0    3081\n",
      "1    2203\n",
      "Name: gender, dtype: int64\n",
      "3    4002\n",
      "1    1028\n",
      "0     250\n",
      "2       4\n",
      "Name: race, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_dataset['primary_dx'].value_counts())\n",
    "print(X_dataset['gender'].value_counts())\n",
    "print(X_dataset['race'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = pd.get_dummies(X_dataset, columns=['primary_dx', 'race'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = {column:i for i, column in enumerate(X_dataset.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_eyeD</th>\n",
       "      <th>gender</th>\n",
       "      <th>avg_cd_ratio</th>\n",
       "      <th>gca_average</th>\n",
       "      <th>gca_tempsup</th>\n",
       "      <th>gca_sup</th>\n",
       "      <th>gca_nassup</th>\n",
       "      <th>gca_nasinf</th>\n",
       "      <th>gca_inf</th>\n",
       "      <th>gca_tempinf</th>\n",
       "      <th>...</th>\n",
       "      <th>psd</th>\n",
       "      <th>p_psd</th>\n",
       "      <th>vfi</th>\n",
       "      <th>age</th>\n",
       "      <th>primary_dx_1</th>\n",
       "      <th>primary_dx_2</th>\n",
       "      <th>primary_dx_3</th>\n",
       "      <th>race_1</th>\n",
       "      <th>race_2</th>\n",
       "      <th>race_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>2.041738</td>\n",
       "      <td>3</td>\n",
       "      <td>94.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>66</td>\n",
       "      <td>64</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>2.588213</td>\n",
       "      <td>5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>67</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905461</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>2.142891</td>\n",
       "      <td>4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>3.749730</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>55</td>\n",
       "      <td>51</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>5.093309</td>\n",
       "      <td>5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>59</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>2.824880</td>\n",
       "      <td>5</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>2.798981</td>\n",
       "      <td>5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>3.273407</td>\n",
       "      <td>5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>4.265795</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>67</td>\n",
       "      <td>57</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>11.561122</td>\n",
       "      <td>5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>19.230917</td>\n",
       "      <td>5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>12.502590</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435489</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>2.218196</td>\n",
       "      <td>4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>72</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "      <td>77</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>2.027683</td>\n",
       "      <td>3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>1.721869</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>69</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>74</td>\n",
       "      <td>69</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>1.794734</td>\n",
       "      <td>2</td>\n",
       "      <td>97.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>80</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458814</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>84</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>1.448772</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.595879</td>\n",
       "      <td>2</td>\n",
       "      <td>99.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399587</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.336596</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>1.770109</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>84</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442115</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>1.733804</td>\n",
       "      <td>2</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>81</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>1.419058</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>62</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>1.273503</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>64</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>1.241652</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1.425608</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>3511OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.811340</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5255</th>\n",
       "      <td>3511OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.909853</td>\n",
       "      <td>3</td>\n",
       "      <td>95.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5256</th>\n",
       "      <td>3514OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>44</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>4.405549</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257</th>\n",
       "      <td>3514OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>43</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>4.111497</td>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>3514OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>4.055085</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5259</th>\n",
       "      <td>3514OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1.770109</td>\n",
       "      <td>3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5260</th>\n",
       "      <td>3514OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>1.603245</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5261</th>\n",
       "      <td>3514OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>...</td>\n",
       "      <td>1.659587</td>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5262</th>\n",
       "      <td>3514OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>1.485936</td>\n",
       "      <td>0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263</th>\n",
       "      <td>3516OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1.892344</td>\n",
       "      <td>3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5264</th>\n",
       "      <td>3516OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>1.990673</td>\n",
       "      <td>3</td>\n",
       "      <td>96.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>3516OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.69</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>1.506607</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>3516OS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>1.559553</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>3518OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>1.324342</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5268</th>\n",
       "      <td>3518OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>72</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291219</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5269</th>\n",
       "      <td>3520OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>90</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>1.555966</td>\n",
       "      <td>1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>3520OD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>2.108628</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5271</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>77</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>88</td>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>2.506109</td>\n",
       "      <td>5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>3.548134</td>\n",
       "      <td>5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>76</td>\n",
       "      <td>74</td>\n",
       "      <td>79</td>\n",
       "      <td>88</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>2.685344</td>\n",
       "      <td>5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "      <td>88</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>3.953666</td>\n",
       "      <td>5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>74</td>\n",
       "      <td>72</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "      <td>77</td>\n",
       "      <td>69</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>5.767665</td>\n",
       "      <td>5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>3521OD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>4.027170</td>\n",
       "      <td>5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>1.462177</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>1.380384</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>89</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>1.367729</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>88</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412538</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>86</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396368</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1.402814</td>\n",
       "      <td>0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>3521OS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>86</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>87</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>1.318257</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5284 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_eyeD  gender  avg_cd_ratio  gca_average  gca_tempsup  gca_sup  \\\n",
       "0             4OD       1          0.70           66           63       61   \n",
       "1             4OD       1          0.69           66           64       61   \n",
       "2             4OD       1          0.72           65           63       59   \n",
       "3             4OD       1          0.73           65           63       60   \n",
       "4             4OS       1          0.76           54           48       53   \n",
       "5             4OS       1          0.74           55           53       53   \n",
       "6             4OS       1          0.76           57           56       56   \n",
       "7             4OS       1          0.76           57           56       55   \n",
       "8             4OS       1          0.75           56           56       56   \n",
       "9             4OS       1          0.77           55           55       53   \n",
       "10           14OD       0          0.69           67           57       63   \n",
       "11           14OD       0          0.69           68           59       69   \n",
       "12           14OD       0          0.71           63           55       65   \n",
       "13           14OS       0          0.59           70           65       71   \n",
       "14           14OS       0          0.60           71           66       71   \n",
       "15           14OS       0          0.59           72           68       73   \n",
       "16           14OS       0          0.61           70           67       72   \n",
       "17           14OS       0          0.60           69           65       70   \n",
       "18           18OD       0          0.72           80           76       80   \n",
       "19           18OD       0          0.72           80           78       80   \n",
       "20           18OD       0          0.72           80           79       78   \n",
       "21           18OD       0          0.72           78           77       78   \n",
       "22           18OD       0          0.73           79           77       78   \n",
       "23           18OS       0          0.76           80           79       81   \n",
       "24           18OS       0          0.75           80           78       79   \n",
       "25           18OS       0          0.76           79           77       79   \n",
       "26           18OS       0          0.77           78           75       77   \n",
       "27           23OD       1          0.70           65           63       64   \n",
       "28           23OD       1          0.72           64           63       62   \n",
       "29           23OD       1          0.70           63           63       62   \n",
       "...           ...     ...           ...          ...          ...      ...   \n",
       "5254       3511OS       0          0.55           79           77       77   \n",
       "5255       3511OS       0          0.58           79           77       77   \n",
       "5256       3514OD       1          0.75           44           57       56   \n",
       "5257       3514OD       1          0.70           43           57       57   \n",
       "5258       3514OD       1          0.72           41           53       54   \n",
       "5259       3514OS       1          0.50           81           74       79   \n",
       "5260       3514OS       1          0.48           79           74       78   \n",
       "5261       3514OS       1          0.48           80           75       78   \n",
       "5262       3514OS       1          0.52           78           73       77   \n",
       "5263       3516OD       1          0.70           71           71       73   \n",
       "5264       3516OD       1          0.71           71           68       74   \n",
       "5265       3516OS       1          0.69           75           71       79   \n",
       "5266       3516OS       1          0.70           73           73       79   \n",
       "5267       3518OD       1          0.17           71           77       76   \n",
       "5268       3518OD       1          0.20           72           77       76   \n",
       "5269       3520OD       1          0.44           90           88       92   \n",
       "5270       3520OD       1          0.45           88           82       91   \n",
       "5271       3521OD       0          0.68           77           74       77   \n",
       "5272       3521OD       0          0.69           77           75       80   \n",
       "5273       3521OD       0          0.69           76           74       79   \n",
       "5274       3521OD       0          0.70           75           73       76   \n",
       "5275       3521OD       0          0.72           74           72       77   \n",
       "5276       3521OD       0          0.71           75           73       78   \n",
       "5277       3521OS       0          0.10           88           88       91   \n",
       "5278       3521OS       0          0.09           87           86       89   \n",
       "5279       3521OS       0          0.09           89           86       90   \n",
       "5280       3521OS       0          0.09           88           86       90   \n",
       "5281       3521OS       0          0.12           86           84       88   \n",
       "5282       3521OS       0          0.12           86           85       89   \n",
       "5283       3521OS       0          0.19           86           84       88   \n",
       "\n",
       "      gca_nassup  gca_nasinf  gca_inf  gca_tempinf   ...          psd  p_psd  \\\n",
       "0             65          71       69           68   ...     2.041738      3   \n",
       "1             65          71       68           68   ...     2.588213      5   \n",
       "2             65          73       67           64   ...     1.905461      3   \n",
       "3             65          70       67           67   ...     2.142891      4   \n",
       "4             59          57       52           57   ...     3.749730      5   \n",
       "5             58          55       51           58   ...     5.093309      5   \n",
       "6             59          57       53           61   ...     2.824880      5   \n",
       "7             60          56       53           61   ...     2.798981      5   \n",
       "8             60          56       52           59   ...     3.273407      5   \n",
       "9             56          53       51           60   ...     4.265795      1   \n",
       "10            75          71       66           68   ...    11.561122      5   \n",
       "11            74          73       65           65   ...    19.230917      5   \n",
       "12            74          69       56           57   ...    12.502590      1   \n",
       "13            76          70       67           68   ...     1.435489      0   \n",
       "14            77          74       70           68   ...     2.218196      4   \n",
       "15            77          72       71           69   ...     2.027683      3   \n",
       "16            77          71       68           68   ...     1.721869      2   \n",
       "17            74          69       68           68   ...     1.794734      2   \n",
       "18            82          82       80           80   ...     1.458814      0   \n",
       "19            81          84       79           81   ...     1.448772      0   \n",
       "20            81          83       77           79   ...     1.595879      2   \n",
       "21            81          81       76           78   ...     1.399587      0   \n",
       "22            80          82       77           78   ...     1.336596      0   \n",
       "23            85          83       76           79   ...     1.770109      3   \n",
       "24            84          83       77           78   ...     1.442115      0   \n",
       "25            83          82       77           78   ...     1.733804      2   \n",
       "26            83          81       75           75   ...     1.419058      0   \n",
       "27            67          66       62           67   ...     1.273503      0   \n",
       "28            66          65       60           66   ...     1.241652      0   \n",
       "29            65          64       60           64   ...     1.425608      0   \n",
       "...          ...         ...      ...          ...   ...          ...    ...   \n",
       "5254          81          80       80           80   ...     1.811340      3   \n",
       "5255          80          80       80           80   ...     1.909853      3   \n",
       "5256          54          33       27           38   ...     4.405549      5   \n",
       "5257          52          30       25           36   ...     4.111497      5   \n",
       "5258          50          32       24           34   ...     4.055085      5   \n",
       "5259          85          80       82           84   ...     1.770109      3   \n",
       "5260          83          78       79           83   ...     1.603245      2   \n",
       "5261          84          78       79           83   ...     1.659587      2   \n",
       "5262          83          78       78           81   ...     1.485936      0   \n",
       "5263          74          74       71           64   ...     1.892344      3   \n",
       "5264          73          74       71           64   ...     1.990673      3   \n",
       "5265          80          75       74           73   ...     1.506607      1   \n",
       "5266          77          72       70           69   ...     1.559553      1   \n",
       "5267          67          71       70           66   ...     1.324342      0   \n",
       "5268          67          71       71           68   ...     1.291219      0   \n",
       "5269          92          90       87           90   ...     1.555966      1   \n",
       "5270          91          91       88           82   ...     2.108628      4   \n",
       "5271          88          80       72           70   ...     2.506109      5   \n",
       "5272          88          78       71           69   ...     3.548134      5   \n",
       "5273          88          77       71           69   ...     2.685344      5   \n",
       "5274          88          79       69           67   ...     3.953666      5   \n",
       "5275          86          77       69           66   ...     5.767665      5   \n",
       "5276          87          78       68           67   ...     4.027170      5   \n",
       "5277          90          89       86           86   ...     1.462177      0   \n",
       "5278          90          89       85           85   ...     1.380384      0   \n",
       "5279          91          91       87           87   ...     1.367729      0   \n",
       "5280          90          89       85           85   ...     1.412538      0   \n",
       "5281          89          88       83           84   ...     1.396368      0   \n",
       "5282          89          88       84           84   ...     1.402814      0   \n",
       "5283          89          87       82           84   ...     1.318257      0   \n",
       "\n",
       "        vfi   age  primary_dx_1  primary_dx_2  primary_dx_3  race_1  race_2  \\\n",
       "0      94.0  64.0             0             0             0       0       0   \n",
       "1      94.0  65.0             0             0             0       0       0   \n",
       "2      96.0  66.0             0             0             0       0       0   \n",
       "3      95.0  67.0             0             0             0       0       0   \n",
       "4      83.0  63.0             0             0             0       0       0   \n",
       "5      54.0  64.0             0             0             0       0       0   \n",
       "6      85.0  65.0             0             0             0       0       0   \n",
       "7      83.0  66.0             0             0             0       0       0   \n",
       "8      76.0  67.0             0             0             0       0       0   \n",
       "9      78.0  71.0             0             0             0       0       0   \n",
       "10     82.0  56.0             0             0             0       0       0   \n",
       "11     78.0  57.0             0             0             0       0       0   \n",
       "12     83.0  59.0             0             0             0       0       0   \n",
       "13     99.0  55.0             0             0             0       0       0   \n",
       "14     93.0  56.0             0             0             0       0       0   \n",
       "15     95.0  56.0             0             0             0       0       0   \n",
       "16     96.0  57.0             0             0             0       0       0   \n",
       "17     97.0  59.0             0             0             0       0       0   \n",
       "18    100.0  46.0             1             0             0       0       0   \n",
       "19    100.0  47.0             1             0             0       0       0   \n",
       "20     99.0  48.0             1             0             0       0       0   \n",
       "21    100.0  49.0             1             0             0       0       0   \n",
       "22    100.0  50.0             1             0             0       0       0   \n",
       "23     98.0  47.0             1             0             0       0       0   \n",
       "24     98.0  48.0             1             0             0       0       0   \n",
       "25     98.0  49.0             1             0             0       0       0   \n",
       "26    100.0  50.0             1             0             0       0       0   \n",
       "27    100.0  67.0             0             0             0       0       0   \n",
       "28    100.0  68.0             0             0             0       0       0   \n",
       "29    100.0  68.0             0             0             0       0       0   \n",
       "...     ...   ...           ...           ...           ...     ...     ...   \n",
       "5254   96.0  73.0             1             0             0       0       0   \n",
       "5255   95.0  74.0             1             0             0       0       0   \n",
       "5256   12.0  68.0             0             0             0       0       0   \n",
       "5257   12.0  69.0             0             0             0       0       0   \n",
       "5258   11.0  70.0             0             0             0       0       0   \n",
       "5259   98.0  68.0             1             0             0       0       0   \n",
       "5260   96.0  69.0             1             0             0       0       0   \n",
       "5261   96.0  69.0             1             0             0       0       0   \n",
       "5262   98.0  70.0             1             0             0       0       0   \n",
       "5263   97.0  65.0             0             0             0       0       0   \n",
       "5264   96.0  66.0             0             0             0       0       0   \n",
       "5265  100.0  65.0             0             0             0       0       0   \n",
       "5266   99.0  66.0             0             0             0       0       0   \n",
       "5267  100.0  46.0             1             0             0       0       0   \n",
       "5268  100.0  47.0             1             0             0       0       0   \n",
       "5269   96.0  76.0             0             0             0       0       0   \n",
       "5270   97.0  77.0             0             0             0       0       0   \n",
       "5271   97.0  58.0             0             0             0       0       0   \n",
       "5272   95.0  59.0             0             0             0       0       0   \n",
       "5273   92.0  59.0             0             0             0       0       0   \n",
       "5274   92.0  60.0             0             0             0       0       0   \n",
       "5275   93.0  61.0             0             0             0       0       0   \n",
       "5276   93.0  62.0             0             0             0       0       0   \n",
       "5277   99.0  58.0             0             0             0       0       0   \n",
       "5278   99.0  59.0             0             0             0       0       0   \n",
       "5279   99.0  59.0             0             0             0       0       0   \n",
       "5280   99.0  60.0             0             0             0       0       0   \n",
       "5281   99.0  60.0             0             0             0       0       0   \n",
       "5282   99.0  61.0             0             0             0       0       0   \n",
       "5283  100.0  62.0             0             0             0       0       0   \n",
       "\n",
       "      race_3  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  \n",
       "6          1  \n",
       "7          1  \n",
       "8          1  \n",
       "9          1  \n",
       "10         1  \n",
       "11         1  \n",
       "12         1  \n",
       "13         1  \n",
       "14         1  \n",
       "15         1  \n",
       "16         1  \n",
       "17         1  \n",
       "18         1  \n",
       "19         1  \n",
       "20         1  \n",
       "21         1  \n",
       "22         1  \n",
       "23         1  \n",
       "24         1  \n",
       "25         1  \n",
       "26         1  \n",
       "27         1  \n",
       "28         1  \n",
       "29         1  \n",
       "...      ...  \n",
       "5254       1  \n",
       "5255       1  \n",
       "5256       1  \n",
       "5257       1  \n",
       "5258       1  \n",
       "5259       1  \n",
       "5260       1  \n",
       "5261       1  \n",
       "5262       1  \n",
       "5263       1  \n",
       "5264       1  \n",
       "5265       1  \n",
       "5266       1  \n",
       "5267       1  \n",
       "5268       1  \n",
       "5269       1  \n",
       "5270       1  \n",
       "5271       1  \n",
       "5272       1  \n",
       "5273       1  \n",
       "5274       1  \n",
       "5275       1  \n",
       "5276       1  \n",
       "5277       1  \n",
       "5278       1  \n",
       "5279       1  \n",
       "5280       1  \n",
       "5281       1  \n",
       "5282       1  \n",
       "5283       1  \n",
       "\n",
       "[5284 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y_MD = []\n",
    "y_VFI = []\n",
    "y_GCA = []\n",
    "y_RNFL = []\n",
    "\n",
    "for subject, dataset in X_dataset.groupby('subject_eyeD'):\n",
    "    if len(dataset) > 2:\n",
    "        X.append(dataset.iloc[0:len(dataset)-1,1:].values.tolist())\n",
    "        y_MD.append(dataset.iloc[-1,dictionary['md']].tolist())\n",
    "        y_VFI.append(dataset.iloc[-1,dictionary['vfi']].tolist())\n",
    "        y_GCA.append(dataset.iloc[-1,dictionary['gca_average']].tolist())\n",
    "        y_RNFL.append(dataset.iloc[-1,dictionary['rnfl_average']].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre.sequence.pad_sequences(X, maxlen=None, dtype='float', padding='post', value=9999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RNFL = np.array(y_RNFL)\n",
    "y_VFI = np.array(y_VFI)\n",
    "y_MD = np.array(y_MD)\n",
    "y_GCA = np.array(y_GCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Masking, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def lstm_baseline(X, y, name):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 200)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 200)\n",
    "    \n",
    "    batch_size=10\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=9999.0))\n",
    "    model.add(BatchNormalization(axis = 1))\n",
    "    model.add(LSTM(128,\n",
    "                   return_sequences=True,\n",
    "                  ))\n",
    "\n",
    "    model.add(LSTM(64))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer='adam',\n",
    "        metrics=['mean_absolute_error'])\n",
    "\n",
    "#     model.fit(X_train,y_train, epochs= 500)\n",
    "\n",
    "    weight_path=\"LSTM_Interval_best_{}.hdf5\".format(name)\n",
    "    checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "    callbacks_list = [checkpoint, early_stop]\n",
    "        \n",
    "    #fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val), shuffle=False, callbacks=callbacks_list, verbose=1)\n",
    "    \n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.savefig('loss_plot_{}.png'.format(name))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    model.load_weights(weight_path)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_pred = y_pred.tolist()\n",
    "    dictionary_DF = {'predicted':y_pred, 'actual':y_test}\n",
    "    data = pd.DataFrame(dictionary_DF)\n",
    "    data.to_csv('Predicted_vs_Actual_of_{}'.format(name))\n",
    "    \n",
    "    mean_absolute_error = abs(data['predicted']-data['actual'])\n",
    "    mean_absolute_error = mean_absolute_error.describe()\n",
    "    mean_absolute_error.to_csv('MAE_of_{}_Predicted_vs_Actual'.format(name))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/500\n",
      "522/522 [==============================] - 3s 6ms/step - loss: 0.2549 - mean_absolute_error: 0.4031 - val_loss: 0.1705 - val_mean_absolute_error: 0.3265\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17050, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 2/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1637 - mean_absolute_error: 0.3291 - val_loss: 0.1324 - val_mean_absolute_error: 0.2831\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17050 to 0.13244, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 3/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1242 - mean_absolute_error: 0.2743 - val_loss: 0.1062 - val_mean_absolute_error: 0.2391\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13244 to 0.10625, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 4/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1268 - mean_absolute_error: 0.2753 - val_loss: 0.1022 - val_mean_absolute_error: 0.2367\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10625 to 0.10220, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 5/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1159 - mean_absolute_error: 0.2545 - val_loss: 0.1217 - val_mean_absolute_error: 0.2386\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10220\n",
      "Epoch 6/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1001 - mean_absolute_error: 0.2368 - val_loss: 0.0945 - val_mean_absolute_error: 0.2112\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10220 to 0.09449, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 7/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1013 - mean_absolute_error: 0.2396 - val_loss: 0.1017 - val_mean_absolute_error: 0.2162\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09449\n",
      "Epoch 8/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0983 - mean_absolute_error: 0.2305 - val_loss: 0.0843 - val_mean_absolute_error: 0.1989\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09449 to 0.08431, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 9/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0942 - mean_absolute_error: 0.2260 - val_loss: 0.0875 - val_mean_absolute_error: 0.1998\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08431\n",
      "Epoch 10/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0887 - mean_absolute_error: 0.2164 - val_loss: 0.0890 - val_mean_absolute_error: 0.2012\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08431\n",
      "Epoch 11/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0854 - mean_absolute_error: 0.2167 - val_loss: 0.0885 - val_mean_absolute_error: 0.1992\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08431\n",
      "Epoch 12/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0859 - mean_absolute_error: 0.2142 - val_loss: 0.0848 - val_mean_absolute_error: 0.1961\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08431\n",
      "Epoch 13/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0833 - mean_absolute_error: 0.2083 - val_loss: 0.0779 - val_mean_absolute_error: 0.1884\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08431 to 0.07787, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 14/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0892 - mean_absolute_error: 0.2177 - val_loss: 0.1117 - val_mean_absolute_error: 0.2211\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07787\n",
      "Epoch 15/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0840 - mean_absolute_error: 0.2116 - val_loss: 0.0746 - val_mean_absolute_error: 0.1856\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07787 to 0.07457, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 16/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0780 - mean_absolute_error: 0.2049 - val_loss: 0.0803 - val_mean_absolute_error: 0.1883\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.07457\n",
      "Epoch 17/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0805 - mean_absolute_error: 0.2059 - val_loss: 0.0855 - val_mean_absolute_error: 0.1920\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.07457\n",
      "Epoch 18/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0879 - mean_absolute_error: 0.2196 - val_loss: 0.0789 - val_mean_absolute_error: 0.1842\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.07457\n",
      "Epoch 19/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0799 - mean_absolute_error: 0.2023 - val_loss: 0.0878 - val_mean_absolute_error: 0.1924\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07457\n",
      "Epoch 20/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0830 - mean_absolute_error: 0.2058 - val_loss: 0.0739 - val_mean_absolute_error: 0.1805\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07457 to 0.07387, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 21/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0788 - mean_absolute_error: 0.2043 - val_loss: 0.0760 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07387\n",
      "Epoch 22/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0780 - mean_absolute_error: 0.2008 - val_loss: 0.0800 - val_mean_absolute_error: 0.1861\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07387\n",
      "Epoch 23/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0783 - mean_absolute_error: 0.2007 - val_loss: 0.0766 - val_mean_absolute_error: 0.1811\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07387\n",
      "Epoch 24/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0764 - mean_absolute_error: 0.1997 - val_loss: 0.0811 - val_mean_absolute_error: 0.1851\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07387\n",
      "Epoch 25/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0758 - mean_absolute_error: 0.1984 - val_loss: 0.0753 - val_mean_absolute_error: 0.1799\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07387\n",
      "Epoch 26/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0796 - mean_absolute_error: 0.2054 - val_loss: 0.0808 - val_mean_absolute_error: 0.1858\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.07387\n",
      "Epoch 27/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0744 - mean_absolute_error: 0.1993 - val_loss: 0.0889 - val_mean_absolute_error: 0.1944\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07387\n",
      "Epoch 28/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0714 - mean_absolute_error: 0.1918 - val_loss: 0.0722 - val_mean_absolute_error: 0.1766\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07387 to 0.07219, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 29/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0704 - mean_absolute_error: 0.1885 - val_loss: 0.0710 - val_mean_absolute_error: 0.1771\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07219 to 0.07096, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 30/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0683 - mean_absolute_error: 0.1826 - val_loss: 0.0685 - val_mean_absolute_error: 0.1728\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07096 to 0.06854, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 31/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0695 - mean_absolute_error: 0.1845 - val_loss: 0.0692 - val_mean_absolute_error: 0.1713\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.06854\n",
      "Epoch 32/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0690 - mean_absolute_error: 0.1906 - val_loss: 0.0692 - val_mean_absolute_error: 0.1722\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.06854\n",
      "Epoch 33/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0701 - mean_absolute_error: 0.1856 - val_loss: 0.0784 - val_mean_absolute_error: 0.1823\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.06854\n",
      "Epoch 34/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0700 - mean_absolute_error: 0.1900 - val_loss: 0.0657 - val_mean_absolute_error: 0.1694\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.06854 to 0.06574, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 35/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0727 - mean_absolute_error: 0.1919 - val_loss: 0.0675 - val_mean_absolute_error: 0.1692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 0.06574\n",
      "Epoch 36/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0743 - mean_absolute_error: 0.1928 - val_loss: 0.0814 - val_mean_absolute_error: 0.1867\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.06574\n",
      "Epoch 37/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0726 - mean_absolute_error: 0.1890 - val_loss: 0.0629 - val_mean_absolute_error: 0.1688\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.06574 to 0.06287, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 38/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0659 - mean_absolute_error: 0.1849 - val_loss: 0.0641 - val_mean_absolute_error: 0.1657\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.06287\n",
      "Epoch 39/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0719 - mean_absolute_error: 0.1898 - val_loss: 0.0773 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.06287\n",
      "Epoch 40/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0678 - mean_absolute_error: 0.1849 - val_loss: 0.0686 - val_mean_absolute_error: 0.1699\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.06287\n",
      "Epoch 41/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0699 - mean_absolute_error: 0.1858 - val_loss: 0.0624 - val_mean_absolute_error: 0.1651\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.06287 to 0.06245, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 42/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0729 - mean_absolute_error: 0.1929 - val_loss: 0.0994 - val_mean_absolute_error: 0.2083\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.06245\n",
      "Epoch 43/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0725 - mean_absolute_error: 0.1900 - val_loss: 0.0630 - val_mean_absolute_error: 0.1695\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.06245\n",
      "Epoch 44/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0665 - mean_absolute_error: 0.1828 - val_loss: 0.0718 - val_mean_absolute_error: 0.1758\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.06245\n",
      "Epoch 45/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0649 - mean_absolute_error: 0.1803 - val_loss: 0.0641 - val_mean_absolute_error: 0.1652\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.06245\n",
      "Epoch 46/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0648 - mean_absolute_error: 0.1785 - val_loss: 0.0748 - val_mean_absolute_error: 0.1783\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.06245\n",
      "Epoch 47/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0659 - mean_absolute_error: 0.1786 - val_loss: 0.0666 - val_mean_absolute_error: 0.1688\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.06245\n",
      "Epoch 48/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0676 - mean_absolute_error: 0.1829 - val_loss: 0.0698 - val_mean_absolute_error: 0.1739\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.06245\n",
      "Epoch 49/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0687 - mean_absolute_error: 0.1832 - val_loss: 0.0793 - val_mean_absolute_error: 0.1838\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.06245\n",
      "Epoch 50/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0719 - mean_absolute_error: 0.1920 - val_loss: 0.0680 - val_mean_absolute_error: 0.1710\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.06245\n",
      "Epoch 51/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0667 - mean_absolute_error: 0.1822 - val_loss: 0.0731 - val_mean_absolute_error: 0.1764\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.06245\n",
      "Epoch 52/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0663 - mean_absolute_error: 0.1819 - val_loss: 0.0685 - val_mean_absolute_error: 0.1704\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.06245\n",
      "Epoch 53/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0655 - mean_absolute_error: 0.1805 - val_loss: 0.0650 - val_mean_absolute_error: 0.1676\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.06245\n",
      "Epoch 54/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0634 - mean_absolute_error: 0.1747 - val_loss: 0.0778 - val_mean_absolute_error: 0.1836\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.06245\n",
      "Epoch 55/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0649 - mean_absolute_error: 0.1814 - val_loss: 0.0690 - val_mean_absolute_error: 0.1726\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.06245\n",
      "Epoch 56/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0627 - mean_absolute_error: 0.1754 - val_loss: 0.0632 - val_mean_absolute_error: 0.1703\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.06245\n",
      "Epoch 57/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0663 - mean_absolute_error: 0.1829 - val_loss: 0.0731 - val_mean_absolute_error: 0.1783\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.06245\n",
      "Epoch 58/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0720 - mean_absolute_error: 0.1900 - val_loss: 0.0822 - val_mean_absolute_error: 0.1880\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.06245\n",
      "Epoch 59/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0670 - mean_absolute_error: 0.1848 - val_loss: 0.0622 - val_mean_absolute_error: 0.1630\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.06245 to 0.06219, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 60/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0566 - mean_absolute_error: 0.1669 - val_loss: 0.0715 - val_mean_absolute_error: 0.1788\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.06219\n",
      "Epoch 61/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0695 - mean_absolute_error: 0.1843 - val_loss: 0.0689 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.06219\n",
      "Epoch 62/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0630 - mean_absolute_error: 0.1776 - val_loss: 0.0659 - val_mean_absolute_error: 0.1690\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.06219\n",
      "Epoch 63/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0599 - mean_absolute_error: 0.1719 - val_loss: 0.0652 - val_mean_absolute_error: 0.1681\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.06219\n",
      "Epoch 64/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0615 - mean_absolute_error: 0.1729 - val_loss: 0.0734 - val_mean_absolute_error: 0.1794\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.06219\n",
      "Epoch 65/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0639 - mean_absolute_error: 0.1774 - val_loss: 0.0701 - val_mean_absolute_error: 0.1759\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.06219\n",
      "Epoch 66/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0569 - mean_absolute_error: 0.1660 - val_loss: 0.0589 - val_mean_absolute_error: 0.1607\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.06219 to 0.05886, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 67/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0564 - mean_absolute_error: 0.1703 - val_loss: 0.0637 - val_mean_absolute_error: 0.1658\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05886\n",
      "Epoch 68/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0649 - mean_absolute_error: 0.1798 - val_loss: 0.0943 - val_mean_absolute_error: 0.2042\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05886\n",
      "Epoch 69/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0577 - mean_absolute_error: 0.1713 - val_loss: 0.0631 - val_mean_absolute_error: 0.1648\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05886\n",
      "Epoch 70/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0567 - mean_absolute_error: 0.1678 - val_loss: 0.0631 - val_mean_absolute_error: 0.1647\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05886\n",
      "Epoch 71/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0540 - mean_absolute_error: 0.1626 - val_loss: 0.0639 - val_mean_absolute_error: 0.1660\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05886\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0581 - mean_absolute_error: 0.1682 - val_loss: 0.0681 - val_mean_absolute_error: 0.1716\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05886\n",
      "Epoch 73/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0596 - mean_absolute_error: 0.1726 - val_loss: 0.0776 - val_mean_absolute_error: 0.1836\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.05886\n",
      "Epoch 74/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0598 - mean_absolute_error: 0.1738 - val_loss: 0.0674 - val_mean_absolute_error: 0.1711\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05886\n",
      "Epoch 75/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0505 - mean_absolute_error: 0.1576 - val_loss: 0.0674 - val_mean_absolute_error: 0.1705\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05886\n",
      "Epoch 76/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0590 - mean_absolute_error: 0.1708 - val_loss: 0.0754 - val_mean_absolute_error: 0.1810\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05886\n",
      "Epoch 77/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0567 - mean_absolute_error: 0.1639 - val_loss: 0.0667 - val_mean_absolute_error: 0.1690\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05886\n",
      "Epoch 78/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0554 - mean_absolute_error: 0.1664 - val_loss: 0.0683 - val_mean_absolute_error: 0.1726\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.05886\n",
      "Epoch 79/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0547 - mean_absolute_error: 0.1654 - val_loss: 0.0713 - val_mean_absolute_error: 0.1744\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.05886\n",
      "Epoch 80/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0529 - mean_absolute_error: 0.1607 - val_loss: 0.0695 - val_mean_absolute_error: 0.1762\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.05886\n",
      "Epoch 81/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0529 - mean_absolute_error: 0.1610 - val_loss: 0.0586 - val_mean_absolute_error: 0.1623\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.05886 to 0.05855, saving model to LSTM_Interval_best_MD.hdf5\n",
      "Epoch 82/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0523 - mean_absolute_error: 0.1598 - val_loss: 0.0641 - val_mean_absolute_error: 0.1712\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.05855\n",
      "Epoch 83/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0552 - mean_absolute_error: 0.1651 - val_loss: 0.0746 - val_mean_absolute_error: 0.1812\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.05855\n",
      "Epoch 84/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0606 - mean_absolute_error: 0.1719 - val_loss: 0.0707 - val_mean_absolute_error: 0.1770\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.05855\n",
      "Epoch 85/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0536 - mean_absolute_error: 0.1602 - val_loss: 0.0730 - val_mean_absolute_error: 0.1794\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.05855\n",
      "Epoch 86/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0542 - mean_absolute_error: 0.1621 - val_loss: 0.0673 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.05855\n",
      "Epoch 87/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0502 - mean_absolute_error: 0.1591 - val_loss: 0.0668 - val_mean_absolute_error: 0.1711\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.05855\n",
      "Epoch 88/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0509 - mean_absolute_error: 0.1602 - val_loss: 0.0672 - val_mean_absolute_error: 0.1737\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.05855\n",
      "Epoch 89/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0512 - mean_absolute_error: 0.1600 - val_loss: 0.0658 - val_mean_absolute_error: 0.1707\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.05855\n",
      "Epoch 90/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0528 - mean_absolute_error: 0.1598 - val_loss: 0.0705 - val_mean_absolute_error: 0.1780\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.05855\n",
      "Epoch 91/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0490 - mean_absolute_error: 0.1537 - val_loss: 0.0757 - val_mean_absolute_error: 0.1853\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.05855\n",
      "Epoch 92/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0522 - mean_absolute_error: 0.1593 - val_loss: 0.0782 - val_mean_absolute_error: 0.1845\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.05855\n",
      "Epoch 93/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0522 - mean_absolute_error: 0.1572 - val_loss: 0.0635 - val_mean_absolute_error: 0.1718\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.05855\n",
      "Epoch 94/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0497 - mean_absolute_error: 0.1558 - val_loss: 0.0647 - val_mean_absolute_error: 0.1708\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.05855\n",
      "Epoch 95/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0539 - mean_absolute_error: 0.1632 - val_loss: 0.0611 - val_mean_absolute_error: 0.1671\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.05855\n",
      "Epoch 96/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0501 - mean_absolute_error: 0.1559 - val_loss: 0.0650 - val_mean_absolute_error: 0.1689\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.05855\n",
      "Epoch 97/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0513 - mean_absolute_error: 0.1617 - val_loss: 0.0712 - val_mean_absolute_error: 0.1734\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.05855\n",
      "Epoch 98/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0488 - mean_absolute_error: 0.1562 - val_loss: 0.0654 - val_mean_absolute_error: 0.1737\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.05855\n",
      "Epoch 99/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0572 - mean_absolute_error: 0.1679 - val_loss: 0.0677 - val_mean_absolute_error: 0.1684\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.05855\n",
      "Epoch 100/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0500 - mean_absolute_error: 0.1570 - val_loss: 0.0598 - val_mean_absolute_error: 0.1665\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.05855\n",
      "Epoch 101/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0459 - mean_absolute_error: 0.1473 - val_loss: 0.0658 - val_mean_absolute_error: 0.1744\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.05855\n",
      "Epoch 102/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0523 - mean_absolute_error: 0.1585 - val_loss: 0.0761 - val_mean_absolute_error: 0.1799\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.05855\n",
      "Epoch 103/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0480 - mean_absolute_error: 0.1551 - val_loss: 0.0699 - val_mean_absolute_error: 0.1757\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.05855\n",
      "Epoch 104/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0450 - mean_absolute_error: 0.1458 - val_loss: 0.0710 - val_mean_absolute_error: 0.1743\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.05855\n",
      "Epoch 105/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0495 - mean_absolute_error: 0.1552 - val_loss: 0.0664 - val_mean_absolute_error: 0.1725\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.05855\n",
      "Epoch 106/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0431 - mean_absolute_error: 0.1435 - val_loss: 0.0670 - val_mean_absolute_error: 0.1722\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.05855\n",
      "Epoch 107/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0477 - mean_absolute_error: 0.1511 - val_loss: 0.0634 - val_mean_absolute_error: 0.1679\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.05855\n",
      "Epoch 108/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0451 - mean_absolute_error: 0.1458 - val_loss: 0.0634 - val_mean_absolute_error: 0.1745\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.05855\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0449 - mean_absolute_error: 0.1474 - val_loss: 0.0638 - val_mean_absolute_error: 0.1708\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.05855\n",
      "Epoch 110/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0455 - mean_absolute_error: 0.1489 - val_loss: 0.0674 - val_mean_absolute_error: 0.1762\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.05855\n",
      "Epoch 111/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0468 - mean_absolute_error: 0.1527 - val_loss: 0.0619 - val_mean_absolute_error: 0.1663\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.05855\n",
      "Epoch 112/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0478 - mean_absolute_error: 0.1493 - val_loss: 0.0716 - val_mean_absolute_error: 0.1799\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.05855\n",
      "Epoch 113/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0453 - mean_absolute_error: 0.1480 - val_loss: 0.0716 - val_mean_absolute_error: 0.1776\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.05855\n",
      "Epoch 114/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0436 - mean_absolute_error: 0.1423 - val_loss: 0.0643 - val_mean_absolute_error: 0.1739\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.05855\n",
      "Epoch 115/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0489 - mean_absolute_error: 0.1543 - val_loss: 0.0636 - val_mean_absolute_error: 0.1684\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.05855\n",
      "Epoch 116/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0552 - mean_absolute_error: 0.1667 - val_loss: 0.0806 - val_mean_absolute_error: 0.1852\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.05855\n",
      "Epoch 117/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0464 - mean_absolute_error: 0.1507 - val_loss: 0.0741 - val_mean_absolute_error: 0.1846\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.05855\n",
      "Epoch 118/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0467 - mean_absolute_error: 0.1517 - val_loss: 0.0646 - val_mean_absolute_error: 0.1736\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.05855\n",
      "Epoch 119/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0420 - mean_absolute_error: 0.1399 - val_loss: 0.0602 - val_mean_absolute_error: 0.1663\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.05855\n",
      "Epoch 120/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0425 - mean_absolute_error: 0.1447 - val_loss: 0.0687 - val_mean_absolute_error: 0.1717\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.05855\n",
      "Epoch 121/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0405 - mean_absolute_error: 0.1381 - val_loss: 0.0675 - val_mean_absolute_error: 0.1712\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.05855\n",
      "Epoch 122/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0433 - mean_absolute_error: 0.1468 - val_loss: 0.0654 - val_mean_absolute_error: 0.1696\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.05855\n",
      "Epoch 123/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0419 - mean_absolute_error: 0.1445 - val_loss: 0.0644 - val_mean_absolute_error: 0.1744\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.05855\n",
      "Epoch 124/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0428 - mean_absolute_error: 0.1464 - val_loss: 0.0655 - val_mean_absolute_error: 0.1850\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.05855\n",
      "Epoch 125/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0402 - mean_absolute_error: 0.1421 - val_loss: 0.0616 - val_mean_absolute_error: 0.1708\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.05855\n",
      "Epoch 126/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0424 - mean_absolute_error: 0.1418 - val_loss: 0.0640 - val_mean_absolute_error: 0.1695\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.05855\n",
      "Epoch 127/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0430 - mean_absolute_error: 0.1454 - val_loss: 0.0663 - val_mean_absolute_error: 0.1764\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.05855\n",
      "Epoch 128/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0382 - mean_absolute_error: 0.1359 - val_loss: 0.0620 - val_mean_absolute_error: 0.1663\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.05855\n",
      "Epoch 129/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0409 - mean_absolute_error: 0.1409 - val_loss: 0.0672 - val_mean_absolute_error: 0.1727\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.05855\n",
      "Epoch 130/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0427 - mean_absolute_error: 0.1437 - val_loss: 0.0626 - val_mean_absolute_error: 0.1725\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.05855\n",
      "Epoch 131/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0389 - mean_absolute_error: 0.1382 - val_loss: 0.0716 - val_mean_absolute_error: 0.1744\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.05855\n",
      "Epoch 00131: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a365eca58>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_baseline(X,y_MD,\"MD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/500\n",
      "522/522 [==============================] - 3s 6ms/step - loss: 7954.8502 - mean_absolute_error: 87.1981 - val_loss: 7028.9057 - val_mean_absolute_error: 80.8727\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7028.90571, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 2/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 6615.1653 - mean_absolute_error: 79.1853 - val_loss: 5741.1733 - val_mean_absolute_error: 72.5851\n",
      "\n",
      "Epoch 00002: val_loss improved from 7028.90571 to 5741.17326, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 3/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 5530.8319 - mean_absolute_error: 72.2085 - val_loss: 4854.4797 - val_mean_absolute_error: 66.6623\n",
      "\n",
      "Epoch 00003: val_loss improved from 5741.17326 to 4854.47971, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 4/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 4671.9088 - mean_absolute_error: 66.2242 - val_loss: 4031.3330 - val_mean_absolute_error: 60.6965\n",
      "\n",
      "Epoch 00004: val_loss improved from 4854.47971 to 4031.33303, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 5/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3833.6637 - mean_absolute_error: 59.7995 - val_loss: 3263.9147 - val_mean_absolute_error: 54.6878\n",
      "\n",
      "Epoch 00005: val_loss improved from 4031.33303 to 3263.91475, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 6/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3133.4192 - mean_absolute_error: 53.9463 - val_loss: 2574.8556 - val_mean_absolute_error: 48.7893\n",
      "\n",
      "Epoch 00006: val_loss improved from 3263.91475 to 2574.85557, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 7/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2479.3918 - mean_absolute_error: 47.6284 - val_loss: 1984.8011 - val_mean_absolute_error: 42.9438\n",
      "\n",
      "Epoch 00007: val_loss improved from 2574.85557 to 1984.80111, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 8/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1889.3781 - mean_absolute_error: 41.1450 - val_loss: 1500.8545 - val_mean_absolute_error: 37.3229\n",
      "\n",
      "Epoch 00008: val_loss improved from 1984.80111 to 1500.85449, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 9/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1382.2942 - mean_absolute_error: 34.5239 - val_loss: 1133.3268 - val_mean_absolute_error: 32.2291\n",
      "\n",
      "Epoch 00009: val_loss improved from 1500.85449 to 1133.32676, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 10/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1080.0267 - mean_absolute_error: 30.0068 - val_loss: 875.9837 - val_mean_absolute_error: 27.8859\n",
      "\n",
      "Epoch 00010: val_loss improved from 1133.32676 to 875.98367, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 11/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 832.6234 - mean_absolute_error: 25.8530 - val_loss: 702.8260 - val_mean_absolute_error: 24.2077\n",
      "\n",
      "Epoch 00011: val_loss improved from 875.98367 to 702.82596, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 12/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 714.1708 - mean_absolute_error: 22.7100 - val_loss: 600.1447 - val_mean_absolute_error: 21.4433\n",
      "\n",
      "Epoch 00012: val_loss improved from 702.82596 to 600.14474, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 13/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 633.5730 - mean_absolute_error: 20.4807 - val_loss: 541.1192 - val_mean_absolute_error: 19.3443\n",
      "\n",
      "Epoch 00013: val_loss improved from 600.14474 to 541.11920, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 14/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 572.7025 - mean_absolute_error: 18.7243 - val_loss: 512.9149 - val_mean_absolute_error: 17.9567\n",
      "\n",
      "Epoch 00014: val_loss improved from 541.11920 to 512.91488, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 15/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 515.1123 - mean_absolute_error: 17.2396 - val_loss: 498.5840 - val_mean_absolute_error: 16.9133\n",
      "\n",
      "Epoch 00015: val_loss improved from 512.91488 to 498.58398, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 16/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 549.5508 - mean_absolute_error: 17.4773 - val_loss: 491.3176 - val_mean_absolute_error: 16.3290\n",
      "\n",
      "Epoch 00016: val_loss improved from 498.58398 to 491.31756, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 17/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 584.1600 - mean_absolute_error: 17.9097 - val_loss: 485.0638 - val_mean_absolute_error: 15.9236\n",
      "\n",
      "Epoch 00017: val_loss improved from 491.31756 to 485.06377, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 18/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 529.7025 - mean_absolute_error: 16.6474 - val_loss: 466.6679 - val_mean_absolute_error: 15.6204\n",
      "\n",
      "Epoch 00018: val_loss improved from 485.06377 to 466.66793, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 19/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 518.7853 - mean_absolute_error: 16.8202 - val_loss: 348.7903 - val_mean_absolute_error: 13.9041\n",
      "\n",
      "Epoch 00019: val_loss improved from 466.66793 to 348.79029, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 20/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 431.4191 - mean_absolute_error: 15.6832 - val_loss: 204.9500 - val_mean_absolute_error: 11.9011\n",
      "\n",
      "Epoch 00020: val_loss improved from 348.79029 to 204.94996, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 21/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 352.1377 - mean_absolute_error: 14.6059 - val_loss: 181.7825 - val_mean_absolute_error: 10.0197\n",
      "\n",
      "Epoch 00021: val_loss improved from 204.94996 to 181.78246, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 22/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 332.1777 - mean_absolute_error: 13.9141 - val_loss: 122.1428 - val_mean_absolute_error: 8.2161\n",
      "\n",
      "Epoch 00022: val_loss improved from 181.78246 to 122.14284, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 23/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 269.7621 - mean_absolute_error: 12.8794 - val_loss: 102.9211 - val_mean_absolute_error: 7.0791\n",
      "\n",
      "Epoch 00023: val_loss improved from 122.14284 to 102.92107, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 24/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 282.6271 - mean_absolute_error: 12.7727 - val_loss: 113.6740 - val_mean_absolute_error: 6.7016\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 102.92107\n",
      "Epoch 25/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 270.8611 - mean_absolute_error: 12.8970 - val_loss: 110.9978 - val_mean_absolute_error: 6.2816\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 102.92107\n",
      "Epoch 26/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 266.5809 - mean_absolute_error: 12.3668 - val_loss: 112.2576 - val_mean_absolute_error: 7.9056\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 102.92107\n",
      "Epoch 27/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 270.4332 - mean_absolute_error: 12.6934 - val_loss: 114.1392 - val_mean_absolute_error: 5.9116\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 102.92107\n",
      "Epoch 28/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 261.5927 - mean_absolute_error: 12.2743 - val_loss: 109.6754 - val_mean_absolute_error: 7.8953\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 102.92107\n",
      "Epoch 29/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 281.9834 - mean_absolute_error: 13.0501 - val_loss: 109.1095 - val_mean_absolute_error: 6.2043\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 102.92107\n",
      "Epoch 30/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 266.8464 - mean_absolute_error: 12.6073 - val_loss: 132.6867 - val_mean_absolute_error: 7.1576\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 102.92107\n",
      "Epoch 31/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 294.0019 - mean_absolute_error: 12.9723 - val_loss: 93.7888 - val_mean_absolute_error: 6.3236\n",
      "\n",
      "Epoch 00031: val_loss improved from 102.92107 to 93.78884, saving model to LSTM_Interval_best_VFI.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 273.3922 - mean_absolute_error: 12.5365 - val_loss: 106.4777 - val_mean_absolute_error: 7.0548\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 93.78884\n",
      "Epoch 33/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 238.4473 - mean_absolute_error: 11.6974 - val_loss: 97.0536 - val_mean_absolute_error: 6.1483\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 93.78884\n",
      "Epoch 34/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 282.6729 - mean_absolute_error: 13.1081 - val_loss: 80.7055 - val_mean_absolute_error: 5.5895\n",
      "\n",
      "Epoch 00034: val_loss improved from 93.78884 to 80.70551, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 35/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 269.1856 - mean_absolute_error: 12.4315 - val_loss: 81.6938 - val_mean_absolute_error: 5.0808\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 80.70551\n",
      "Epoch 36/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 238.4410 - mean_absolute_error: 11.7213 - val_loss: 82.0016 - val_mean_absolute_error: 5.3081\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 80.70551\n",
      "Epoch 37/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 248.9449 - mean_absolute_error: 12.3974 - val_loss: 86.0612 - val_mean_absolute_error: 5.3280\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 80.70551\n",
      "Epoch 38/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 282.8973 - mean_absolute_error: 12.8073 - val_loss: 88.7994 - val_mean_absolute_error: 6.5200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 80.70551\n",
      "Epoch 39/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 239.7019 - mean_absolute_error: 11.7917 - val_loss: 91.9524 - val_mean_absolute_error: 6.2356\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 80.70551\n",
      "Epoch 40/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 238.3805 - mean_absolute_error: 11.8997 - val_loss: 81.5963 - val_mean_absolute_error: 5.5609\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 80.70551\n",
      "Epoch 41/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 249.2753 - mean_absolute_error: 12.0057 - val_loss: 85.4535 - val_mean_absolute_error: 5.2256\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 80.70551\n",
      "Epoch 42/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 271.4131 - mean_absolute_error: 12.4139 - val_loss: 81.9328 - val_mean_absolute_error: 4.9396\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 80.70551\n",
      "Epoch 43/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 245.5379 - mean_absolute_error: 11.9046 - val_loss: 158.6399 - val_mean_absolute_error: 11.1740\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 80.70551\n",
      "Epoch 44/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 251.0197 - mean_absolute_error: 12.2083 - val_loss: 90.0552 - val_mean_absolute_error: 4.9890\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 80.70551\n",
      "Epoch 45/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 250.4859 - mean_absolute_error: 12.1729 - val_loss: 71.5339 - val_mean_absolute_error: 4.9291\n",
      "\n",
      "Epoch 00045: val_loss improved from 80.70551 to 71.53388, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 46/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 232.0572 - mean_absolute_error: 11.5024 - val_loss: 79.2765 - val_mean_absolute_error: 5.7960\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 71.53388\n",
      "Epoch 47/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 228.8398 - mean_absolute_error: 11.6424 - val_loss: 77.8592 - val_mean_absolute_error: 6.0103\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 71.53388\n",
      "Epoch 48/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 229.3780 - mean_absolute_error: 11.7008 - val_loss: 76.8779 - val_mean_absolute_error: 5.5492\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 71.53388\n",
      "Epoch 49/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 243.9650 - mean_absolute_error: 11.7577 - val_loss: 77.1791 - val_mean_absolute_error: 5.1119\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 71.53388\n",
      "Epoch 50/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.2503 - mean_absolute_error: 11.4636 - val_loss: 75.7381 - val_mean_absolute_error: 5.1841\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 71.53388\n",
      "Epoch 51/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 238.3392 - mean_absolute_error: 11.7954 - val_loss: 73.3317 - val_mean_absolute_error: 4.3660\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 71.53388\n",
      "Epoch 52/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 219.7717 - mean_absolute_error: 11.4896 - val_loss: 96.8164 - val_mean_absolute_error: 4.4632\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 71.53388\n",
      "Epoch 53/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 223.8044 - mean_absolute_error: 11.8196 - val_loss: 79.2386 - val_mean_absolute_error: 6.1661\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 71.53388\n",
      "Epoch 54/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 217.1782 - mean_absolute_error: 11.1486 - val_loss: 75.0185 - val_mean_absolute_error: 5.1316\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 71.53388\n",
      "Epoch 55/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 228.4985 - mean_absolute_error: 11.3822 - val_loss: 83.7413 - val_mean_absolute_error: 6.6110\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 71.53388\n",
      "Epoch 56/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.0164 - mean_absolute_error: 10.8295 - val_loss: 77.4197 - val_mean_absolute_error: 5.8731\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 71.53388\n",
      "Epoch 57/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 221.2407 - mean_absolute_error: 11.2860 - val_loss: 67.4796 - val_mean_absolute_error: 4.7888\n",
      "\n",
      "Epoch 00057: val_loss improved from 71.53388 to 67.47957, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 58/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.3442 - mean_absolute_error: 11.4336 - val_loss: 84.2332 - val_mean_absolute_error: 6.4190\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 67.47957\n",
      "Epoch 59/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 235.9922 - mean_absolute_error: 11.9163 - val_loss: 77.4167 - val_mean_absolute_error: 5.6082\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 67.47957\n",
      "Epoch 60/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 226.5272 - mean_absolute_error: 11.6638 - val_loss: 69.6752 - val_mean_absolute_error: 4.4890\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 67.47957\n",
      "Epoch 61/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 227.9795 - mean_absolute_error: 11.3485 - val_loss: 68.0785 - val_mean_absolute_error: 4.8607\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 67.47957\n",
      "Epoch 62/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 227.1154 - mean_absolute_error: 11.0308 - val_loss: 83.7911 - val_mean_absolute_error: 6.8452\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 67.47957\n",
      "Epoch 63/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 213.1814 - mean_absolute_error: 11.3137 - val_loss: 68.5255 - val_mean_absolute_error: 5.4351\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 67.47957\n",
      "Epoch 64/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 217.7320 - mean_absolute_error: 11.5799 - val_loss: 59.0336 - val_mean_absolute_error: 4.0509\n",
      "\n",
      "Epoch 00064: val_loss improved from 67.47957 to 59.03363, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 65/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 233.0472 - mean_absolute_error: 11.6242 - val_loss: 68.0955 - val_mean_absolute_error: 5.2019\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 59.03363\n",
      "Epoch 66/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 227.7921 - mean_absolute_error: 11.7956 - val_loss: 60.9255 - val_mean_absolute_error: 4.3291\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 59.03363\n",
      "Epoch 67/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.0554 - mean_absolute_error: 10.8664 - val_loss: 57.7248 - val_mean_absolute_error: 4.3163\n",
      "\n",
      "Epoch 00067: val_loss improved from 59.03363 to 57.72482, saving model to LSTM_Interval_best_VFI.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.8771 - mean_absolute_error: 11.0534 - val_loss: 62.8917 - val_mean_absolute_error: 4.7840\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 57.72482\n",
      "Epoch 69/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 224.7976 - mean_absolute_error: 11.5494 - val_loss: 70.8615 - val_mean_absolute_error: 6.2505\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 57.72482\n",
      "Epoch 70/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 223.2248 - mean_absolute_error: 11.6803 - val_loss: 71.8144 - val_mean_absolute_error: 5.8485\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 57.72482\n",
      "Epoch 71/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 232.2157 - mean_absolute_error: 11.5443 - val_loss: 100.6090 - val_mean_absolute_error: 8.0893\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 57.72482\n",
      "Epoch 72/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 225.3373 - mean_absolute_error: 11.6685 - val_loss: 69.5065 - val_mean_absolute_error: 4.8758\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 57.72482\n",
      "Epoch 73/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.8633 - mean_absolute_error: 11.0374 - val_loss: 70.9779 - val_mean_absolute_error: 5.4807\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 57.72482\n",
      "Epoch 74/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.2605 - mean_absolute_error: 11.1266 - val_loss: 66.2374 - val_mean_absolute_error: 5.6367\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 57.72482\n",
      "Epoch 75/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 242.8675 - mean_absolute_error: 11.4655 - val_loss: 65.4142 - val_mean_absolute_error: 5.5517\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 57.72482\n",
      "Epoch 76/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.8054 - mean_absolute_error: 10.6419 - val_loss: 64.8813 - val_mean_absolute_error: 5.3418\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 57.72482\n",
      "Epoch 77/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 214.6019 - mean_absolute_error: 11.1426 - val_loss: 62.6511 - val_mean_absolute_error: 4.3059\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 57.72482\n",
      "Epoch 78/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 203.8976 - mean_absolute_error: 10.7645 - val_loss: 63.7728 - val_mean_absolute_error: 4.3968\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 57.72482\n",
      "Epoch 79/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 214.1235 - mean_absolute_error: 11.1096 - val_loss: 65.4482 - val_mean_absolute_error: 5.3206\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 57.72482\n",
      "Epoch 80/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.4017 - mean_absolute_error: 10.8286 - val_loss: 61.7488 - val_mean_absolute_error: 4.8805\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 57.72482\n",
      "Epoch 81/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 223.8819 - mean_absolute_error: 11.4129 - val_loss: 57.5952 - val_mean_absolute_error: 3.7719\n",
      "\n",
      "Epoch 00081: val_loss improved from 57.72482 to 57.59518, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 82/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 203.2229 - mean_absolute_error: 11.2315 - val_loss: 57.1374 - val_mean_absolute_error: 4.0873\n",
      "\n",
      "Epoch 00082: val_loss improved from 57.59518 to 57.13738, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 83/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 213.9122 - mean_absolute_error: 11.1370 - val_loss: 65.5165 - val_mean_absolute_error: 5.4308\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 57.13738\n",
      "Epoch 84/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.4074 - mean_absolute_error: 10.9183 - val_loss: 66.1021 - val_mean_absolute_error: 5.6103\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 57.13738\n",
      "Epoch 85/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.0360 - mean_absolute_error: 11.1994 - val_loss: 61.6024 - val_mean_absolute_error: 4.6896\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 57.13738\n",
      "Epoch 86/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.8243 - mean_absolute_error: 11.3145 - val_loss: 59.5176 - val_mean_absolute_error: 4.8239\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 57.13738\n",
      "Epoch 87/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 223.7683 - mean_absolute_error: 11.2843 - val_loss: 69.2401 - val_mean_absolute_error: 6.1239\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 57.13738\n",
      "Epoch 88/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.1715 - mean_absolute_error: 11.0318 - val_loss: 50.5570 - val_mean_absolute_error: 3.7208\n",
      "\n",
      "Epoch 00088: val_loss improved from 57.13738 to 50.55700, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 89/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 242.7385 - mean_absolute_error: 12.0797 - val_loss: 56.7943 - val_mean_absolute_error: 4.5722\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 50.55700\n",
      "Epoch 90/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.6069 - mean_absolute_error: 10.9072 - val_loss: 52.7922 - val_mean_absolute_error: 4.3474\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 50.55700\n",
      "Epoch 91/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 229.5242 - mean_absolute_error: 11.4014 - val_loss: 63.7759 - val_mean_absolute_error: 5.3265\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 50.55700\n",
      "Epoch 92/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.2194 - mean_absolute_error: 11.0470 - val_loss: 62.6482 - val_mean_absolute_error: 4.5309\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 50.55700\n",
      "Epoch 93/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 218.3761 - mean_absolute_error: 11.2393 - val_loss: 78.0315 - val_mean_absolute_error: 6.6927\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 50.55700\n",
      "Epoch 94/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 230.1717 - mean_absolute_error: 11.4937 - val_loss: 87.9595 - val_mean_absolute_error: 7.3750\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 50.55700\n",
      "Epoch 95/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 257.0170 - mean_absolute_error: 11.7927 - val_loss: 59.8252 - val_mean_absolute_error: 4.2213\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 50.55700\n",
      "Epoch 96/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 255.2708 - mean_absolute_error: 12.2665 - val_loss: 53.9060 - val_mean_absolute_error: 3.5277\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 50.55700\n",
      "Epoch 97/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 220.9179 - mean_absolute_error: 11.5980 - val_loss: 56.2670 - val_mean_absolute_error: 4.5447\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 50.55700\n",
      "Epoch 98/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.2954 - mean_absolute_error: 10.8195 - val_loss: 61.0452 - val_mean_absolute_error: 5.2793\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 50.55700\n",
      "Epoch 99/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.4932 - mean_absolute_error: 10.9514 - val_loss: 53.2322 - val_mean_absolute_error: 4.5876\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 50.55700\n",
      "Epoch 100/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 220.1463 - mean_absolute_error: 11.3557 - val_loss: 51.7207 - val_mean_absolute_error: 4.1762\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 50.55700\n",
      "Epoch 101/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 236.8675 - mean_absolute_error: 11.8831 - val_loss: 51.0556 - val_mean_absolute_error: 4.0638\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 50.55700\n",
      "Epoch 102/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.8001 - mean_absolute_error: 11.1655 - val_loss: 54.8531 - val_mean_absolute_error: 3.8120\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 50.55700\n",
      "Epoch 103/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.4170 - mean_absolute_error: 10.9716 - val_loss: 51.8102 - val_mean_absolute_error: 3.9479\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 50.55700\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 213.5968 - mean_absolute_error: 11.2442 - val_loss: 74.4434 - val_mean_absolute_error: 6.7610\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 50.55700\n",
      "Epoch 105/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 218.4182 - mean_absolute_error: 11.3950 - val_loss: 68.9555 - val_mean_absolute_error: 6.2430\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 50.55700\n",
      "Epoch 106/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 202.0828 - mean_absolute_error: 10.7022 - val_loss: 58.3478 - val_mean_absolute_error: 5.2262\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 50.55700\n",
      "Epoch 107/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.7126 - mean_absolute_error: 10.9894 - val_loss: 47.2759 - val_mean_absolute_error: 4.2773\n",
      "\n",
      "Epoch 00107: val_loss improved from 50.55700 to 47.27590, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 108/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.0643 - mean_absolute_error: 10.8934 - val_loss: 60.8836 - val_mean_absolute_error: 5.7089\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 47.27590\n",
      "Epoch 109/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.1393 - mean_absolute_error: 10.7929 - val_loss: 47.6969 - val_mean_absolute_error: 3.8654\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 47.27590\n",
      "Epoch 110/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.1993 - mean_absolute_error: 11.0300 - val_loss: 47.6207 - val_mean_absolute_error: 3.7094\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 47.27590\n",
      "Epoch 111/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.8133 - mean_absolute_error: 10.8550 - val_loss: 48.7886 - val_mean_absolute_error: 4.2393\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 47.27590\n",
      "Epoch 112/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.6906 - mean_absolute_error: 10.3844 - val_loss: 50.9003 - val_mean_absolute_error: 4.5828\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 47.27590\n",
      "Epoch 113/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.3573 - mean_absolute_error: 10.8818 - val_loss: 49.1329 - val_mean_absolute_error: 4.1545\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 47.27590\n",
      "Epoch 114/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.6613 - mean_absolute_error: 10.7016 - val_loss: 49.5358 - val_mean_absolute_error: 4.5942\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 47.27590\n",
      "Epoch 115/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.7487 - mean_absolute_error: 10.7358 - val_loss: 44.9982 - val_mean_absolute_error: 4.1896\n",
      "\n",
      "Epoch 00115: val_loss improved from 47.27590 to 44.99821, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 116/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.6014 - mean_absolute_error: 10.7548 - val_loss: 52.2864 - val_mean_absolute_error: 5.0516\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 44.99821\n",
      "Epoch 117/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.9450 - mean_absolute_error: 10.1013 - val_loss: 45.5387 - val_mean_absolute_error: 4.3825\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 44.99821\n",
      "Epoch 118/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 202.5576 - mean_absolute_error: 10.8594 - val_loss: 41.8345 - val_mean_absolute_error: 3.8909\n",
      "\n",
      "Epoch 00118: val_loss improved from 44.99821 to 41.83454, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 119/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 214.6138 - mean_absolute_error: 11.4156 - val_loss: 45.7803 - val_mean_absolute_error: 4.4926\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 41.83454\n",
      "Epoch 120/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.0771 - mean_absolute_error: 11.0547 - val_loss: 43.2927 - val_mean_absolute_error: 3.9729\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 41.83454\n",
      "Epoch 121/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.9323 - mean_absolute_error: 11.2824 - val_loss: 46.5273 - val_mean_absolute_error: 4.1092\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 41.83454\n",
      "Epoch 122/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 221.9694 - mean_absolute_error: 11.5061 - val_loss: 41.7045 - val_mean_absolute_error: 3.9449\n",
      "\n",
      "Epoch 00122: val_loss improved from 41.83454 to 41.70453, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 123/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.0285 - mean_absolute_error: 11.1337 - val_loss: 47.8779 - val_mean_absolute_error: 4.5948\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 41.70453\n",
      "Epoch 124/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 218.4924 - mean_absolute_error: 11.5167 - val_loss: 48.3301 - val_mean_absolute_error: 4.0716\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 41.70453\n",
      "Epoch 125/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 217.7111 - mean_absolute_error: 11.1198 - val_loss: 42.6472 - val_mean_absolute_error: 3.4044\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 41.70453\n",
      "Epoch 126/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.2571 - mean_absolute_error: 11.0917 - val_loss: 43.3690 - val_mean_absolute_error: 3.5763\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 41.70453\n",
      "Epoch 127/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.6432 - mean_absolute_error: 10.6092 - val_loss: 39.1339 - val_mean_absolute_error: 3.7025\n",
      "\n",
      "Epoch 00127: val_loss improved from 41.70453 to 39.13390, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 128/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.7450 - mean_absolute_error: 10.7426 - val_loss: 48.2443 - val_mean_absolute_error: 5.0574\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 39.13390\n",
      "Epoch 129/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.5090 - mean_absolute_error: 11.0700 - val_loss: 62.9649 - val_mean_absolute_error: 6.0588\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 39.13390\n",
      "Epoch 130/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.1170 - mean_absolute_error: 11.0167 - val_loss: 49.1274 - val_mean_absolute_error: 5.1049\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 39.13390\n",
      "Epoch 131/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 202.9570 - mean_absolute_error: 11.0559 - val_loss: 41.2501 - val_mean_absolute_error: 4.1985\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 39.13390\n",
      "Epoch 132/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.3443 - mean_absolute_error: 10.8007 - val_loss: 40.2281 - val_mean_absolute_error: 4.0110\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 39.13390\n",
      "Epoch 133/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.1232 - mean_absolute_error: 10.7503 - val_loss: 40.9965 - val_mean_absolute_error: 3.9127\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 39.13390\n",
      "Epoch 134/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 218.1884 - mean_absolute_error: 11.3211 - val_loss: 32.1085 - val_mean_absolute_error: 3.3926\n",
      "\n",
      "Epoch 00134: val_loss improved from 39.13390 to 32.10850, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 135/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.8720 - mean_absolute_error: 11.0885 - val_loss: 32.3850 - val_mean_absolute_error: 3.5616\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 32.10850\n",
      "Epoch 136/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 221.8822 - mean_absolute_error: 11.4972 - val_loss: 32.6142 - val_mean_absolute_error: 3.5936\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 32.10850\n",
      "Epoch 137/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.8825 - mean_absolute_error: 10.0079 - val_loss: 35.0922 - val_mean_absolute_error: 4.0392\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 32.10850\n",
      "Epoch 138/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 189.3711 - mean_absolute_error: 10.5123 - val_loss: 33.8110 - val_mean_absolute_error: 4.1981\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 32.10850\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 191.6891 - mean_absolute_error: 10.8331 - val_loss: 38.3304 - val_mean_absolute_error: 4.6765\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 32.10850\n",
      "Epoch 140/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 198.4965 - mean_absolute_error: 10.9687 - val_loss: 33.3508 - val_mean_absolute_error: 4.0485\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 32.10850\n",
      "Epoch 141/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 198.8787 - mean_absolute_error: 11.0208 - val_loss: 40.1682 - val_mean_absolute_error: 4.3522\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 32.10850\n",
      "Epoch 142/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.2384 - mean_absolute_error: 10.2315 - val_loss: 49.5899 - val_mean_absolute_error: 5.2562\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 32.10850\n",
      "Epoch 143/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.1167 - mean_absolute_error: 10.8215 - val_loss: 59.7987 - val_mean_absolute_error: 5.7988\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 32.10850\n",
      "Epoch 144/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 242.8827 - mean_absolute_error: 11.8933 - val_loss: 49.6042 - val_mean_absolute_error: 5.1798\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 32.10850\n",
      "Epoch 145/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 211.1096 - mean_absolute_error: 11.2031 - val_loss: 49.8294 - val_mean_absolute_error: 5.0342\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 32.10850\n",
      "Epoch 146/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 180.4821 - mean_absolute_error: 10.4434 - val_loss: 37.7945 - val_mean_absolute_error: 3.8472\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 32.10850\n",
      "Epoch 147/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.7515 - mean_absolute_error: 10.7396 - val_loss: 39.1972 - val_mean_absolute_error: 4.6386\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 32.10850\n",
      "Epoch 148/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.3185 - mean_absolute_error: 10.1157 - val_loss: 42.9582 - val_mean_absolute_error: 4.7245\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 32.10850\n",
      "Epoch 149/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.3512 - mean_absolute_error: 10.0679 - val_loss: 35.6800 - val_mean_absolute_error: 4.3591\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 32.10850\n",
      "Epoch 150/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.8890 - mean_absolute_error: 10.6432 - val_loss: 34.2592 - val_mean_absolute_error: 4.0134\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 32.10850\n",
      "Epoch 151/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 202.6930 - mean_absolute_error: 11.1591 - val_loss: 31.2097 - val_mean_absolute_error: 3.8376\n",
      "\n",
      "Epoch 00151: val_loss improved from 32.10850 to 31.20965, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 152/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.4750 - mean_absolute_error: 10.2660 - val_loss: 29.8479 - val_mean_absolute_error: 3.5476\n",
      "\n",
      "Epoch 00152: val_loss improved from 31.20965 to 29.84793, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 153/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 203.6505 - mean_absolute_error: 10.7538 - val_loss: 45.5472 - val_mean_absolute_error: 5.1107\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 29.84793\n",
      "Epoch 154/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.7046 - mean_absolute_error: 11.1149 - val_loss: 41.3015 - val_mean_absolute_error: 4.5704\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 29.84793\n",
      "Epoch 155/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.5286 - mean_absolute_error: 10.7783 - val_loss: 38.9889 - val_mean_absolute_error: 4.6607\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 29.84793\n",
      "Epoch 156/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.8577 - mean_absolute_error: 10.5819 - val_loss: 33.8068 - val_mean_absolute_error: 4.0982\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 29.84793\n",
      "Epoch 157/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.7133 - mean_absolute_error: 10.7943 - val_loss: 29.6380 - val_mean_absolute_error: 3.9111\n",
      "\n",
      "Epoch 00157: val_loss improved from 29.84793 to 29.63797, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 158/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.6799 - mean_absolute_error: 10.3773 - val_loss: 33.6447 - val_mean_absolute_error: 4.2499\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 29.63797\n",
      "Epoch 159/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.6618 - mean_absolute_error: 10.7991 - val_loss: 32.5974 - val_mean_absolute_error: 3.8635\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 29.63797\n",
      "Epoch 160/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 194.1504 - mean_absolute_error: 10.7715 - val_loss: 30.2658 - val_mean_absolute_error: 3.4779\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 29.63797\n",
      "Epoch 161/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.6174 - mean_absolute_error: 10.4379 - val_loss: 29.3899 - val_mean_absolute_error: 3.9087\n",
      "\n",
      "Epoch 00161: val_loss improved from 29.63797 to 29.38987, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 162/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.5300 - mean_absolute_error: 10.9854 - val_loss: 41.0873 - val_mean_absolute_error: 4.8897\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 29.38987\n",
      "Epoch 163/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.3955 - mean_absolute_error: 10.8383 - val_loss: 44.8315 - val_mean_absolute_error: 4.1786\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 29.38987\n",
      "Epoch 164/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.6822 - mean_absolute_error: 11.2801 - val_loss: 28.0822 - val_mean_absolute_error: 3.7272\n",
      "\n",
      "Epoch 00164: val_loss improved from 29.38987 to 28.08217, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 165/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.6787 - mean_absolute_error: 10.6203 - val_loss: 23.1846 - val_mean_absolute_error: 3.1930\n",
      "\n",
      "Epoch 00165: val_loss improved from 28.08217 to 23.18458, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 166/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 181.3493 - mean_absolute_error: 10.3661 - val_loss: 27.9020 - val_mean_absolute_error: 3.4040\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 23.18458\n",
      "Epoch 167/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 173.8847 - mean_absolute_error: 10.3166 - val_loss: 28.9485 - val_mean_absolute_error: 3.3339\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 23.18458\n",
      "Epoch 168/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.8885 - mean_absolute_error: 10.7085 - val_loss: 36.5124 - val_mean_absolute_error: 3.5514\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 23.18458\n",
      "Epoch 169/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.0537 - mean_absolute_error: 10.8018 - val_loss: 42.2542 - val_mean_absolute_error: 5.1518\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 23.18458\n",
      "Epoch 170/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 194.8890 - mean_absolute_error: 10.6470 - val_loss: 31.3038 - val_mean_absolute_error: 4.0316\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 23.18458\n",
      "Epoch 171/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 169.6341 - mean_absolute_error: 9.9089 - val_loss: 28.4033 - val_mean_absolute_error: 3.7930\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 23.18458\n",
      "Epoch 172/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.2789 - mean_absolute_error: 11.0113 - val_loss: 35.7861 - val_mean_absolute_error: 4.6396\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 23.18458\n",
      "Epoch 173/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.7743 - mean_absolute_error: 10.0612 - val_loss: 22.7394 - val_mean_absolute_error: 3.3591\n",
      "\n",
      "Epoch 00173: val_loss improved from 23.18458 to 22.73936, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 174/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 188.5526 - mean_absolute_error: 10.4858 - val_loss: 22.0733 - val_mean_absolute_error: 3.0821\n",
      "\n",
      "Epoch 00174: val_loss improved from 22.73936 to 22.07334, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 175/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 206.7278 - mean_absolute_error: 11.2205 - val_loss: 20.4680 - val_mean_absolute_error: 3.0655\n",
      "\n",
      "Epoch 00175: val_loss improved from 22.07334 to 20.46805, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 176/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 176.2573 - mean_absolute_error: 10.3340 - val_loss: 30.3342 - val_mean_absolute_error: 4.5273\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 20.46805\n",
      "Epoch 177/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.1512 - mean_absolute_error: 9.8736 - val_loss: 26.6348 - val_mean_absolute_error: 3.7940\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 20.46805\n",
      "Epoch 178/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.1036 - mean_absolute_error: 10.8564 - val_loss: 22.9417 - val_mean_absolute_error: 3.4595\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 20.46805\n",
      "Epoch 179/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.1916 - mean_absolute_error: 10.6193 - val_loss: 34.6296 - val_mean_absolute_error: 4.5260\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 20.46805\n",
      "Epoch 180/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.7759 - mean_absolute_error: 10.3739 - val_loss: 39.3542 - val_mean_absolute_error: 4.8527\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 20.46805\n",
      "Epoch 181/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 175.1996 - mean_absolute_error: 10.1344 - val_loss: 33.7465 - val_mean_absolute_error: 4.1747\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 20.46805\n",
      "Epoch 182/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.8442 - mean_absolute_error: 10.4229 - val_loss: 30.9144 - val_mean_absolute_error: 3.7740\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 20.46805\n",
      "Epoch 183/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.9978 - mean_absolute_error: 10.8858 - val_loss: 29.1417 - val_mean_absolute_error: 3.5133\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 20.46805\n",
      "Epoch 184/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.4571 - mean_absolute_error: 10.6682 - val_loss: 32.0052 - val_mean_absolute_error: 4.1765\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 20.46805\n",
      "Epoch 185/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.6363 - mean_absolute_error: 10.5042 - val_loss: 37.3286 - val_mean_absolute_error: 4.4805\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 20.46805\n",
      "Epoch 186/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.9451 - mean_absolute_error: 10.3088 - val_loss: 29.7863 - val_mean_absolute_error: 3.8090\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 20.46805\n",
      "Epoch 187/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.1243 - mean_absolute_error: 10.1840 - val_loss: 25.7747 - val_mean_absolute_error: 3.3486\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 20.46805\n",
      "Epoch 188/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.9066 - mean_absolute_error: 9.9027 - val_loss: 24.6234 - val_mean_absolute_error: 3.6956\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 20.46805\n",
      "Epoch 189/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 195.7552 - mean_absolute_error: 10.4870 - val_loss: 23.4340 - val_mean_absolute_error: 3.5104\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 20.46805\n",
      "Epoch 190/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.0954 - mean_absolute_error: 11.4073 - val_loss: 34.8931 - val_mean_absolute_error: 4.8216\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 20.46805\n",
      "Epoch 191/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.6146 - mean_absolute_error: 10.6891 - val_loss: 54.6459 - val_mean_absolute_error: 6.0627\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 20.46805\n",
      "Epoch 192/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.2808 - mean_absolute_error: 11.1816 - val_loss: 36.5971 - val_mean_absolute_error: 3.9776\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 20.46805\n",
      "Epoch 193/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.6437 - mean_absolute_error: 11.0456 - val_loss: 24.9119 - val_mean_absolute_error: 3.4064\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 20.46805\n",
      "Epoch 194/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.3390 - mean_absolute_error: 10.6189 - val_loss: 22.4457 - val_mean_absolute_error: 3.5139\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 20.46805\n",
      "Epoch 195/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.2275 - mean_absolute_error: 10.0606 - val_loss: 18.8819 - val_mean_absolute_error: 3.2102\n",
      "\n",
      "Epoch 00195: val_loss improved from 20.46805 to 18.88195, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 196/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.3067 - mean_absolute_error: 10.5007 - val_loss: 24.8655 - val_mean_absolute_error: 3.8250\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 18.88195\n",
      "Epoch 197/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.5574 - mean_absolute_error: 10.0611 - val_loss: 28.2955 - val_mean_absolute_error: 4.1419\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 18.88195\n",
      "Epoch 198/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.6485 - mean_absolute_error: 10.5031 - val_loss: 30.1667 - val_mean_absolute_error: 4.2248\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 18.88195\n",
      "Epoch 199/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.9615 - mean_absolute_error: 10.9044 - val_loss: 44.0307 - val_mean_absolute_error: 4.5597\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 18.88195\n",
      "Epoch 200/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 233.4968 - mean_absolute_error: 11.6636 - val_loss: 49.1795 - val_mean_absolute_error: 5.0766\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 18.88195\n",
      "Epoch 201/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.7469 - mean_absolute_error: 10.4056 - val_loss: 45.4374 - val_mean_absolute_error: 4.5110\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 18.88195\n",
      "Epoch 202/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.9087 - mean_absolute_error: 10.8982 - val_loss: 45.5130 - val_mean_absolute_error: 4.8555\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 18.88195\n",
      "Epoch 203/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 194.7364 - mean_absolute_error: 10.6668 - val_loss: 48.4859 - val_mean_absolute_error: 5.2272\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 18.88195\n",
      "Epoch 204/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.2441 - mean_absolute_error: 10.2704 - val_loss: 40.6267 - val_mean_absolute_error: 4.9520\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 18.88195\n",
      "Epoch 205/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.4651 - mean_absolute_error: 10.5605 - val_loss: 43.6876 - val_mean_absolute_error: 5.2606\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 18.88195\n",
      "Epoch 206/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.1728 - mean_absolute_error: 10.4799 - val_loss: 47.2735 - val_mean_absolute_error: 5.6950\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 18.88195\n",
      "Epoch 207/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.3866 - mean_absolute_error: 10.1807 - val_loss: 40.0425 - val_mean_absolute_error: 5.0539\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 18.88195\n",
      "Epoch 208/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.9487 - mean_absolute_error: 10.3838 - val_loss: 20.7896 - val_mean_absolute_error: 2.8653\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 18.88195\n",
      "Epoch 209/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 173.2684 - mean_absolute_error: 10.3260 - val_loss: 25.5338 - val_mean_absolute_error: 3.6207\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 18.88195\n",
      "Epoch 210/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 198.0942 - mean_absolute_error: 10.9618 - val_loss: 17.8492 - val_mean_absolute_error: 2.8653\n",
      "\n",
      "Epoch 00210: val_loss improved from 18.88195 to 17.84924, saving model to LSTM_Interval_best_VFI.hdf5\n",
      "Epoch 211/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.1387 - mean_absolute_error: 10.8274 - val_loss: 24.8031 - val_mean_absolute_error: 3.3615\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 17.84924\n",
      "Epoch 212/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.6773 - mean_absolute_error: 10.3265 - val_loss: 36.5138 - val_mean_absolute_error: 4.7398\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 17.84924\n",
      "Epoch 213/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.5127 - mean_absolute_error: 9.8559 - val_loss: 25.6732 - val_mean_absolute_error: 3.7088\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 17.84924\n",
      "Epoch 214/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.5831 - mean_absolute_error: 10.4783 - val_loss: 23.1103 - val_mean_absolute_error: 3.1674\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 17.84924\n",
      "Epoch 215/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.7051 - mean_absolute_error: 10.7018 - val_loss: 23.9811 - val_mean_absolute_error: 3.0450\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 17.84924\n",
      "Epoch 216/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.0742 - mean_absolute_error: 10.6199 - val_loss: 22.1442 - val_mean_absolute_error: 3.0544\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 17.84924\n",
      "Epoch 217/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.2022 - mean_absolute_error: 10.1337 - val_loss: 33.2667 - val_mean_absolute_error: 4.7349\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 17.84924\n",
      "Epoch 218/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.9783 - mean_absolute_error: 9.9864 - val_loss: 25.6369 - val_mean_absolute_error: 3.5287\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 17.84924\n",
      "Epoch 219/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.9997 - mean_absolute_error: 10.2850 - val_loss: 25.6063 - val_mean_absolute_error: 3.3527\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 17.84924\n",
      "Epoch 220/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.8089 - mean_absolute_error: 10.6827 - val_loss: 34.4160 - val_mean_absolute_error: 4.5440\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 17.84924\n",
      "Epoch 221/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.7903 - mean_absolute_error: 10.4816 - val_loss: 22.3425 - val_mean_absolute_error: 3.4205\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 17.84924\n",
      "Epoch 222/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.5152 - mean_absolute_error: 9.5501 - val_loss: 39.6815 - val_mean_absolute_error: 4.6972\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 17.84924\n",
      "Epoch 223/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.1622 - mean_absolute_error: 11.0780 - val_loss: 26.2290 - val_mean_absolute_error: 3.9982\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 17.84924\n",
      "Epoch 224/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.6376 - mean_absolute_error: 11.0150 - val_loss: 31.9230 - val_mean_absolute_error: 3.4546\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 17.84924\n",
      "Epoch 225/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.3580 - mean_absolute_error: 11.2618 - val_loss: 24.2270 - val_mean_absolute_error: 3.5257\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 17.84924\n",
      "Epoch 226/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 180.4855 - mean_absolute_error: 10.4439 - val_loss: 47.3182 - val_mean_absolute_error: 5.8894\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 17.84924\n",
      "Epoch 227/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.0951 - mean_absolute_error: 10.8361 - val_loss: 25.8111 - val_mean_absolute_error: 3.6751\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 17.84924\n",
      "Epoch 228/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.9943 - mean_absolute_error: 10.5466 - val_loss: 24.6047 - val_mean_absolute_error: 3.8451\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 17.84924\n",
      "Epoch 229/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.7082 - mean_absolute_error: 9.9264 - val_loss: 22.4896 - val_mean_absolute_error: 3.0987\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 17.84924\n",
      "Epoch 230/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.7359 - mean_absolute_error: 10.5936 - val_loss: 24.3669 - val_mean_absolute_error: 3.3173\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 17.84924\n",
      "Epoch 231/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 173.3367 - mean_absolute_error: 10.1722 - val_loss: 38.9435 - val_mean_absolute_error: 4.7014\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 17.84924\n",
      "Epoch 232/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.9901 - mean_absolute_error: 10.8693 - val_loss: 35.0873 - val_mean_absolute_error: 4.2638\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 17.84924\n",
      "Epoch 233/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.1058 - mean_absolute_error: 10.5003 - val_loss: 28.1846 - val_mean_absolute_error: 4.0156\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 17.84924\n",
      "Epoch 234/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.8926 - mean_absolute_error: 10.4096 - val_loss: 34.8599 - val_mean_absolute_error: 4.6438\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 17.84924\n",
      "Epoch 235/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.2738 - mean_absolute_error: 10.5053 - val_loss: 22.6903 - val_mean_absolute_error: 3.4466\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 17.84924\n",
      "Epoch 236/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.6638 - mean_absolute_error: 10.7342 - val_loss: 53.5328 - val_mean_absolute_error: 5.3843\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 17.84924\n",
      "Epoch 237/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 239.8909 - mean_absolute_error: 11.7558 - val_loss: 49.0619 - val_mean_absolute_error: 4.7529\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 17.84924\n",
      "Epoch 238/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 211.2888 - mean_absolute_error: 10.7668 - val_loss: 44.8407 - val_mean_absolute_error: 5.5055\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 17.84924\n",
      "Epoch 239/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.3366 - mean_absolute_error: 10.8232 - val_loss: 34.2649 - val_mean_absolute_error: 4.1518\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 17.84924\n",
      "Epoch 240/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.9369 - mean_absolute_error: 11.0382 - val_loss: 38.8584 - val_mean_absolute_error: 4.9194\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 17.84924\n",
      "Epoch 241/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.2319 - mean_absolute_error: 10.3126 - val_loss: 30.7745 - val_mean_absolute_error: 3.7137\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 17.84924\n",
      "Epoch 242/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 214.6617 - mean_absolute_error: 11.4864 - val_loss: 33.3457 - val_mean_absolute_error: 4.2734\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 17.84924\n",
      "Epoch 243/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 175.8679 - mean_absolute_error: 10.5653 - val_loss: 24.0212 - val_mean_absolute_error: 3.6699\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 17.84924\n",
      "Epoch 244/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.8457 - mean_absolute_error: 10.5687 - val_loss: 26.4990 - val_mean_absolute_error: 3.7396\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 17.84924\n",
      "Epoch 245/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.7518 - mean_absolute_error: 10.1613 - val_loss: 23.4534 - val_mean_absolute_error: 3.6022\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 17.84924\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 169.7622 - mean_absolute_error: 10.1772 - val_loss: 21.4057 - val_mean_absolute_error: 3.4424\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 17.84924\n",
      "Epoch 247/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.7128 - mean_absolute_error: 10.5030 - val_loss: 25.0520 - val_mean_absolute_error: 3.6804\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 17.84924\n",
      "Epoch 248/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.2035 - mean_absolute_error: 10.2532 - val_loss: 35.1789 - val_mean_absolute_error: 4.2358\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 17.84924\n",
      "Epoch 249/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.1341 - mean_absolute_error: 10.0920 - val_loss: 25.2209 - val_mean_absolute_error: 3.4821\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 17.84924\n",
      "Epoch 250/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.5784 - mean_absolute_error: 10.2248 - val_loss: 22.3838 - val_mean_absolute_error: 2.9980\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 17.84924\n",
      "Epoch 251/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.8182 - mean_absolute_error: 10.8919 - val_loss: 21.0030 - val_mean_absolute_error: 3.4438\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 17.84924\n",
      "Epoch 252/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.2569 - mean_absolute_error: 10.2957 - val_loss: 49.7666 - val_mean_absolute_error: 5.5227\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 17.84924\n",
      "Epoch 253/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.9699 - mean_absolute_error: 10.4875 - val_loss: 27.8977 - val_mean_absolute_error: 3.8754\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 17.84924\n",
      "Epoch 254/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 176.8157 - mean_absolute_error: 10.3261 - val_loss: 21.5807 - val_mean_absolute_error: 3.3434\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 17.84924\n",
      "Epoch 255/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.0233 - mean_absolute_error: 10.1933 - val_loss: 29.4338 - val_mean_absolute_error: 3.6939\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 17.84924\n",
      "Epoch 256/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.0909 - mean_absolute_error: 9.7921 - val_loss: 26.8708 - val_mean_absolute_error: 3.8038\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 17.84924\n",
      "Epoch 257/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.0806 - mean_absolute_error: 10.3004 - val_loss: 31.6160 - val_mean_absolute_error: 4.4067\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 17.84924\n",
      "Epoch 258/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.5934 - mean_absolute_error: 10.2358 - val_loss: 20.7587 - val_mean_absolute_error: 2.8728\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 17.84924\n",
      "Epoch 259/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.5152 - mean_absolute_error: 10.5718 - val_loss: 20.4849 - val_mean_absolute_error: 3.3087\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 17.84924\n",
      "Epoch 260/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.2548 - mean_absolute_error: 11.2789 - val_loss: 22.8462 - val_mean_absolute_error: 3.6303\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 17.84924\n",
      "Epoch 00260: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a389f2898>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_baseline(X,y_VFI,\"VFI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/500\n",
      "522/522 [==============================] - 4s 7ms/step - loss: 4702.6889 - mean_absolute_error: 67.8814 - val_loss: 4087.2674 - val_mean_absolute_error: 63.2164\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4087.26741, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 2/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3729.2996 - mean_absolute_error: 60.2235 - val_loss: 3072.0769 - val_mean_absolute_error: 54.6112\n",
      "\n",
      "Epoch 00002: val_loss improved from 4087.26741 to 3072.07686, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 3/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2919.0866 - mean_absolute_error: 53.0583 - val_loss: 2425.4521 - val_mean_absolute_error: 48.3042\n",
      "\n",
      "Epoch 00003: val_loss improved from 3072.07686 to 2425.45213, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 4/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2296.5176 - mean_absolute_error: 46.6750 - val_loss: 1845.9978 - val_mean_absolute_error: 41.8797\n",
      "\n",
      "Epoch 00004: val_loss improved from 2425.45213 to 1845.99779, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 5/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1763.8948 - mean_absolute_error: 40.5449 - val_loss: 1331.3177 - val_mean_absolute_error: 35.2036\n",
      "\n",
      "Epoch 00005: val_loss improved from 1845.99779 to 1331.31773, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 6/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1265.2888 - mean_absolute_error: 33.5006 - val_loss: 905.8572 - val_mean_absolute_error: 28.5267\n",
      "\n",
      "Epoch 00006: val_loss improved from 1331.31773 to 905.85721, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 7/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 828.5315 - mean_absolute_error: 26.3782 - val_loss: 581.5847 - val_mean_absolute_error: 22.1877\n",
      "\n",
      "Epoch 00007: val_loss improved from 905.85721 to 581.58465, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 8/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 596.3203 - mean_absolute_error: 21.2694 - val_loss: 357.3802 - val_mean_absolute_error: 16.6975\n",
      "\n",
      "Epoch 00008: val_loss improved from 581.58465 to 357.38024, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 9/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 408.3845 - mean_absolute_error: 16.9846 - val_loss: 219.4203 - val_mean_absolute_error: 12.5965\n",
      "\n",
      "Epoch 00009: val_loss improved from 357.38024 to 219.42028, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 10/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 297.5777 - mean_absolute_error: 13.7929 - val_loss: 146.0850 - val_mean_absolute_error: 9.8635\n",
      "\n",
      "Epoch 00010: val_loss improved from 219.42028 to 146.08499, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 11/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 237.7265 - mean_absolute_error: 12.3813 - val_loss: 111.3984 - val_mean_absolute_error: 8.5043\n",
      "\n",
      "Epoch 00011: val_loss improved from 146.08499 to 111.39843, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 12/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.7425 - mean_absolute_error: 11.2943 - val_loss: 98.6014 - val_mean_absolute_error: 7.9576\n",
      "\n",
      "Epoch 00012: val_loss improved from 111.39843 to 98.60143, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 13/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 203.9472 - mean_absolute_error: 11.3306 - val_loss: 94.0209 - val_mean_absolute_error: 7.7352\n",
      "\n",
      "Epoch 00013: val_loss improved from 98.60143 to 94.02091, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 14/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.1492 - mean_absolute_error: 10.6101 - val_loss: 92.6850 - val_mean_absolute_error: 7.6579\n",
      "\n",
      "Epoch 00014: val_loss improved from 94.02091 to 92.68502, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 15/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.0350 - mean_absolute_error: 11.3032 - val_loss: 91.9639 - val_mean_absolute_error: 7.6198\n",
      "\n",
      "Epoch 00015: val_loss improved from 92.68502 to 91.96390, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 16/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.8842 - mean_absolute_error: 10.7795 - val_loss: 91.4648 - val_mean_absolute_error: 7.5916\n",
      "\n",
      "Epoch 00016: val_loss improved from 91.96390 to 91.46481, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 17/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.2468 - mean_absolute_error: 11.1674 - val_loss: 91.3540 - val_mean_absolute_error: 7.5899\n",
      "\n",
      "Epoch 00017: val_loss improved from 91.46481 to 91.35401, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 18/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.0826 - mean_absolute_error: 10.8120 - val_loss: 90.8929 - val_mean_absolute_error: 7.5590\n",
      "\n",
      "Epoch 00018: val_loss improved from 91.35401 to 90.89287, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 19/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.3749 - mean_absolute_error: 11.2993 - val_loss: 90.6856 - val_mean_absolute_error: 7.5500\n",
      "\n",
      "Epoch 00019: val_loss improved from 90.89287 to 90.68562, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 20/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.2763 - mean_absolute_error: 11.2262 - val_loss: 90.5817 - val_mean_absolute_error: 7.5466\n",
      "\n",
      "Epoch 00020: val_loss improved from 90.68562 to 90.58167, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 21/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.8403 - mean_absolute_error: 11.1699 - val_loss: 90.5796 - val_mean_absolute_error: 7.5547\n",
      "\n",
      "Epoch 00021: val_loss improved from 90.58167 to 90.57962, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 22/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 213.5173 - mean_absolute_error: 11.4906 - val_loss: 90.2250 - val_mean_absolute_error: 7.5372\n",
      "\n",
      "Epoch 00022: val_loss improved from 90.57962 to 90.22505, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 23/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.9283 - mean_absolute_error: 11.7162 - val_loss: 89.6967 - val_mean_absolute_error: 7.5041\n",
      "\n",
      "Epoch 00023: val_loss improved from 90.22505 to 89.69669, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 24/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.6045 - mean_absolute_error: 11.0683 - val_loss: 88.8401 - val_mean_absolute_error: 7.4710\n",
      "\n",
      "Epoch 00024: val_loss improved from 89.69669 to 88.84009, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 25/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.7129 - mean_absolute_error: 11.0887 - val_loss: 86.3071 - val_mean_absolute_error: 7.3570\n",
      "\n",
      "Epoch 00025: val_loss improved from 88.84009 to 86.30712, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 26/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.9274 - mean_absolute_error: 11.0565 - val_loss: 87.8648 - val_mean_absolute_error: 7.3860\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 86.30712\n",
      "Epoch 27/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 195.8183 - mean_absolute_error: 11.1218 - val_loss: 83.5966 - val_mean_absolute_error: 7.1853\n",
      "\n",
      "Epoch 00027: val_loss improved from 86.30712 to 83.59660, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 28/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 196.6222 - mean_absolute_error: 11.2382 - val_loss: 86.4544 - val_mean_absolute_error: 7.3045\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 83.59660\n",
      "Epoch 29/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.9272 - mean_absolute_error: 10.4559 - val_loss: 85.0929 - val_mean_absolute_error: 7.2390\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 83.59660\n",
      "Epoch 30/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.2741 - mean_absolute_error: 11.1879 - val_loss: 80.0473 - val_mean_absolute_error: 6.9944\n",
      "\n",
      "Epoch 00030: val_loss improved from 83.59660 to 80.04733, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 177.4426 - mean_absolute_error: 10.5467 - val_loss: 78.5502 - val_mean_absolute_error: 6.9685\n",
      "\n",
      "Epoch 00031: val_loss improved from 80.04733 to 78.55020, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 32/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.7126 - mean_absolute_error: 10.3901 - val_loss: 63.8795 - val_mean_absolute_error: 6.0960\n",
      "\n",
      "Epoch 00032: val_loss improved from 78.55020 to 63.87947, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 33/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.1903 - mean_absolute_error: 10.0251 - val_loss: 60.4713 - val_mean_absolute_error: 6.0739\n",
      "\n",
      "Epoch 00033: val_loss improved from 63.87947 to 60.47131, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 34/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.7921 - mean_absolute_error: 10.4662 - val_loss: 43.9161 - val_mean_absolute_error: 4.8694\n",
      "\n",
      "Epoch 00034: val_loss improved from 60.47131 to 43.91609, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 35/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.3350 - mean_absolute_error: 9.4549 - val_loss: 37.6981 - val_mean_absolute_error: 4.4300\n",
      "\n",
      "Epoch 00035: val_loss improved from 43.91609 to 37.69809, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 36/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.0127 - mean_absolute_error: 9.5046 - val_loss: 35.8750 - val_mean_absolute_error: 4.5325\n",
      "\n",
      "Epoch 00036: val_loss improved from 37.69809 to 35.87495, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 37/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.8144 - mean_absolute_error: 9.9897 - val_loss: 32.1108 - val_mean_absolute_error: 4.1171\n",
      "\n",
      "Epoch 00037: val_loss improved from 35.87495 to 32.11076, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 38/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 138.5138 - mean_absolute_error: 9.1797 - val_loss: 37.6631 - val_mean_absolute_error: 4.5499\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 32.11076\n",
      "Epoch 39/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.0623 - mean_absolute_error: 10.0561 - val_loss: 40.2495 - val_mean_absolute_error: 5.0181\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 32.11076\n",
      "Epoch 40/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.3022 - mean_absolute_error: 9.1834 - val_loss: 31.3799 - val_mean_absolute_error: 4.1113\n",
      "\n",
      "Epoch 00040: val_loss improved from 32.11076 to 31.37995, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 41/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.5803 - mean_absolute_error: 9.1015 - val_loss: 30.4217 - val_mean_absolute_error: 4.0448\n",
      "\n",
      "Epoch 00041: val_loss improved from 31.37995 to 30.42175, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 42/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.1513 - mean_absolute_error: 9.6175 - val_loss: 28.6646 - val_mean_absolute_error: 3.8930\n",
      "\n",
      "Epoch 00042: val_loss improved from 30.42175 to 28.66460, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 43/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.0737 - mean_absolute_error: 9.3012 - val_loss: 26.8215 - val_mean_absolute_error: 3.6448\n",
      "\n",
      "Epoch 00043: val_loss improved from 28.66460 to 26.82151, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 44/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.5271 - mean_absolute_error: 8.8812 - val_loss: 28.9618 - val_mean_absolute_error: 4.0334\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 26.82151\n",
      "Epoch 45/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.2596 - mean_absolute_error: 9.4150 - val_loss: 27.9496 - val_mean_absolute_error: 4.0217\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 26.82151\n",
      "Epoch 46/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.8509 - mean_absolute_error: 8.9304 - val_loss: 27.4972 - val_mean_absolute_error: 3.8459\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 26.82151\n",
      "Epoch 47/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.2058 - mean_absolute_error: 9.0344 - val_loss: 26.5341 - val_mean_absolute_error: 3.8171\n",
      "\n",
      "Epoch 00047: val_loss improved from 26.82151 to 26.53405, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 48/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.2372 - mean_absolute_error: 9.4036 - val_loss: 36.8497 - val_mean_absolute_error: 4.8421\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 26.53405\n",
      "Epoch 49/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.0428 - mean_absolute_error: 8.9905 - val_loss: 29.2201 - val_mean_absolute_error: 4.2471\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 26.53405\n",
      "Epoch 50/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.8346 - mean_absolute_error: 9.4544 - val_loss: 22.0313 - val_mean_absolute_error: 3.3111\n",
      "\n",
      "Epoch 00050: val_loss improved from 26.53405 to 22.03134, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 51/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.3238 - mean_absolute_error: 8.7711 - val_loss: 21.1675 - val_mean_absolute_error: 3.2850\n",
      "\n",
      "Epoch 00051: val_loss improved from 22.03134 to 21.16747, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 52/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.8875 - mean_absolute_error: 9.0338 - val_loss: 26.1084 - val_mean_absolute_error: 4.0577\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 21.16747\n",
      "Epoch 53/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.0647 - mean_absolute_error: 9.4231 - val_loss: 26.6455 - val_mean_absolute_error: 4.0332\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 21.16747\n",
      "Epoch 54/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.6654 - mean_absolute_error: 8.7390 - val_loss: 24.2007 - val_mean_absolute_error: 3.6224\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 21.16747\n",
      "Epoch 55/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.3833 - mean_absolute_error: 8.9674 - val_loss: 26.7494 - val_mean_absolute_error: 4.1331\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 21.16747\n",
      "Epoch 56/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.5390 - mean_absolute_error: 8.5504 - val_loss: 32.1249 - val_mean_absolute_error: 4.5882\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 21.16747\n",
      "Epoch 57/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.2111 - mean_absolute_error: 8.6711 - val_loss: 31.2494 - val_mean_absolute_error: 4.5262\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 21.16747\n",
      "Epoch 58/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.4062 - mean_absolute_error: 8.8817 - val_loss: 26.2650 - val_mean_absolute_error: 3.9107\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 21.16747\n",
      "Epoch 59/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.9532 - mean_absolute_error: 8.6918 - val_loss: 21.1081 - val_mean_absolute_error: 3.4318\n",
      "\n",
      "Epoch 00059: val_loss improved from 21.16747 to 21.10809, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 60/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.4901 - mean_absolute_error: 8.6956 - val_loss: 19.8722 - val_mean_absolute_error: 3.1525\n",
      "\n",
      "Epoch 00060: val_loss improved from 21.10809 to 19.87215, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 61/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.2275 - mean_absolute_error: 8.5493 - val_loss: 19.4080 - val_mean_absolute_error: 3.1829\n",
      "\n",
      "Epoch 00061: val_loss improved from 19.87215 to 19.40797, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 62/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.7365 - mean_absolute_error: 8.3234 - val_loss: 21.1959 - val_mean_absolute_error: 3.5529\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 19.40797\n",
      "Epoch 63/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.9312 - mean_absolute_error: 8.3970 - val_loss: 22.9635 - val_mean_absolute_error: 3.7334\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 19.40797\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 114.7295 - mean_absolute_error: 8.4578 - val_loss: 20.2658 - val_mean_absolute_error: 3.3949\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 19.40797\n",
      "Epoch 65/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.0965 - mean_absolute_error: 8.8009 - val_loss: 15.7799 - val_mean_absolute_error: 2.7146\n",
      "\n",
      "Epoch 00065: val_loss improved from 19.40797 to 15.77990, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 66/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.7908 - mean_absolute_error: 8.9403 - val_loss: 16.0870 - val_mean_absolute_error: 2.8691\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 15.77990\n",
      "Epoch 67/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.5913 - mean_absolute_error: 8.5312 - val_loss: 17.4392 - val_mean_absolute_error: 3.0615\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 15.77990\n",
      "Epoch 68/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.5756 - mean_absolute_error: 8.7573 - val_loss: 27.4502 - val_mean_absolute_error: 4.4775\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 15.77990\n",
      "Epoch 69/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.7171 - mean_absolute_error: 8.1425 - val_loss: 25.3301 - val_mean_absolute_error: 4.1440\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 15.77990\n",
      "Epoch 70/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.8663 - mean_absolute_error: 8.7236 - val_loss: 18.1796 - val_mean_absolute_error: 3.2080\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 15.77990\n",
      "Epoch 71/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.6264 - mean_absolute_error: 8.2622 - val_loss: 18.3223 - val_mean_absolute_error: 3.1411\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 15.77990\n",
      "Epoch 72/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.6109 - mean_absolute_error: 8.3880 - val_loss: 16.2897 - val_mean_absolute_error: 2.9640\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 15.77990\n",
      "Epoch 73/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.9691 - mean_absolute_error: 8.1825 - val_loss: 20.5148 - val_mean_absolute_error: 3.6581\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 15.77990\n",
      "Epoch 74/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.5825 - mean_absolute_error: 8.6823 - val_loss: 17.4413 - val_mean_absolute_error: 3.3304\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 15.77990\n",
      "Epoch 75/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.4250 - mean_absolute_error: 8.2476 - val_loss: 19.8486 - val_mean_absolute_error: 3.3293\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 15.77990\n",
      "Epoch 76/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.0534 - mean_absolute_error: 8.9332 - val_loss: 17.5211 - val_mean_absolute_error: 3.2625\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 15.77990\n",
      "Epoch 77/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.4792 - mean_absolute_error: 8.4938 - val_loss: 20.3057 - val_mean_absolute_error: 3.6787\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 15.77990\n",
      "Epoch 78/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.7828 - mean_absolute_error: 9.0342 - val_loss: 14.8916 - val_mean_absolute_error: 2.8556\n",
      "\n",
      "Epoch 00078: val_loss improved from 15.77990 to 14.89159, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 79/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.3443 - mean_absolute_error: 8.5088 - val_loss: 12.4747 - val_mean_absolute_error: 2.5476\n",
      "\n",
      "Epoch 00079: val_loss improved from 14.89159 to 12.47470, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 80/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.0277 - mean_absolute_error: 8.0998 - val_loss: 15.2222 - val_mean_absolute_error: 2.9737\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 12.47470\n",
      "Epoch 81/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.1053 - mean_absolute_error: 8.7383 - val_loss: 17.6707 - val_mean_absolute_error: 3.2162\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 12.47470\n",
      "Epoch 82/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.1930 - mean_absolute_error: 8.6175 - val_loss: 14.6537 - val_mean_absolute_error: 2.8081\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 12.47470\n",
      "Epoch 83/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.6357 - mean_absolute_error: 8.4454 - val_loss: 14.6884 - val_mean_absolute_error: 2.7482\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 12.47470\n",
      "Epoch 84/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.8579 - mean_absolute_error: 8.6598 - val_loss: 21.0794 - val_mean_absolute_error: 3.9118\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 12.47470\n",
      "Epoch 85/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.4633 - mean_absolute_error: 8.5975 - val_loss: 12.2505 - val_mean_absolute_error: 2.4359\n",
      "\n",
      "Epoch 00085: val_loss improved from 12.47470 to 12.25046, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 86/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.7127 - mean_absolute_error: 7.9854 - val_loss: 14.5162 - val_mean_absolute_error: 2.8182\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 12.25046\n",
      "Epoch 87/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.1278 - mean_absolute_error: 8.3951 - val_loss: 16.3295 - val_mean_absolute_error: 3.1110\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 12.25046\n",
      "Epoch 88/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.9980 - mean_absolute_error: 8.7094 - val_loss: 26.5625 - val_mean_absolute_error: 4.4469\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 12.25046\n",
      "Epoch 89/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.8962 - mean_absolute_error: 9.2090 - val_loss: 27.7605 - val_mean_absolute_error: 4.4714\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 12.25046\n",
      "Epoch 90/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.0080 - mean_absolute_error: 8.7243 - val_loss: 16.6350 - val_mean_absolute_error: 3.2340\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 12.25046\n",
      "Epoch 91/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.8455 - mean_absolute_error: 8.8630 - val_loss: 13.2455 - val_mean_absolute_error: 2.5524\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 12.25046\n",
      "Epoch 92/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.7821 - mean_absolute_error: 8.6251 - val_loss: 14.1212 - val_mean_absolute_error: 2.7634\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 12.25046\n",
      "Epoch 93/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.0193 - mean_absolute_error: 8.4258 - val_loss: 20.7253 - val_mean_absolute_error: 3.7735\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 12.25046\n",
      "Epoch 94/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.8713 - mean_absolute_error: 8.2056 - val_loss: 21.4522 - val_mean_absolute_error: 3.8427\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 12.25046\n",
      "Epoch 95/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.4487 - mean_absolute_error: 8.8569 - val_loss: 13.0312 - val_mean_absolute_error: 2.6603\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 12.25046\n",
      "Epoch 96/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.8677 - mean_absolute_error: 8.1432 - val_loss: 15.4589 - val_mean_absolute_error: 3.1550\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 12.25046\n",
      "Epoch 97/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.4395 - mean_absolute_error: 8.9834 - val_loss: 28.8287 - val_mean_absolute_error: 4.7575\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 12.25046\n",
      "Epoch 98/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.3749 - mean_absolute_error: 8.3489 - val_loss: 22.4320 - val_mean_absolute_error: 4.0151\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 12.25046\n",
      "Epoch 99/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.2137 - mean_absolute_error: 8.3234 - val_loss: 12.9734 - val_mean_absolute_error: 2.5815\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 12.25046\n",
      "Epoch 100/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 109.7440 - mean_absolute_error: 8.3472 - val_loss: 19.4283 - val_mean_absolute_error: 3.6911\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 12.25046\n",
      "Epoch 101/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.1535 - mean_absolute_error: 8.5844 - val_loss: 24.7482 - val_mean_absolute_error: 4.2947\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 12.25046\n",
      "Epoch 102/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.5767 - mean_absolute_error: 8.7358 - val_loss: 15.1559 - val_mean_absolute_error: 2.9475\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 12.25046\n",
      "Epoch 103/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.4138 - mean_absolute_error: 7.9786 - val_loss: 11.9128 - val_mean_absolute_error: 2.3943\n",
      "\n",
      "Epoch 00103: val_loss improved from 12.25046 to 11.91281, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 104/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.9565 - mean_absolute_error: 7.7327 - val_loss: 12.0879 - val_mean_absolute_error: 2.5492\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 11.91281\n",
      "Epoch 105/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 119.0684 - mean_absolute_error: 8.4900 - val_loss: 20.0403 - val_mean_absolute_error: 3.6997\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 11.91281\n",
      "Epoch 106/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.5313 - mean_absolute_error: 9.0291 - val_loss: 12.1743 - val_mean_absolute_error: 2.6121\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 11.91281\n",
      "Epoch 107/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.6101 - mean_absolute_error: 8.4643 - val_loss: 16.3548 - val_mean_absolute_error: 3.3202\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 11.91281\n",
      "Epoch 108/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.5879 - mean_absolute_error: 8.7865 - val_loss: 15.2277 - val_mean_absolute_error: 3.0234\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 11.91281\n",
      "Epoch 109/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.8387 - mean_absolute_error: 8.2104 - val_loss: 18.8273 - val_mean_absolute_error: 3.5695\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 11.91281\n",
      "Epoch 110/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.5464 - mean_absolute_error: 8.6352 - val_loss: 15.0046 - val_mean_absolute_error: 3.0527\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 11.91281\n",
      "Epoch 111/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 122.5760 - mean_absolute_error: 8.8662 - val_loss: 12.0410 - val_mean_absolute_error: 2.4105\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 11.91281\n",
      "Epoch 112/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.0713 - mean_absolute_error: 8.6324 - val_loss: 12.2002 - val_mean_absolute_error: 2.5443\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 11.91281\n",
      "Epoch 113/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.2910 - mean_absolute_error: 9.0592 - val_loss: 16.8378 - val_mean_absolute_error: 3.3739\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 11.91281\n",
      "Epoch 114/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.2996 - mean_absolute_error: 8.4824 - val_loss: 18.2212 - val_mean_absolute_error: 3.4765\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 11.91281\n",
      "Epoch 115/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.1735 - mean_absolute_error: 8.7648 - val_loss: 11.0928 - val_mean_absolute_error: 2.3131\n",
      "\n",
      "Epoch 00115: val_loss improved from 11.91281 to 11.09281, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 116/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.5091 - mean_absolute_error: 8.4987 - val_loss: 11.7534 - val_mean_absolute_error: 2.5402\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 11.09281\n",
      "Epoch 117/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.2821 - mean_absolute_error: 8.5379 - val_loss: 15.2840 - val_mean_absolute_error: 3.0930\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 11.09281\n",
      "Epoch 118/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.2055 - mean_absolute_error: 8.2817 - val_loss: 17.7319 - val_mean_absolute_error: 3.4107\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 11.09281\n",
      "Epoch 119/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.7955 - mean_absolute_error: 8.7960 - val_loss: 16.3032 - val_mean_absolute_error: 3.2281\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 11.09281\n",
      "Epoch 120/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.5472 - mean_absolute_error: 8.1347 - val_loss: 13.8525 - val_mean_absolute_error: 2.8090\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 11.09281\n",
      "Epoch 121/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.9347 - mean_absolute_error: 8.3465 - val_loss: 13.0879 - val_mean_absolute_error: 2.7813\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 11.09281\n",
      "Epoch 122/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.6391 - mean_absolute_error: 8.6402 - val_loss: 13.3200 - val_mean_absolute_error: 2.7298\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 11.09281\n",
      "Epoch 123/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.4985 - mean_absolute_error: 8.9351 - val_loss: 23.8655 - val_mean_absolute_error: 4.2475\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 11.09281\n",
      "Epoch 124/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.8230 - mean_absolute_error: 8.3181 - val_loss: 18.7268 - val_mean_absolute_error: 3.5664\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 11.09281\n",
      "Epoch 125/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.4762 - mean_absolute_error: 9.0151 - val_loss: 11.6839 - val_mean_absolute_error: 2.4475\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 11.09281\n",
      "Epoch 126/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.1845 - mean_absolute_error: 8.1262 - val_loss: 13.6567 - val_mean_absolute_error: 2.8546\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 11.09281\n",
      "Epoch 127/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.9181 - mean_absolute_error: 8.4776 - val_loss: 16.0618 - val_mean_absolute_error: 3.2129\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 11.09281\n",
      "Epoch 128/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.5625 - mean_absolute_error: 8.8099 - val_loss: 20.2499 - val_mean_absolute_error: 3.7419\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 11.09281\n",
      "Epoch 129/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.9559 - mean_absolute_error: 8.3751 - val_loss: 18.5833 - val_mean_absolute_error: 3.5641\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 11.09281\n",
      "Epoch 130/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.9027 - mean_absolute_error: 8.8100 - val_loss: 32.3426 - val_mean_absolute_error: 5.0637\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 11.09281\n",
      "Epoch 131/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 105.4185 - mean_absolute_error: 8.1648 - val_loss: 11.4932 - val_mean_absolute_error: 2.4307\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 11.09281\n",
      "Epoch 132/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 103.2705 - mean_absolute_error: 8.0694 - val_loss: 10.5815 - val_mean_absolute_error: 2.3178\n",
      "\n",
      "Epoch 00132: val_loss improved from 11.09281 to 10.58147, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 133/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.1801 - mean_absolute_error: 8.3460 - val_loss: 19.2773 - val_mean_absolute_error: 3.6206\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 10.58147\n",
      "Epoch 134/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.3371 - mean_absolute_error: 8.3055 - val_loss: 10.5701 - val_mean_absolute_error: 2.4205\n",
      "\n",
      "Epoch 00134: val_loss improved from 10.58147 to 10.57010, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 135/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.4643 - mean_absolute_error: 8.4290 - val_loss: 10.9409 - val_mean_absolute_error: 2.5176\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 10.57010\n",
      "Epoch 136/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 125.7738 - mean_absolute_error: 8.6705 - val_loss: 12.5582 - val_mean_absolute_error: 2.7397\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 10.57010\n",
      "Epoch 137/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.6524 - mean_absolute_error: 8.3277 - val_loss: 15.3813 - val_mean_absolute_error: 3.1139\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 10.57010\n",
      "Epoch 138/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.3563 - mean_absolute_error: 8.5739 - val_loss: 18.0168 - val_mean_absolute_error: 3.4709\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 10.57010\n",
      "Epoch 139/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.8897 - mean_absolute_error: 8.0099 - val_loss: 15.1374 - val_mean_absolute_error: 3.1313\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 10.57010\n",
      "Epoch 140/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.6085 - mean_absolute_error: 8.0745 - val_loss: 12.0573 - val_mean_absolute_error: 2.5558\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 10.57010\n",
      "Epoch 141/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.1752 - mean_absolute_error: 8.5162 - val_loss: 21.5609 - val_mean_absolute_error: 4.0451\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 10.57010\n",
      "Epoch 142/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 122.9854 - mean_absolute_error: 8.7906 - val_loss: 17.4960 - val_mean_absolute_error: 3.5055\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 10.57010\n",
      "Epoch 143/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.7157 - mean_absolute_error: 7.9153 - val_loss: 14.2204 - val_mean_absolute_error: 3.0346\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 10.57010\n",
      "Epoch 144/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.7440 - mean_absolute_error: 8.2282 - val_loss: 11.3579 - val_mean_absolute_error: 2.4459\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 10.57010\n",
      "Epoch 145/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.0696 - mean_absolute_error: 8.5568 - val_loss: 10.7262 - val_mean_absolute_error: 2.3078\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 10.57010\n",
      "Epoch 146/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.4061 - mean_absolute_error: 9.1601 - val_loss: 13.4045 - val_mean_absolute_error: 2.8684\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 10.57010\n",
      "Epoch 147/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.6801 - mean_absolute_error: 7.7907 - val_loss: 11.6919 - val_mean_absolute_error: 2.5250\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 10.57010\n",
      "Epoch 148/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.7695 - mean_absolute_error: 8.6613 - val_loss: 17.9853 - val_mean_absolute_error: 3.5020\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 10.57010\n",
      "Epoch 149/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.5178 - mean_absolute_error: 8.6750 - val_loss: 14.6807 - val_mean_absolute_error: 2.9996\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 10.57010\n",
      "Epoch 150/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.0636 - mean_absolute_error: 8.8617 - val_loss: 13.3195 - val_mean_absolute_error: 2.7680\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 10.57010\n",
      "Epoch 151/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.8734 - mean_absolute_error: 8.0105 - val_loss: 15.8271 - val_mean_absolute_error: 3.2232\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 10.57010\n",
      "Epoch 152/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.0309 - mean_absolute_error: 8.7588 - val_loss: 14.0152 - val_mean_absolute_error: 2.9532\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 10.57010\n",
      "Epoch 153/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 122.5368 - mean_absolute_error: 8.8143 - val_loss: 12.4581 - val_mean_absolute_error: 2.6562\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 10.57010\n",
      "Epoch 154/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.9227 - mean_absolute_error: 8.6444 - val_loss: 15.8045 - val_mean_absolute_error: 3.2370\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 10.57010\n",
      "Epoch 155/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.2314 - mean_absolute_error: 8.1970 - val_loss: 11.7988 - val_mean_absolute_error: 2.5822\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 10.57010\n",
      "Epoch 156/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.0524 - mean_absolute_error: 8.0527 - val_loss: 10.6300 - val_mean_absolute_error: 2.3786\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 10.57010\n",
      "Epoch 157/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.0302 - mean_absolute_error: 8.5207 - val_loss: 10.2176 - val_mean_absolute_error: 2.2343\n",
      "\n",
      "Epoch 00157: val_loss improved from 10.57010 to 10.21758, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 158/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.4715 - mean_absolute_error: 8.0417 - val_loss: 14.1847 - val_mean_absolute_error: 2.9234\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 10.21758\n",
      "Epoch 159/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.1212 - mean_absolute_error: 8.6295 - val_loss: 18.4163 - val_mean_absolute_error: 3.5092\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 10.21758\n",
      "Epoch 160/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.8953 - mean_absolute_error: 8.6571 - val_loss: 13.4835 - val_mean_absolute_error: 2.9459\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 10.21758\n",
      "Epoch 161/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.1725 - mean_absolute_error: 8.4483 - val_loss: 10.4418 - val_mean_absolute_error: 2.3879\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 10.21758\n",
      "Epoch 162/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.5544 - mean_absolute_error: 8.3342 - val_loss: 10.3965 - val_mean_absolute_error: 2.3174\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 10.21758\n",
      "Epoch 163/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 96.2768 - mean_absolute_error: 7.9318 - val_loss: 10.7296 - val_mean_absolute_error: 2.5376\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 10.21758\n",
      "Epoch 164/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.2458 - mean_absolute_error: 8.6587 - val_loss: 17.7772 - val_mean_absolute_error: 3.5864\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 10.21758\n",
      "Epoch 165/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.1833 - mean_absolute_error: 8.2084 - val_loss: 15.2062 - val_mean_absolute_error: 3.1501\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 10.21758\n",
      "Epoch 166/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.8256 - mean_absolute_error: 8.6737 - val_loss: 9.3411 - val_mean_absolute_error: 2.2373\n",
      "\n",
      "Epoch 00166: val_loss improved from 10.21758 to 9.34110, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 167/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.7724 - mean_absolute_error: 8.0915 - val_loss: 13.3228 - val_mean_absolute_error: 2.9943\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 9.34110\n",
      "Epoch 168/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.5761 - mean_absolute_error: 8.2647 - val_loss: 10.1975 - val_mean_absolute_error: 2.5010\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 9.34110\n",
      "Epoch 169/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.9041 - mean_absolute_error: 8.0340 - val_loss: 13.8344 - val_mean_absolute_error: 3.0298\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 9.34110\n",
      "Epoch 170/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.9022 - mean_absolute_error: 8.3095 - val_loss: 9.3030 - val_mean_absolute_error: 2.2908\n",
      "\n",
      "Epoch 00170: val_loss improved from 9.34110 to 9.30300, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 171/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.7266 - mean_absolute_error: 8.4646 - val_loss: 8.7972 - val_mean_absolute_error: 2.1444\n",
      "\n",
      "Epoch 00171: val_loss improved from 9.30300 to 8.79717, saving model to LSTM_Interval_best_GCA.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.4226 - mean_absolute_error: 8.0285 - val_loss: 11.9427 - val_mean_absolute_error: 2.6802\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 8.79717\n",
      "Epoch 173/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.2295 - mean_absolute_error: 8.6058 - val_loss: 13.3545 - val_mean_absolute_error: 2.9560\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 8.79717\n",
      "Epoch 174/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.4175 - mean_absolute_error: 8.2298 - val_loss: 10.2059 - val_mean_absolute_error: 2.4269\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 8.79717\n",
      "Epoch 175/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.4764 - mean_absolute_error: 8.0106 - val_loss: 9.0931 - val_mean_absolute_error: 2.1117\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 8.79717\n",
      "Epoch 176/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.7765 - mean_absolute_error: 8.5284 - val_loss: 14.2972 - val_mean_absolute_error: 3.0745\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 8.79717\n",
      "Epoch 177/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.9437 - mean_absolute_error: 8.3308 - val_loss: 14.2279 - val_mean_absolute_error: 3.0647\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 8.79717\n",
      "Epoch 178/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.0361 - mean_absolute_error: 8.0871 - val_loss: 11.1341 - val_mean_absolute_error: 2.3541\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 8.79717\n",
      "Epoch 179/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.9884 - mean_absolute_error: 8.3778 - val_loss: 10.9269 - val_mean_absolute_error: 2.5537\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 8.79717\n",
      "Epoch 180/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.5936 - mean_absolute_error: 8.2207 - val_loss: 11.2534 - val_mean_absolute_error: 2.5517\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 8.79717\n",
      "Epoch 181/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.4188 - mean_absolute_error: 8.2485 - val_loss: 14.1770 - val_mean_absolute_error: 3.0506\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 8.79717\n",
      "Epoch 182/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.6748 - mean_absolute_error: 8.1429 - val_loss: 10.3073 - val_mean_absolute_error: 2.3387\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 8.79717\n",
      "Epoch 183/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.3467 - mean_absolute_error: 8.2472 - val_loss: 10.8605 - val_mean_absolute_error: 2.5551\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 8.79717\n",
      "Epoch 184/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.7714 - mean_absolute_error: 7.8174 - val_loss: 10.3074 - val_mean_absolute_error: 2.4609\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 8.79717\n",
      "Epoch 185/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 112.4267 - mean_absolute_error: 8.2321 - val_loss: 20.5344 - val_mean_absolute_error: 3.8664\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 8.79717\n",
      "Epoch 186/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.3158 - mean_absolute_error: 8.4215 - val_loss: 11.8170 - val_mean_absolute_error: 2.7080\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 8.79717\n",
      "Epoch 187/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.8202 - mean_absolute_error: 8.3915 - val_loss: 10.1720 - val_mean_absolute_error: 2.4488\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 8.79717\n",
      "Epoch 188/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.6284 - mean_absolute_error: 8.1366 - val_loss: 9.0869 - val_mean_absolute_error: 2.1938\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 8.79717\n",
      "Epoch 189/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.2688 - mean_absolute_error: 8.5235 - val_loss: 10.0794 - val_mean_absolute_error: 2.4244\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 8.79717\n",
      "Epoch 190/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.4342 - mean_absolute_error: 8.6143 - val_loss: 11.2129 - val_mean_absolute_error: 2.7815\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 8.79717\n",
      "Epoch 191/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.2033 - mean_absolute_error: 8.3508 - val_loss: 9.5201 - val_mean_absolute_error: 2.4657\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 8.79717\n",
      "Epoch 192/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.2393 - mean_absolute_error: 7.8781 - val_loss: 9.3282 - val_mean_absolute_error: 2.3805\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 8.79717\n",
      "Epoch 193/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.5645 - mean_absolute_error: 8.5512 - val_loss: 17.5983 - val_mean_absolute_error: 3.5835\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 8.79717\n",
      "Epoch 194/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.6497 - mean_absolute_error: 8.3416 - val_loss: 8.8266 - val_mean_absolute_error: 2.1734\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 8.79717\n",
      "Epoch 195/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.6993 - mean_absolute_error: 8.4684 - val_loss: 9.7598 - val_mean_absolute_error: 2.5467\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 8.79717\n",
      "Epoch 196/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.0059 - mean_absolute_error: 8.2031 - val_loss: 13.6343 - val_mean_absolute_error: 3.1291\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 8.79717\n",
      "Epoch 197/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.3252 - mean_absolute_error: 8.0742 - val_loss: 8.1058 - val_mean_absolute_error: 1.9778\n",
      "\n",
      "Epoch 00197: val_loss improved from 8.79717 to 8.10580, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 198/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.5159 - mean_absolute_error: 7.9864 - val_loss: 9.4240 - val_mean_absolute_error: 2.2354\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 8.10580\n",
      "Epoch 199/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.6013 - mean_absolute_error: 7.9218 - val_loss: 11.8819 - val_mean_absolute_error: 2.5790\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 8.10580\n",
      "Epoch 200/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.8617 - mean_absolute_error: 8.3659 - val_loss: 10.5159 - val_mean_absolute_error: 2.5572\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 8.10580\n",
      "Epoch 201/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.7828 - mean_absolute_error: 8.7048 - val_loss: 16.6258 - val_mean_absolute_error: 3.3312\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 8.10580\n",
      "Epoch 202/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.6242 - mean_absolute_error: 8.2937 - val_loss: 8.4084 - val_mean_absolute_error: 2.0512\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 8.10580\n",
      "Epoch 203/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.8445 - mean_absolute_error: 8.5374 - val_loss: 11.2378 - val_mean_absolute_error: 2.6216\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 8.10580\n",
      "Epoch 204/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.6428 - mean_absolute_error: 8.4050 - val_loss: 9.3383 - val_mean_absolute_error: 2.1784\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 8.10580\n",
      "Epoch 205/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.3980 - mean_absolute_error: 8.2779 - val_loss: 14.0522 - val_mean_absolute_error: 3.0628\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 8.10580\n",
      "Epoch 206/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.4296 - mean_absolute_error: 7.9225 - val_loss: 26.2287 - val_mean_absolute_error: 4.5854\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 8.10580\n",
      "Epoch 207/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.5678 - mean_absolute_error: 8.0421 - val_loss: 14.2221 - val_mean_absolute_error: 3.0077\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 8.10580\n",
      "Epoch 208/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.7220 - mean_absolute_error: 8.3732 - val_loss: 10.0002 - val_mean_absolute_error: 2.3495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00208: val_loss did not improve from 8.10580\n",
      "Epoch 209/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.0593 - mean_absolute_error: 8.2532 - val_loss: 9.4622 - val_mean_absolute_error: 2.3537\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 8.10580\n",
      "Epoch 210/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.9297 - mean_absolute_error: 8.6800 - val_loss: 18.4897 - val_mean_absolute_error: 3.7651\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 8.10580\n",
      "Epoch 211/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.6323 - mean_absolute_error: 8.4534 - val_loss: 9.3069 - val_mean_absolute_error: 2.2665\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 8.10580\n",
      "Epoch 212/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.8966 - mean_absolute_error: 8.1240 - val_loss: 9.1277 - val_mean_absolute_error: 2.2212\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 8.10580\n",
      "Epoch 213/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.5594 - mean_absolute_error: 7.8049 - val_loss: 9.0845 - val_mean_absolute_error: 2.3211\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 8.10580\n",
      "Epoch 214/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.4169 - mean_absolute_error: 8.3917 - val_loss: 9.5470 - val_mean_absolute_error: 2.3767\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 8.10580\n",
      "Epoch 215/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.7861 - mean_absolute_error: 8.3903 - val_loss: 20.6544 - val_mean_absolute_error: 3.8535\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 8.10580\n",
      "Epoch 216/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.1906 - mean_absolute_error: 8.0394 - val_loss: 12.8972 - val_mean_absolute_error: 2.9201\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 8.10580\n",
      "Epoch 217/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.8151 - mean_absolute_error: 7.8926 - val_loss: 14.2336 - val_mean_absolute_error: 3.1630\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 8.10580\n",
      "Epoch 218/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.9980 - mean_absolute_error: 8.0797 - val_loss: 12.9669 - val_mean_absolute_error: 2.9942\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 8.10580\n",
      "Epoch 219/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.7439 - mean_absolute_error: 8.3935 - val_loss: 12.8461 - val_mean_absolute_error: 3.0084\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 8.10580\n",
      "Epoch 220/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.4514 - mean_absolute_error: 8.0981 - val_loss: 7.8362 - val_mean_absolute_error: 2.1265\n",
      "\n",
      "Epoch 00220: val_loss improved from 8.10580 to 7.83622, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 221/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.8261 - mean_absolute_error: 7.9371 - val_loss: 7.8846 - val_mean_absolute_error: 2.0654\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 7.83622\n",
      "Epoch 222/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.4587 - mean_absolute_error: 8.4282 - val_loss: 19.1610 - val_mean_absolute_error: 3.7284\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 7.83622\n",
      "Epoch 223/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.7876 - mean_absolute_error: 8.5816 - val_loss: 8.0827 - val_mean_absolute_error: 2.1438\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 7.83622\n",
      "Epoch 224/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.6737 - mean_absolute_error: 8.0713 - val_loss: 9.4007 - val_mean_absolute_error: 2.2532\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 7.83622\n",
      "Epoch 225/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.5119 - mean_absolute_error: 8.5486 - val_loss: 8.8936 - val_mean_absolute_error: 2.0813\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 7.83622\n",
      "Epoch 226/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.3473 - mean_absolute_error: 8.1341 - val_loss: 14.8505 - val_mean_absolute_error: 3.2174\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 7.83622\n",
      "Epoch 227/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.6987 - mean_absolute_error: 8.7909 - val_loss: 12.8900 - val_mean_absolute_error: 2.8875\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 7.83622\n",
      "Epoch 228/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 113.4006 - mean_absolute_error: 8.2556 - val_loss: 10.4462 - val_mean_absolute_error: 2.6347\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 7.83622\n",
      "Epoch 229/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.9058 - mean_absolute_error: 8.3925 - val_loss: 15.9401 - val_mean_absolute_error: 3.4735\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 7.83622\n",
      "Epoch 230/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.5073 - mean_absolute_error: 8.2244 - val_loss: 9.6184 - val_mean_absolute_error: 2.5856\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 7.83622\n",
      "Epoch 231/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.7916 - mean_absolute_error: 7.9571 - val_loss: 8.5095 - val_mean_absolute_error: 2.2376\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 7.83622\n",
      "Epoch 232/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.7881 - mean_absolute_error: 8.2923 - val_loss: 11.8018 - val_mean_absolute_error: 2.8381\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 7.83622\n",
      "Epoch 233/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.2567 - mean_absolute_error: 8.1395 - val_loss: 11.3624 - val_mean_absolute_error: 2.6528\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 7.83622\n",
      "Epoch 234/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.1649 - mean_absolute_error: 8.8493 - val_loss: 8.3231 - val_mean_absolute_error: 2.1335\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 7.83622\n",
      "Epoch 235/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.2447 - mean_absolute_error: 8.4072 - val_loss: 8.7630 - val_mean_absolute_error: 2.1401\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 7.83622\n",
      "Epoch 236/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.0166 - mean_absolute_error: 8.0882 - val_loss: 15.7421 - val_mean_absolute_error: 3.4214\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 7.83622\n",
      "Epoch 237/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 95.1898 - mean_absolute_error: 7.5310 - val_loss: 8.8933 - val_mean_absolute_error: 2.1590\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 7.83622\n",
      "Epoch 238/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 115.3886 - mean_absolute_error: 8.5186 - val_loss: 19.1520 - val_mean_absolute_error: 3.8193\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 7.83622\n",
      "Epoch 239/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.9314 - mean_absolute_error: 8.2313 - val_loss: 12.4236 - val_mean_absolute_error: 2.8948\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 7.83622\n",
      "Epoch 240/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.4941 - mean_absolute_error: 8.2465 - val_loss: 6.7224 - val_mean_absolute_error: 1.8215\n",
      "\n",
      "Epoch 00240: val_loss improved from 7.83622 to 6.72240, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 241/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.6872 - mean_absolute_error: 8.2421 - val_loss: 19.9691 - val_mean_absolute_error: 3.9062\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 6.72240\n",
      "Epoch 242/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.9449 - mean_absolute_error: 9.0773 - val_loss: 19.7893 - val_mean_absolute_error: 3.9306\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 6.72240\n",
      "Epoch 243/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 114.2935 - mean_absolute_error: 8.4341 - val_loss: 19.1263 - val_mean_absolute_error: 3.8197\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 6.72240\n",
      "Epoch 244/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.4844 - mean_absolute_error: 8.5127 - val_loss: 10.9975 - val_mean_absolute_error: 2.6272\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 6.72240\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 100.2925 - mean_absolute_error: 7.8168 - val_loss: 8.1064 - val_mean_absolute_error: 1.9972\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 6.72240\n",
      "Epoch 246/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.9232 - mean_absolute_error: 8.1221 - val_loss: 9.8492 - val_mean_absolute_error: 2.2883\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 6.72240\n",
      "Epoch 247/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.2623 - mean_absolute_error: 8.3683 - val_loss: 12.9107 - val_mean_absolute_error: 2.7476\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 6.72240\n",
      "Epoch 248/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.0659 - mean_absolute_error: 7.9892 - val_loss: 10.1076 - val_mean_absolute_error: 2.4393\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 6.72240\n",
      "Epoch 249/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.2196 - mean_absolute_error: 8.2340 - val_loss: 6.7401 - val_mean_absolute_error: 1.8926\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 6.72240\n",
      "Epoch 250/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.9083 - mean_absolute_error: 8.0113 - val_loss: 10.8684 - val_mean_absolute_error: 2.6786\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 6.72240\n",
      "Epoch 251/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.5733 - mean_absolute_error: 7.8946 - val_loss: 7.7221 - val_mean_absolute_error: 2.1070\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 6.72240\n",
      "Epoch 252/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.5611 - mean_absolute_error: 8.2216 - val_loss: 7.7518 - val_mean_absolute_error: 2.0607\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 6.72240\n",
      "Epoch 253/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.5161 - mean_absolute_error: 8.2112 - val_loss: 10.3342 - val_mean_absolute_error: 2.5909\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 6.72240\n",
      "Epoch 254/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.7970 - mean_absolute_error: 8.7471 - val_loss: 12.6652 - val_mean_absolute_error: 2.9615\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 6.72240\n",
      "Epoch 255/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.5592 - mean_absolute_error: 8.4490 - val_loss: 7.6692 - val_mean_absolute_error: 2.0885\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 6.72240\n",
      "Epoch 256/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.5477 - mean_absolute_error: 8.1905 - val_loss: 6.6776 - val_mean_absolute_error: 1.8926\n",
      "\n",
      "Epoch 00256: val_loss improved from 6.72240 to 6.67757, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 257/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.3321 - mean_absolute_error: 8.3374 - val_loss: 19.9049 - val_mean_absolute_error: 3.9312\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 6.67757\n",
      "Epoch 258/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.8675 - mean_absolute_error: 8.6794 - val_loss: 15.3200 - val_mean_absolute_error: 3.2667\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 6.67757\n",
      "Epoch 259/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.0623 - mean_absolute_error: 7.8199 - val_loss: 9.4455 - val_mean_absolute_error: 2.4235\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 6.67757\n",
      "Epoch 260/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 111.1791 - mean_absolute_error: 8.1625 - val_loss: 7.3133 - val_mean_absolute_error: 1.9971\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 6.67757\n",
      "Epoch 261/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.0668 - mean_absolute_error: 8.4500 - val_loss: 8.2369 - val_mean_absolute_error: 2.0617\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 6.67757\n",
      "Epoch 262/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.7634 - mean_absolute_error: 7.8683 - val_loss: 8.9108 - val_mean_absolute_error: 2.3191\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 6.67757\n",
      "Epoch 263/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.9676 - mean_absolute_error: 8.1428 - val_loss: 9.6692 - val_mean_absolute_error: 2.4753\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 6.67757\n",
      "Epoch 264/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.5443 - mean_absolute_error: 8.7433 - val_loss: 9.2638 - val_mean_absolute_error: 2.3518\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 6.67757\n",
      "Epoch 265/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.0898 - mean_absolute_error: 8.5896 - val_loss: 15.8368 - val_mean_absolute_error: 3.3742\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 6.67757\n",
      "Epoch 266/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.2569 - mean_absolute_error: 8.4822 - val_loss: 7.8038 - val_mean_absolute_error: 2.0438\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 6.67757\n",
      "Epoch 267/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.1338 - mean_absolute_error: 7.6841 - val_loss: 6.8850 - val_mean_absolute_error: 1.8978\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 6.67757\n",
      "Epoch 268/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.5077 - mean_absolute_error: 8.1179 - val_loss: 14.3485 - val_mean_absolute_error: 3.1453\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 6.67757\n",
      "Epoch 269/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.9071 - mean_absolute_error: 8.6488 - val_loss: 13.6962 - val_mean_absolute_error: 3.1302\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 6.67757\n",
      "Epoch 270/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.9385 - mean_absolute_error: 8.2191 - val_loss: 6.2917 - val_mean_absolute_error: 1.7912\n",
      "\n",
      "Epoch 00270: val_loss improved from 6.67757 to 6.29173, saving model to LSTM_Interval_best_GCA.hdf5\n",
      "Epoch 271/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.9342 - mean_absolute_error: 7.8620 - val_loss: 9.7588 - val_mean_absolute_error: 2.4622\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 6.29173\n",
      "Epoch 272/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.0829 - mean_absolute_error: 8.1240 - val_loss: 34.1855 - val_mean_absolute_error: 5.3034\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 6.29173\n",
      "Epoch 273/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.6806 - mean_absolute_error: 8.6195 - val_loss: 9.2592 - val_mean_absolute_error: 2.2132\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 6.29173\n",
      "Epoch 274/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.0240 - mean_absolute_error: 8.3675 - val_loss: 8.4286 - val_mean_absolute_error: 2.2535\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 6.29173\n",
      "Epoch 275/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.1399 - mean_absolute_error: 8.0138 - val_loss: 7.3253 - val_mean_absolute_error: 1.9933\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 6.29173\n",
      "Epoch 276/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.3556 - mean_absolute_error: 8.6691 - val_loss: 8.0676 - val_mean_absolute_error: 2.1403\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 6.29173\n",
      "Epoch 277/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.1787 - mean_absolute_error: 7.8783 - val_loss: 13.4442 - val_mean_absolute_error: 2.9724\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 6.29173\n",
      "Epoch 278/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.9333 - mean_absolute_error: 7.8616 - val_loss: 9.6562 - val_mean_absolute_error: 2.4358\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 6.29173\n",
      "Epoch 279/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.2404 - mean_absolute_error: 8.3411 - val_loss: 19.5579 - val_mean_absolute_error: 3.8695\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 6.29173\n",
      "Epoch 280/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.4485 - mean_absolute_error: 8.1231 - val_loss: 7.6268 - val_mean_absolute_error: 1.9674\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 6.29173\n",
      "Epoch 281/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.9261 - mean_absolute_error: 7.8579 - val_loss: 11.1345 - val_mean_absolute_error: 2.6640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00281: val_loss did not improve from 6.29173\n",
      "Epoch 282/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.6341 - mean_absolute_error: 8.0829 - val_loss: 7.2663 - val_mean_absolute_error: 2.0398\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 6.29173\n",
      "Epoch 283/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.1120 - mean_absolute_error: 8.4117 - val_loss: 7.6196 - val_mean_absolute_error: 2.0639\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 6.29173\n",
      "Epoch 284/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.8218 - mean_absolute_error: 8.0121 - val_loss: 11.0993 - val_mean_absolute_error: 2.5495\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 6.29173\n",
      "Epoch 285/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.9239 - mean_absolute_error: 7.7620 - val_loss: 10.5683 - val_mean_absolute_error: 2.5334\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 6.29173\n",
      "Epoch 286/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.0818 - mean_absolute_error: 7.9342 - val_loss: 14.5616 - val_mean_absolute_error: 3.2426\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 6.29173\n",
      "Epoch 287/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.4071 - mean_absolute_error: 8.5461 - val_loss: 7.6957 - val_mean_absolute_error: 2.0259\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 6.29173\n",
      "Epoch 288/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.3056 - mean_absolute_error: 8.2726 - val_loss: 7.0990 - val_mean_absolute_error: 2.0044\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 6.29173\n",
      "Epoch 289/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.0860 - mean_absolute_error: 8.1899 - val_loss: 7.5341 - val_mean_absolute_error: 2.0498\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 6.29173\n",
      "Epoch 290/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.6152 - mean_absolute_error: 8.1806 - val_loss: 14.4888 - val_mean_absolute_error: 3.2907\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 6.29173\n",
      "Epoch 291/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.1792 - mean_absolute_error: 8.3312 - val_loss: 14.1205 - val_mean_absolute_error: 3.0443\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 6.29173\n",
      "Epoch 292/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.5991 - mean_absolute_error: 7.9875 - val_loss: 11.9572 - val_mean_absolute_error: 2.7834\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 6.29173\n",
      "Epoch 293/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.4638 - mean_absolute_error: 8.0036 - val_loss: 7.4095 - val_mean_absolute_error: 1.9482\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 6.29173\n",
      "Epoch 294/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 94.5974 - mean_absolute_error: 7.6309 - val_loss: 7.9349 - val_mean_absolute_error: 2.1432\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 6.29173\n",
      "Epoch 295/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.8974 - mean_absolute_error: 7.9567 - val_loss: 15.9166 - val_mean_absolute_error: 3.4994\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 6.29173\n",
      "Epoch 296/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.6910 - mean_absolute_error: 8.8049 - val_loss: 13.1436 - val_mean_absolute_error: 3.1409\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 6.29173\n",
      "Epoch 297/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.6208 - mean_absolute_error: 8.1157 - val_loss: 7.9348 - val_mean_absolute_error: 2.2409\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 6.29173\n",
      "Epoch 298/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.4790 - mean_absolute_error: 8.3609 - val_loss: 11.8409 - val_mean_absolute_error: 2.8584\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 6.29173\n",
      "Epoch 299/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.8816 - mean_absolute_error: 7.9752 - val_loss: 10.1970 - val_mean_absolute_error: 2.6102\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 6.29173\n",
      "Epoch 300/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.1708 - mean_absolute_error: 7.8001 - val_loss: 6.4680 - val_mean_absolute_error: 1.8523\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 6.29173\n",
      "Epoch 301/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.4424 - mean_absolute_error: 8.2554 - val_loss: 7.6632 - val_mean_absolute_error: 2.0856\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 6.29173\n",
      "Epoch 302/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 113.8061 - mean_absolute_error: 8.4701 - val_loss: 10.4053 - val_mean_absolute_error: 2.5945\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 6.29173\n",
      "Epoch 303/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 114.0835 - mean_absolute_error: 8.3872 - val_loss: 10.2256 - val_mean_absolute_error: 2.3702\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 6.29173\n",
      "Epoch 304/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.8543 - mean_absolute_error: 8.1280 - val_loss: 6.5922 - val_mean_absolute_error: 1.8388\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 6.29173\n",
      "Epoch 305/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.6945 - mean_absolute_error: 8.0855 - val_loss: 10.0211 - val_mean_absolute_error: 2.5629\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 6.29173\n",
      "Epoch 306/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.7680 - mean_absolute_error: 7.7799 - val_loss: 15.0170 - val_mean_absolute_error: 3.3857\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 6.29173\n",
      "Epoch 307/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.9940 - mean_absolute_error: 8.2496 - val_loss: 12.6804 - val_mean_absolute_error: 3.0383\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 6.29173\n",
      "Epoch 308/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 108.1919 - mean_absolute_error: 8.2457 - val_loss: 9.2805 - val_mean_absolute_error: 2.4666\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 6.29173\n",
      "Epoch 309/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.2910 - mean_absolute_error: 8.1922 - val_loss: 10.9506 - val_mean_absolute_error: 2.7235\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 6.29173\n",
      "Epoch 310/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 109.3211 - mean_absolute_error: 8.1449 - val_loss: 6.9139 - val_mean_absolute_error: 2.0183\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 6.29173\n",
      "Epoch 311/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 89.0782 - mean_absolute_error: 7.3668 - val_loss: 8.2067 - val_mean_absolute_error: 2.0528\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 6.29173\n",
      "Epoch 312/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.4001 - mean_absolute_error: 8.7171 - val_loss: 12.4437 - val_mean_absolute_error: 2.9027\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 6.29173\n",
      "Epoch 313/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.2272 - mean_absolute_error: 8.3632 - val_loss: 18.7406 - val_mean_absolute_error: 3.7238\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 6.29173\n",
      "Epoch 314/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 106.7653 - mean_absolute_error: 8.0925 - val_loss: 17.9200 - val_mean_absolute_error: 3.6140\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 6.29173\n",
      "Epoch 315/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.9576 - mean_absolute_error: 7.9550 - val_loss: 8.4038 - val_mean_absolute_error: 2.2972\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 6.29173\n",
      "Epoch 316/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.8043 - mean_absolute_error: 7.9279 - val_loss: 7.6624 - val_mean_absolute_error: 2.1183\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 6.29173\n",
      "Epoch 317/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.8862 - mean_absolute_error: 7.7838 - val_loss: 11.5541 - val_mean_absolute_error: 2.8736\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 6.29173\n",
      "Epoch 318/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.9011 - mean_absolute_error: 8.1159 - val_loss: 7.4668 - val_mean_absolute_error: 2.0403\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 6.29173\n",
      "Epoch 319/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 105.6503 - mean_absolute_error: 8.0063 - val_loss: 10.6825 - val_mean_absolute_error: 2.6211\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 6.29173\n",
      "Epoch 320/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.8089 - mean_absolute_error: 8.7943 - val_loss: 13.6691 - val_mean_absolute_error: 3.1466\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 6.29173\n",
      "Epoch 00320: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a31b90630>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_baseline(X,y_GCA,\"GCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/500\n",
      "522/522 [==============================] - 3s 7ms/step - loss: 5706.2796 - mean_absolute_error: 74.3250 - val_loss: 5202.1395 - val_mean_absolute_error: 70.8036\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5202.13953, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 2/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 4720.8920 - mean_absolute_error: 67.3043 - val_loss: 4240.0614 - val_mean_absolute_error: 63.6572\n",
      "\n",
      "Epoch 00002: val_loss improved from 5202.13953 to 4240.06136, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 3/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3911.9689 - mean_absolute_error: 60.9675 - val_loss: 3569.5855 - val_mean_absolute_error: 58.1385\n",
      "\n",
      "Epoch 00003: val_loss improved from 4240.06136 to 3569.58552, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 4/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3258.5630 - mean_absolute_error: 55.2860 - val_loss: 2925.4966 - val_mean_absolute_error: 52.3074\n",
      "\n",
      "Epoch 00004: val_loss improved from 3569.58552 to 2925.49659, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 5/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2656.5694 - mean_absolute_error: 49.5412 - val_loss: 2317.1031 - val_mean_absolute_error: 46.1266\n",
      "\n",
      "Epoch 00005: val_loss improved from 2925.49659 to 2317.10314, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 6/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2095.4700 - mean_absolute_error: 43.5009 - val_loss: 1767.9938 - val_mean_absolute_error: 39.7293\n",
      "\n",
      "Epoch 00006: val_loss improved from 2317.10314 to 1767.99375, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 7/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1609.4621 - mean_absolute_error: 37.2127 - val_loss: 1298.0484 - val_mean_absolute_error: 33.2922\n",
      "\n",
      "Epoch 00007: val_loss improved from 1767.99375 to 1298.04841, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 8/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1182.6844 - mean_absolute_error: 30.8359 - val_loss: 922.1349 - val_mean_absolute_error: 27.0790\n",
      "\n",
      "Epoch 00008: val_loss improved from 1298.04841 to 922.13491, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 9/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 821.5244 - mean_absolute_error: 24.8099 - val_loss: 641.4652 - val_mean_absolute_error: 21.7163\n",
      "\n",
      "Epoch 00009: val_loss improved from 922.13491 to 641.46523, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 10/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 628.0965 - mean_absolute_error: 20.6454 - val_loss: 448.5127 - val_mean_absolute_error: 17.3348\n",
      "\n",
      "Epoch 00010: val_loss improved from 641.46523 to 448.51273, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 11/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 474.1552 - mean_absolute_error: 17.5734 - val_loss: 329.2417 - val_mean_absolute_error: 14.3692\n",
      "\n",
      "Epoch 00011: val_loss improved from 448.51273 to 329.24170, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 12/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 385.4548 - mean_absolute_error: 15.3632 - val_loss: 262.8095 - val_mean_absolute_error: 12.7604\n",
      "\n",
      "Epoch 00012: val_loss improved from 329.24170 to 262.80945, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 13/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 356.7445 - mean_absolute_error: 15.0305 - val_loss: 227.3777 - val_mean_absolute_error: 11.9349\n",
      "\n",
      "Epoch 00013: val_loss improved from 262.80945 to 227.37774, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 14/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 324.4401 - mean_absolute_error: 14.4183 - val_loss: 209.7081 - val_mean_absolute_error: 11.5374\n",
      "\n",
      "Epoch 00014: val_loss improved from 227.37774 to 209.70809, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 15/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 326.3590 - mean_absolute_error: 14.4666 - val_loss: 200.7862 - val_mean_absolute_error: 11.3779\n",
      "\n",
      "Epoch 00015: val_loss improved from 209.70809 to 200.78616, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 16/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 331.0919 - mean_absolute_error: 14.7763 - val_loss: 197.1610 - val_mean_absolute_error: 11.3225\n",
      "\n",
      "Epoch 00016: val_loss improved from 200.78616 to 197.16098, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 17/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 337.0309 - mean_absolute_error: 14.8297 - val_loss: 194.7417 - val_mean_absolute_error: 11.2898\n",
      "\n",
      "Epoch 00017: val_loss improved from 197.16098 to 194.74174, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 18/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 318.6353 - mean_absolute_error: 14.1386 - val_loss: 194.2762 - val_mean_absolute_error: 11.2761\n",
      "\n",
      "Epoch 00018: val_loss improved from 194.74174 to 194.27625, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 19/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 321.8746 - mean_absolute_error: 14.0565 - val_loss: 194.3069 - val_mean_absolute_error: 11.2687\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 194.27625\n",
      "Epoch 20/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 324.4613 - mean_absolute_error: 14.4455 - val_loss: 195.0073 - val_mean_absolute_error: 11.2685\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 194.27625\n",
      "Epoch 21/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 310.1991 - mean_absolute_error: 14.2005 - val_loss: 194.2082 - val_mean_absolute_error: 11.2530\n",
      "\n",
      "Epoch 00021: val_loss improved from 194.27625 to 194.20820, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 22/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 300.7446 - mean_absolute_error: 13.7270 - val_loss: 193.5605 - val_mean_absolute_error: 11.2393\n",
      "\n",
      "Epoch 00022: val_loss improved from 194.20820 to 193.56052, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 23/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 317.1970 - mean_absolute_error: 14.1716 - val_loss: 192.8734 - val_mean_absolute_error: 11.2243\n",
      "\n",
      "Epoch 00023: val_loss improved from 193.56052 to 192.87337, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 24/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 326.2701 - mean_absolute_error: 14.4764 - val_loss: 193.0383 - val_mean_absolute_error: 11.2184\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 192.87337\n",
      "Epoch 25/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 330.5711 - mean_absolute_error: 14.6857 - val_loss: 190.9682 - val_mean_absolute_error: 11.1909\n",
      "\n",
      "Epoch 00025: val_loss improved from 192.87337 to 190.96819, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 26/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 338.5338 - mean_absolute_error: 14.7270 - val_loss: 190.2364 - val_mean_absolute_error: 11.1739\n",
      "\n",
      "Epoch 00026: val_loss improved from 190.96819 to 190.23643, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 27/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 325.4916 - mean_absolute_error: 14.4773 - val_loss: 192.3338 - val_mean_absolute_error: 11.1866\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 190.23643\n",
      "Epoch 28/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 310.5682 - mean_absolute_error: 14.2931 - val_loss: 191.2188 - val_mean_absolute_error: 11.1648\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 190.23643\n",
      "Epoch 29/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 297.8048 - mean_absolute_error: 13.5313 - val_loss: 190.5850 - val_mean_absolute_error: 11.1418\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 190.23643\n",
      "Epoch 30/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 306.6028 - mean_absolute_error: 13.7170 - val_loss: 189.5144 - val_mean_absolute_error: 11.1113\n",
      "\n",
      "Epoch 00030: val_loss improved from 190.23643 to 189.51442, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 315.2960 - mean_absolute_error: 14.3870 - val_loss: 189.0507 - val_mean_absolute_error: 11.0634\n",
      "\n",
      "Epoch 00031: val_loss improved from 189.51442 to 189.05075, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 32/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 308.7411 - mean_absolute_error: 13.8555 - val_loss: 190.3493 - val_mean_absolute_error: 11.0863\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 189.05075\n",
      "Epoch 33/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 309.4397 - mean_absolute_error: 13.8861 - val_loss: 193.4352 - val_mean_absolute_error: 11.1690\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 189.05075\n",
      "Epoch 34/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 328.2486 - mean_absolute_error: 14.2312 - val_loss: 189.0117 - val_mean_absolute_error: 11.0563\n",
      "\n",
      "Epoch 00034: val_loss improved from 189.05075 to 189.01172, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 35/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 285.0775 - mean_absolute_error: 13.4721 - val_loss: 191.8235 - val_mean_absolute_error: 11.1063\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 189.01172\n",
      "Epoch 36/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 278.0422 - mean_absolute_error: 13.1676 - val_loss: 186.9505 - val_mean_absolute_error: 11.0136\n",
      "\n",
      "Epoch 00036: val_loss improved from 189.01172 to 186.95048, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 37/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 291.3470 - mean_absolute_error: 13.3872 - val_loss: 177.6112 - val_mean_absolute_error: 10.7093\n",
      "\n",
      "Epoch 00037: val_loss improved from 186.95048 to 177.61124, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 38/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 283.7768 - mean_absolute_error: 13.1383 - val_loss: 165.7017 - val_mean_absolute_error: 10.1329\n",
      "\n",
      "Epoch 00038: val_loss improved from 177.61124 to 165.70166, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 39/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 282.0085 - mean_absolute_error: 13.4348 - val_loss: 147.9275 - val_mean_absolute_error: 9.4231\n",
      "\n",
      "Epoch 00039: val_loss improved from 165.70166 to 147.92749, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 40/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 227.1613 - mean_absolute_error: 11.6551 - val_loss: 128.5554 - val_mean_absolute_error: 8.8493\n",
      "\n",
      "Epoch 00040: val_loss improved from 147.92749 to 128.55543, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 41/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 225.8192 - mean_absolute_error: 11.8584 - val_loss: 116.1730 - val_mean_absolute_error: 8.3729\n",
      "\n",
      "Epoch 00041: val_loss improved from 128.55543 to 116.17301, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 42/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 246.5478 - mean_absolute_error: 12.4239 - val_loss: 124.7829 - val_mean_absolute_error: 8.6420\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 116.17301\n",
      "Epoch 43/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 261.1763 - mean_absolute_error: 12.6671 - val_loss: 129.7366 - val_mean_absolute_error: 8.7622\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 116.17301\n",
      "Epoch 44/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 263.4186 - mean_absolute_error: 12.3668 - val_loss: 107.4168 - val_mean_absolute_error: 7.9601\n",
      "\n",
      "Epoch 00044: val_loss improved from 116.17301 to 107.41675, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 45/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 250.0977 - mean_absolute_error: 12.2539 - val_loss: 105.0898 - val_mean_absolute_error: 7.7395\n",
      "\n",
      "Epoch 00045: val_loss improved from 107.41675 to 105.08982, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 46/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 216.8108 - mean_absolute_error: 11.4258 - val_loss: 102.9742 - val_mean_absolute_error: 7.6941\n",
      "\n",
      "Epoch 00046: val_loss improved from 105.08982 to 102.97417, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 47/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.2951 - mean_absolute_error: 10.8885 - val_loss: 90.9247 - val_mean_absolute_error: 7.0772\n",
      "\n",
      "Epoch 00047: val_loss improved from 102.97417 to 90.92465, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 48/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.5734 - mean_absolute_error: 10.8819 - val_loss: 78.4832 - val_mean_absolute_error: 6.5457\n",
      "\n",
      "Epoch 00048: val_loss improved from 90.92465 to 78.48322, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 49/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.4192 - mean_absolute_error: 11.2631 - val_loss: 72.2899 - val_mean_absolute_error: 6.3095\n",
      "\n",
      "Epoch 00049: val_loss improved from 78.48322 to 72.28987, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 50/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.6272 - mean_absolute_error: 10.3735 - val_loss: 70.3012 - val_mean_absolute_error: 6.1588\n",
      "\n",
      "Epoch 00050: val_loss improved from 72.28987 to 70.30119, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 51/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.4069 - mean_absolute_error: 10.7901 - val_loss: 64.8267 - val_mean_absolute_error: 5.8750\n",
      "\n",
      "Epoch 00051: val_loss improved from 70.30119 to 64.82673, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 52/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.8156 - mean_absolute_error: 10.5400 - val_loss: 68.6666 - val_mean_absolute_error: 5.9919\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 64.82673\n",
      "Epoch 53/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.7864 - mean_absolute_error: 11.4301 - val_loss: 57.9193 - val_mean_absolute_error: 5.6733\n",
      "\n",
      "Epoch 00053: val_loss improved from 64.82673 to 57.91927, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 54/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.3657 - mean_absolute_error: 10.5717 - val_loss: 62.2326 - val_mean_absolute_error: 5.9034\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 57.91927\n",
      "Epoch 55/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.1247 - mean_absolute_error: 10.0108 - val_loss: 74.8096 - val_mean_absolute_error: 6.4196\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 57.91927\n",
      "Epoch 56/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.5896 - mean_absolute_error: 10.1336 - val_loss: 52.1119 - val_mean_absolute_error: 5.3400\n",
      "\n",
      "Epoch 00056: val_loss improved from 57.91927 to 52.11191, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 57/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 176.6272 - mean_absolute_error: 10.5917 - val_loss: 50.4617 - val_mean_absolute_error: 5.1303\n",
      "\n",
      "Epoch 00057: val_loss improved from 52.11191 to 50.46170, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 58/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.1561 - mean_absolute_error: 10.8321 - val_loss: 72.9529 - val_mean_absolute_error: 6.7035\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 50.46170\n",
      "Epoch 59/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.4199 - mean_absolute_error: 11.1355 - val_loss: 54.7043 - val_mean_absolute_error: 5.3793\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 50.46170\n",
      "Epoch 60/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.7442 - mean_absolute_error: 10.7136 - val_loss: 54.8924 - val_mean_absolute_error: 5.4090\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 50.46170\n",
      "Epoch 61/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 165.8933 - mean_absolute_error: 9.8811 - val_loss: 52.2228 - val_mean_absolute_error: 5.1592\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 50.46170\n",
      "Epoch 62/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.0467 - mean_absolute_error: 10.4533 - val_loss: 55.2327 - val_mean_absolute_error: 5.3888\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 50.46170\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 177.9091 - mean_absolute_error: 10.3377 - val_loss: 49.4965 - val_mean_absolute_error: 4.9145\n",
      "\n",
      "Epoch 00063: val_loss improved from 50.46170 to 49.49650, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 64/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.3918 - mean_absolute_error: 10.0616 - val_loss: 50.1273 - val_mean_absolute_error: 5.0505\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 49.49650\n",
      "Epoch 65/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.7595 - mean_absolute_error: 9.6491 - val_loss: 59.3924 - val_mean_absolute_error: 5.7672\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 49.49650\n",
      "Epoch 66/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 195.3224 - mean_absolute_error: 10.8680 - val_loss: 61.6981 - val_mean_absolute_error: 5.6813\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 49.49650\n",
      "Epoch 67/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.5066 - mean_absolute_error: 10.4395 - val_loss: 46.6505 - val_mean_absolute_error: 4.7711\n",
      "\n",
      "Epoch 00067: val_loss improved from 49.49650 to 46.65053, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 68/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.6531 - mean_absolute_error: 9.2953 - val_loss: 48.4057 - val_mean_absolute_error: 4.8819\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 46.65053\n",
      "Epoch 69/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.2437 - mean_absolute_error: 10.1363 - val_loss: 46.5924 - val_mean_absolute_error: 4.8416\n",
      "\n",
      "Epoch 00069: val_loss improved from 46.65053 to 46.59243, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 70/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.3369 - mean_absolute_error: 10.0019 - val_loss: 54.2016 - val_mean_absolute_error: 5.4403\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 46.59243\n",
      "Epoch 71/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.4038 - mean_absolute_error: 10.2628 - val_loss: 50.6523 - val_mean_absolute_error: 5.0039\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 46.59243\n",
      "Epoch 72/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.9169 - mean_absolute_error: 9.4950 - val_loss: 72.9283 - val_mean_absolute_error: 6.7975\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 46.59243\n",
      "Epoch 73/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 228.8919 - mean_absolute_error: 11.8366 - val_loss: 71.3687 - val_mean_absolute_error: 6.4866\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 46.59243\n",
      "Epoch 74/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.8618 - mean_absolute_error: 10.3040 - val_loss: 47.3115 - val_mean_absolute_error: 4.7566\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 46.59243\n",
      "Epoch 75/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.4674 - mean_absolute_error: 10.2593 - val_loss: 44.4460 - val_mean_absolute_error: 4.7194\n",
      "\n",
      "Epoch 00075: val_loss improved from 46.59243 to 44.44596, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 76/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.5093 - mean_absolute_error: 9.9843 - val_loss: 46.4410 - val_mean_absolute_error: 4.8622\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 44.44596\n",
      "Epoch 77/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.7267 - mean_absolute_error: 9.9367 - val_loss: 41.7170 - val_mean_absolute_error: 4.5393\n",
      "\n",
      "Epoch 00077: val_loss improved from 44.44596 to 41.71701, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 78/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.6341 - mean_absolute_error: 9.4019 - val_loss: 38.0251 - val_mean_absolute_error: 4.1653\n",
      "\n",
      "Epoch 00078: val_loss improved from 41.71701 to 38.02509, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 79/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.1145 - mean_absolute_error: 9.9197 - val_loss: 40.2325 - val_mean_absolute_error: 4.4340\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 38.02509\n",
      "Epoch 80/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.5069 - mean_absolute_error: 9.8266 - val_loss: 40.4341 - val_mean_absolute_error: 4.5229\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 38.02509\n",
      "Epoch 81/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.6129 - mean_absolute_error: 9.7205 - val_loss: 42.2580 - val_mean_absolute_error: 4.5876\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 38.02509\n",
      "Epoch 82/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.5910 - mean_absolute_error: 9.6265 - val_loss: 43.1950 - val_mean_absolute_error: 4.5697\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 38.02509\n",
      "Epoch 83/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.8606 - mean_absolute_error: 10.3628 - val_loss: 56.4006 - val_mean_absolute_error: 5.7915\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 38.02509\n",
      "Epoch 84/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.9373 - mean_absolute_error: 10.0538 - val_loss: 41.8215 - val_mean_absolute_error: 4.6059\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 38.02509\n",
      "Epoch 85/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.6335 - mean_absolute_error: 9.3565 - val_loss: 37.7437 - val_mean_absolute_error: 4.3723\n",
      "\n",
      "Epoch 00085: val_loss improved from 38.02509 to 37.74371, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 86/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.7050 - mean_absolute_error: 9.7474 - val_loss: 34.1630 - val_mean_absolute_error: 4.0346\n",
      "\n",
      "Epoch 00086: val_loss improved from 37.74371 to 34.16305, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 87/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.4366 - mean_absolute_error: 10.4672 - val_loss: 32.4960 - val_mean_absolute_error: 3.9365\n",
      "\n",
      "Epoch 00087: val_loss improved from 34.16305 to 32.49605, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 88/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.4282 - mean_absolute_error: 9.4400 - val_loss: 33.0375 - val_mean_absolute_error: 4.0044\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 32.49605\n",
      "Epoch 89/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.5589 - mean_absolute_error: 10.1264 - val_loss: 38.5528 - val_mean_absolute_error: 4.4303\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 32.49605\n",
      "Epoch 90/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.8351 - mean_absolute_error: 9.6445 - val_loss: 37.8291 - val_mean_absolute_error: 4.4387\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 32.49605\n",
      "Epoch 91/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.1379 - mean_absolute_error: 10.5902 - val_loss: 35.6460 - val_mean_absolute_error: 4.1917\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 32.49605\n",
      "Epoch 92/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.8778 - mean_absolute_error: 9.8924 - val_loss: 64.2034 - val_mean_absolute_error: 6.1918\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 32.49605\n",
      "Epoch 93/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 197.6043 - mean_absolute_error: 10.8665 - val_loss: 46.7332 - val_mean_absolute_error: 4.9346\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 32.49605\n",
      "Epoch 94/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.7625 - mean_absolute_error: 10.0332 - val_loss: 48.8993 - val_mean_absolute_error: 5.2589\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 32.49605\n",
      "Epoch 95/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.6512 - mean_absolute_error: 9.5830 - val_loss: 35.3517 - val_mean_absolute_error: 4.1773\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 32.49605\n",
      "Epoch 96/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.9687 - mean_absolute_error: 9.7468 - val_loss: 32.4680 - val_mean_absolute_error: 3.9954\n",
      "\n",
      "Epoch 00096: val_loss improved from 32.49605 to 32.46796, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 97/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.2010 - mean_absolute_error: 10.3114 - val_loss: 44.4710 - val_mean_absolute_error: 4.8960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00097: val_loss did not improve from 32.46796\n",
      "Epoch 98/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.3389 - mean_absolute_error: 10.1965 - val_loss: 42.0190 - val_mean_absolute_error: 4.6930\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 32.46796\n",
      "Epoch 99/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.1724 - mean_absolute_error: 9.4319 - val_loss: 43.6781 - val_mean_absolute_error: 4.9796\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 32.46796\n",
      "Epoch 100/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.8280 - mean_absolute_error: 9.8329 - val_loss: 38.7603 - val_mean_absolute_error: 4.5003\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 32.46796\n",
      "Epoch 101/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5809 - mean_absolute_error: 9.4248 - val_loss: 31.4486 - val_mean_absolute_error: 3.9889\n",
      "\n",
      "Epoch 00101: val_loss improved from 32.46796 to 31.44856, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 102/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.2922 - mean_absolute_error: 9.6836 - val_loss: 33.7066 - val_mean_absolute_error: 4.1067\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 31.44856\n",
      "Epoch 103/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.5802 - mean_absolute_error: 9.9567 - val_loss: 38.6320 - val_mean_absolute_error: 4.4919\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 31.44856\n",
      "Epoch 104/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.3776 - mean_absolute_error: 9.6010 - val_loss: 36.5314 - val_mean_absolute_error: 4.4726\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 31.44856\n",
      "Epoch 105/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.3350 - mean_absolute_error: 9.0061 - val_loss: 34.4595 - val_mean_absolute_error: 4.2021\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 31.44856\n",
      "Epoch 106/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.4313 - mean_absolute_error: 9.0354 - val_loss: 35.6320 - val_mean_absolute_error: 4.4449\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 31.44856\n",
      "Epoch 107/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.6539 - mean_absolute_error: 9.5084 - val_loss: 37.0796 - val_mean_absolute_error: 4.5430\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 31.44856\n",
      "Epoch 108/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.3622 - mean_absolute_error: 9.5529 - val_loss: 46.4694 - val_mean_absolute_error: 5.3007\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 31.44856\n",
      "Epoch 109/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.7734 - mean_absolute_error: 9.6618 - val_loss: 43.0437 - val_mean_absolute_error: 4.8384\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 31.44856\n",
      "Epoch 110/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.4566 - mean_absolute_error: 9.6276 - val_loss: 32.1933 - val_mean_absolute_error: 4.1430\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 31.44856\n",
      "Epoch 111/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.5708 - mean_absolute_error: 9.2021 - val_loss: 38.0583 - val_mean_absolute_error: 4.5846\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 31.44856\n",
      "Epoch 112/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.9207 - mean_absolute_error: 9.7090 - val_loss: 47.1437 - val_mean_absolute_error: 5.2962\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 31.44856\n",
      "Epoch 113/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.0848 - mean_absolute_error: 9.8066 - val_loss: 38.9128 - val_mean_absolute_error: 4.5322\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 31.44856\n",
      "Epoch 114/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.7626 - mean_absolute_error: 9.7871 - val_loss: 28.8125 - val_mean_absolute_error: 3.7101\n",
      "\n",
      "Epoch 00114: val_loss improved from 31.44856 to 28.81247, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 115/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.5676 - mean_absolute_error: 9.3658 - val_loss: 31.4656 - val_mean_absolute_error: 4.0031\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 28.81247\n",
      "Epoch 116/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.2122 - mean_absolute_error: 9.5589 - val_loss: 30.8463 - val_mean_absolute_error: 3.8491\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 28.81247\n",
      "Epoch 117/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 162.0408 - mean_absolute_error: 9.9798 - val_loss: 31.4649 - val_mean_absolute_error: 4.0683\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 28.81247\n",
      "Epoch 118/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.7341 - mean_absolute_error: 9.2984 - val_loss: 31.9381 - val_mean_absolute_error: 4.0472\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 28.81247\n",
      "Epoch 119/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.6977 - mean_absolute_error: 9.7681 - val_loss: 32.1220 - val_mean_absolute_error: 4.0410\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 28.81247\n",
      "Epoch 120/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.3877 - mean_absolute_error: 9.8078 - val_loss: 28.3861 - val_mean_absolute_error: 3.6971\n",
      "\n",
      "Epoch 00120: val_loss improved from 28.81247 to 28.38607, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 121/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.8517 - mean_absolute_error: 9.7759 - val_loss: 28.4779 - val_mean_absolute_error: 3.8170\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 28.38607\n",
      "Epoch 122/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.0698 - mean_absolute_error: 9.8636 - val_loss: 35.6059 - val_mean_absolute_error: 4.3965\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 28.38607\n",
      "Epoch 123/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.1573 - mean_absolute_error: 9.7132 - val_loss: 36.8421 - val_mean_absolute_error: 4.4086\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 28.38607\n",
      "Epoch 124/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.4501 - mean_absolute_error: 9.8884 - val_loss: 43.0559 - val_mean_absolute_error: 5.0565\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 28.38607\n",
      "Epoch 125/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.4917 - mean_absolute_error: 9.7763 - val_loss: 31.3047 - val_mean_absolute_error: 4.0225\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 28.38607\n",
      "Epoch 126/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.3990 - mean_absolute_error: 8.9907 - val_loss: 35.8628 - val_mean_absolute_error: 4.3464\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 28.38607\n",
      "Epoch 127/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.7563 - mean_absolute_error: 10.0211 - val_loss: 31.0990 - val_mean_absolute_error: 3.9607\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 28.38607\n",
      "Epoch 128/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.8861 - mean_absolute_error: 10.1578 - val_loss: 32.3343 - val_mean_absolute_error: 4.1203\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 28.38607\n",
      "Epoch 129/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.3203 - mean_absolute_error: 10.0596 - val_loss: 33.9089 - val_mean_absolute_error: 4.2298\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 28.38607\n",
      "Epoch 130/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.1371 - mean_absolute_error: 9.4565 - val_loss: 30.9960 - val_mean_absolute_error: 3.9748\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 28.38607\n",
      "Epoch 131/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.2421 - mean_absolute_error: 9.9014 - val_loss: 40.0778 - val_mean_absolute_error: 4.7848\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 28.38607\n",
      "Epoch 132/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.8274 - mean_absolute_error: 9.1919 - val_loss: 45.3452 - val_mean_absolute_error: 5.1508\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 28.38607\n",
      "Epoch 133/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.8331 - mean_absolute_error: 9.6773 - val_loss: 44.4465 - val_mean_absolute_error: 5.0465\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 28.38607\n",
      "Epoch 134/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 145.5825 - mean_absolute_error: 9.5877 - val_loss: 27.0456 - val_mean_absolute_error: 3.7096\n",
      "\n",
      "Epoch 00134: val_loss improved from 28.38607 to 27.04559, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 135/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.7714 - mean_absolute_error: 9.7007 - val_loss: 31.2841 - val_mean_absolute_error: 3.8697\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 27.04559\n",
      "Epoch 136/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.3894 - mean_absolute_error: 9.6163 - val_loss: 34.1872 - val_mean_absolute_error: 4.3579\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 27.04559\n",
      "Epoch 137/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.9263 - mean_absolute_error: 10.1067 - val_loss: 29.8559 - val_mean_absolute_error: 3.9880\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 27.04559\n",
      "Epoch 138/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.5009 - mean_absolute_error: 9.6539 - val_loss: 27.2329 - val_mean_absolute_error: 3.6567\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 27.04559\n",
      "Epoch 139/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.8002 - mean_absolute_error: 9.3420 - val_loss: 31.8521 - val_mean_absolute_error: 4.0662\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 27.04559\n",
      "Epoch 140/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.1546 - mean_absolute_error: 9.5925 - val_loss: 49.4060 - val_mean_absolute_error: 5.4865\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 27.04559\n",
      "Epoch 141/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.1811 - mean_absolute_error: 9.5622 - val_loss: 28.7862 - val_mean_absolute_error: 3.7007\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 27.04559\n",
      "Epoch 142/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.3458 - mean_absolute_error: 9.7456 - val_loss: 27.2875 - val_mean_absolute_error: 3.5868\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 27.04559\n",
      "Epoch 143/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.2757 - mean_absolute_error: 10.1384 - val_loss: 35.3498 - val_mean_absolute_error: 4.2836\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 27.04559\n",
      "Epoch 144/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 162.9887 - mean_absolute_error: 9.7326 - val_loss: 36.2643 - val_mean_absolute_error: 4.4258\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 27.04559\n",
      "Epoch 145/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.7977 - mean_absolute_error: 9.9241 - val_loss: 27.4227 - val_mean_absolute_error: 3.6381\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 27.04559\n",
      "Epoch 146/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.8017 - mean_absolute_error: 9.2642 - val_loss: 31.0435 - val_mean_absolute_error: 3.9662\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 27.04559\n",
      "Epoch 147/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.5137 - mean_absolute_error: 9.3562 - val_loss: 31.8019 - val_mean_absolute_error: 4.0764\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 27.04559\n",
      "Epoch 148/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.9658 - mean_absolute_error: 10.1290 - val_loss: 30.1713 - val_mean_absolute_error: 3.9312\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 27.04559\n",
      "Epoch 149/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.9068 - mean_absolute_error: 9.6969 - val_loss: 29.4279 - val_mean_absolute_error: 3.8788\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 27.04559\n",
      "Epoch 150/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.4840 - mean_absolute_error: 9.6483 - val_loss: 32.6969 - val_mean_absolute_error: 4.2943\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 27.04559\n",
      "Epoch 151/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.8151 - mean_absolute_error: 9.5828 - val_loss: 33.0332 - val_mean_absolute_error: 4.2681\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 27.04559\n",
      "Epoch 152/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.8593 - mean_absolute_error: 9.0675 - val_loss: 27.7404 - val_mean_absolute_error: 3.7898\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 27.04559\n",
      "Epoch 153/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.9401 - mean_absolute_error: 9.4375 - val_loss: 42.1063 - val_mean_absolute_error: 4.9355\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 27.04559\n",
      "Epoch 154/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.0692 - mean_absolute_error: 9.7155 - val_loss: 41.1321 - val_mean_absolute_error: 4.7997\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 27.04559\n",
      "Epoch 155/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.6815 - mean_absolute_error: 9.1210 - val_loss: 37.5443 - val_mean_absolute_error: 4.6050\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 27.04559\n",
      "Epoch 156/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.5481 - mean_absolute_error: 9.3593 - val_loss: 29.2433 - val_mean_absolute_error: 3.9798\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 27.04559\n",
      "Epoch 157/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.4019 - mean_absolute_error: 9.3661 - val_loss: 29.0018 - val_mean_absolute_error: 3.8856\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 27.04559\n",
      "Epoch 158/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.8572 - mean_absolute_error: 9.5891 - val_loss: 26.1302 - val_mean_absolute_error: 3.5754\n",
      "\n",
      "Epoch 00158: val_loss improved from 27.04559 to 26.13017, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 159/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.1548 - mean_absolute_error: 9.8522 - val_loss: 25.7646 - val_mean_absolute_error: 3.5121\n",
      "\n",
      "Epoch 00159: val_loss improved from 26.13017 to 25.76464, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 160/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.3190 - mean_absolute_error: 9.4861 - val_loss: 29.2217 - val_mean_absolute_error: 3.8897\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 25.76464\n",
      "Epoch 161/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.2494 - mean_absolute_error: 9.5948 - val_loss: 25.7641 - val_mean_absolute_error: 3.5430\n",
      "\n",
      "Epoch 00161: val_loss improved from 25.76464 to 25.76406, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 162/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.2785 - mean_absolute_error: 9.6565 - val_loss: 28.1624 - val_mean_absolute_error: 3.7898\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 25.76406\n",
      "Epoch 163/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.8610 - mean_absolute_error: 9.3379 - val_loss: 29.7829 - val_mean_absolute_error: 3.9548\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 25.76406\n",
      "Epoch 164/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.0757 - mean_absolute_error: 8.7305 - val_loss: 31.4378 - val_mean_absolute_error: 3.9633\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 25.76406\n",
      "Epoch 165/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.6162 - mean_absolute_error: 9.3958 - val_loss: 28.9310 - val_mean_absolute_error: 3.8973\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 25.76406\n",
      "Epoch 166/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.0633 - mean_absolute_error: 9.4718 - val_loss: 34.5612 - val_mean_absolute_error: 4.3496\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 25.76406\n",
      "Epoch 167/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.0163 - mean_absolute_error: 10.2553 - val_loss: 26.9415 - val_mean_absolute_error: 3.5661\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 25.76406\n",
      "Epoch 168/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.4403 - mean_absolute_error: 9.7743 - val_loss: 35.7797 - val_mean_absolute_error: 4.4303\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 25.76406\n",
      "Epoch 169/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.8862 - mean_absolute_error: 9.2615 - val_loss: 27.4361 - val_mean_absolute_error: 3.7324\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 25.76406\n",
      "Epoch 170/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 144.8136 - mean_absolute_error: 9.4975 - val_loss: 26.2090 - val_mean_absolute_error: 3.6517\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 25.76406\n",
      "Epoch 171/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.6202 - mean_absolute_error: 9.5258 - val_loss: 28.1403 - val_mean_absolute_error: 3.7245\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 25.76406\n",
      "Epoch 172/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.1467 - mean_absolute_error: 9.9088 - val_loss: 27.9458 - val_mean_absolute_error: 3.8125\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 25.76406\n",
      "Epoch 173/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.1399 - mean_absolute_error: 9.8501 - val_loss: 29.3773 - val_mean_absolute_error: 4.0815\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 25.76406\n",
      "Epoch 174/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.8586 - mean_absolute_error: 10.0134 - val_loss: 38.0194 - val_mean_absolute_error: 4.7575\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 25.76406\n",
      "Epoch 175/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.2474 - mean_absolute_error: 9.4812 - val_loss: 36.6112 - val_mean_absolute_error: 4.4620\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 25.76406\n",
      "Epoch 176/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.5166 - mean_absolute_error: 9.3439 - val_loss: 32.1759 - val_mean_absolute_error: 4.1230\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 25.76406\n",
      "Epoch 177/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.0885 - mean_absolute_error: 9.5421 - val_loss: 37.6137 - val_mean_absolute_error: 4.7359\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 25.76406\n",
      "Epoch 178/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.8761 - mean_absolute_error: 10.4637 - val_loss: 31.6000 - val_mean_absolute_error: 4.2175\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 25.76406\n",
      "Epoch 179/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.1950 - mean_absolute_error: 9.4599 - val_loss: 27.0550 - val_mean_absolute_error: 3.7880\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 25.76406\n",
      "Epoch 180/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.7362 - mean_absolute_error: 9.2467 - val_loss: 30.5766 - val_mean_absolute_error: 4.0682\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 25.76406\n",
      "Epoch 181/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.0538 - mean_absolute_error: 9.8255 - val_loss: 34.1061 - val_mean_absolute_error: 4.3802\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 25.76406\n",
      "Epoch 182/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.1255 - mean_absolute_error: 9.4560 - val_loss: 25.3500 - val_mean_absolute_error: 3.7464\n",
      "\n",
      "Epoch 00182: val_loss improved from 25.76406 to 25.35000, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 183/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.5178 - mean_absolute_error: 9.1690 - val_loss: 25.1888 - val_mean_absolute_error: 3.4901\n",
      "\n",
      "Epoch 00183: val_loss improved from 25.35000 to 25.18880, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 184/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.1053 - mean_absolute_error: 9.4656 - val_loss: 24.3458 - val_mean_absolute_error: 3.5344\n",
      "\n",
      "Epoch 00184: val_loss improved from 25.18880 to 24.34581, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 185/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.7550 - mean_absolute_error: 9.6173 - val_loss: 28.0975 - val_mean_absolute_error: 3.7950\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 24.34581\n",
      "Epoch 186/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.3407 - mean_absolute_error: 9.1396 - val_loss: 26.7204 - val_mean_absolute_error: 3.6377\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 24.34581\n",
      "Epoch 187/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.0535 - mean_absolute_error: 9.2229 - val_loss: 27.2039 - val_mean_absolute_error: 3.8979\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 24.34581\n",
      "Epoch 188/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.9172 - mean_absolute_error: 9.0354 - val_loss: 27.1322 - val_mean_absolute_error: 3.8090\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 24.34581\n",
      "Epoch 189/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.9577 - mean_absolute_error: 9.5960 - val_loss: 29.3562 - val_mean_absolute_error: 3.9036\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 24.34581\n",
      "Epoch 190/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.1568 - mean_absolute_error: 10.2306 - val_loss: 29.7914 - val_mean_absolute_error: 4.0240\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 24.34581\n",
      "Epoch 191/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.2237 - mean_absolute_error: 9.5131 - val_loss: 28.3632 - val_mean_absolute_error: 3.9544\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 24.34581\n",
      "Epoch 192/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.0330 - mean_absolute_error: 9.4180 - val_loss: 26.8231 - val_mean_absolute_error: 3.7507\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 24.34581\n",
      "Epoch 193/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.0705 - mean_absolute_error: 9.3111 - val_loss: 38.9404 - val_mean_absolute_error: 4.8053\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 24.34581\n",
      "Epoch 194/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.8881 - mean_absolute_error: 9.6574 - val_loss: 39.5176 - val_mean_absolute_error: 4.8872\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 24.34581\n",
      "Epoch 195/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.4468 - mean_absolute_error: 9.9101 - val_loss: 31.5367 - val_mean_absolute_error: 4.2406\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 24.34581\n",
      "Epoch 196/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.0289 - mean_absolute_error: 9.4435 - val_loss: 24.2154 - val_mean_absolute_error: 3.5320\n",
      "\n",
      "Epoch 00196: val_loss improved from 24.34581 to 24.21539, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 197/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.7211 - mean_absolute_error: 9.4362 - val_loss: 27.8852 - val_mean_absolute_error: 3.7597\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 24.21539\n",
      "Epoch 198/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.5511 - mean_absolute_error: 9.2854 - val_loss: 24.2594 - val_mean_absolute_error: 3.5259\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 24.21539\n",
      "Epoch 199/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.6684 - mean_absolute_error: 9.8522 - val_loss: 32.0276 - val_mean_absolute_error: 4.0582\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 24.21539\n",
      "Epoch 200/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.4699 - mean_absolute_error: 9.5319 - val_loss: 24.4452 - val_mean_absolute_error: 3.5149\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 24.21539\n",
      "Epoch 201/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.3515 - mean_absolute_error: 9.1294 - val_loss: 35.1252 - val_mean_absolute_error: 4.5141\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 24.21539\n",
      "Epoch 202/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.6794 - mean_absolute_error: 8.7137 - val_loss: 25.9865 - val_mean_absolute_error: 3.7080\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 24.21539\n",
      "Epoch 203/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.7883 - mean_absolute_error: 8.9944 - val_loss: 28.5709 - val_mean_absolute_error: 3.9549\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 24.21539\n",
      "Epoch 204/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.2232 - mean_absolute_error: 9.4025 - val_loss: 25.2364 - val_mean_absolute_error: 3.7331\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 24.21539\n",
      "Epoch 205/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.0271 - mean_absolute_error: 9.5857 - val_loss: 25.0540 - val_mean_absolute_error: 3.6130\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 24.21539\n",
      "Epoch 206/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 141.5137 - mean_absolute_error: 9.2861 - val_loss: 33.2528 - val_mean_absolute_error: 4.3386\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 24.21539\n",
      "Epoch 207/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.8584 - mean_absolute_error: 9.1292 - val_loss: 42.8384 - val_mean_absolute_error: 5.0364\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 24.21539\n",
      "Epoch 208/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.2762 - mean_absolute_error: 9.5508 - val_loss: 23.7745 - val_mean_absolute_error: 3.4845\n",
      "\n",
      "Epoch 00208: val_loss improved from 24.21539 to 23.77446, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 209/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.6354 - mean_absolute_error: 9.3307 - val_loss: 26.3435 - val_mean_absolute_error: 3.6910\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 23.77446\n",
      "Epoch 210/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.0788 - mean_absolute_error: 9.4599 - val_loss: 31.2664 - val_mean_absolute_error: 4.1932\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 23.77446\n",
      "Epoch 211/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.1061 - mean_absolute_error: 9.3256 - val_loss: 25.5723 - val_mean_absolute_error: 3.6543\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 23.77446\n",
      "Epoch 212/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.6892 - mean_absolute_error: 9.0666 - val_loss: 32.0306 - val_mean_absolute_error: 4.0682\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 23.77446\n",
      "Epoch 213/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.4643 - mean_absolute_error: 9.4858 - val_loss: 25.6602 - val_mean_absolute_error: 3.6235\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 23.77446\n",
      "Epoch 214/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.9414 - mean_absolute_error: 9.1218 - val_loss: 34.5211 - val_mean_absolute_error: 4.3521\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 23.77446\n",
      "Epoch 215/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.4006 - mean_absolute_error: 8.9350 - val_loss: 25.5196 - val_mean_absolute_error: 3.6085\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 23.77446\n",
      "Epoch 216/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.3492 - mean_absolute_error: 9.5188 - val_loss: 28.1977 - val_mean_absolute_error: 3.8682\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 23.77446\n",
      "Epoch 217/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.9251 - mean_absolute_error: 9.2840 - val_loss: 24.8618 - val_mean_absolute_error: 3.6762\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 23.77446\n",
      "Epoch 218/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.5564 - mean_absolute_error: 9.3237 - val_loss: 29.4155 - val_mean_absolute_error: 4.0544\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 23.77446\n",
      "Epoch 219/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.7047 - mean_absolute_error: 10.1866 - val_loss: 29.5294 - val_mean_absolute_error: 3.9769\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 23.77446\n",
      "Epoch 220/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.4541 - mean_absolute_error: 9.6953 - val_loss: 29.9405 - val_mean_absolute_error: 3.9430\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 23.77446\n",
      "Epoch 221/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.0986 - mean_absolute_error: 9.3328 - val_loss: 22.4208 - val_mean_absolute_error: 3.3498\n",
      "\n",
      "Epoch 00221: val_loss improved from 23.77446 to 22.42083, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 222/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.2387 - mean_absolute_error: 9.0230 - val_loss: 32.1772 - val_mean_absolute_error: 4.1870\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 22.42083\n",
      "Epoch 223/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.6526 - mean_absolute_error: 9.7044 - val_loss: 29.5224 - val_mean_absolute_error: 3.9695\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 22.42083\n",
      "Epoch 224/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 156.8193 - mean_absolute_error: 9.9322 - val_loss: 29.5978 - val_mean_absolute_error: 3.9278\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 22.42083\n",
      "Epoch 225/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 121.8962 - mean_absolute_error: 8.6926 - val_loss: 25.2675 - val_mean_absolute_error: 3.5738\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 22.42083\n",
      "Epoch 226/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.8954 - mean_absolute_error: 9.3582 - val_loss: 30.4782 - val_mean_absolute_error: 3.9708\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 22.42083\n",
      "Epoch 227/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.5777 - mean_absolute_error: 9.5578 - val_loss: 36.3251 - val_mean_absolute_error: 4.4617\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 22.42083\n",
      "Epoch 228/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.5089 - mean_absolute_error: 9.3857 - val_loss: 24.5584 - val_mean_absolute_error: 3.3987\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 22.42083\n",
      "Epoch 229/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.9248 - mean_absolute_error: 9.6802 - val_loss: 24.8253 - val_mean_absolute_error: 3.5753\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 22.42083\n",
      "Epoch 230/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 124.3888 - mean_absolute_error: 8.8091 - val_loss: 26.0589 - val_mean_absolute_error: 3.5924\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 22.42083\n",
      "Epoch 231/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.6737 - mean_absolute_error: 9.4500 - val_loss: 29.1879 - val_mean_absolute_error: 3.9357\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 22.42083\n",
      "Epoch 232/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.7603 - mean_absolute_error: 8.8644 - val_loss: 43.9438 - val_mean_absolute_error: 5.2457\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 22.42083\n",
      "Epoch 233/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.0705 - mean_absolute_error: 9.4875 - val_loss: 44.5458 - val_mean_absolute_error: 5.2124\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 22.42083\n",
      "Epoch 234/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.2504 - mean_absolute_error: 9.4197 - val_loss: 24.3689 - val_mean_absolute_error: 3.5902\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 22.42083\n",
      "Epoch 235/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.0983 - mean_absolute_error: 9.2355 - val_loss: 25.5379 - val_mean_absolute_error: 3.6995\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 22.42083\n",
      "Epoch 236/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 120.7722 - mean_absolute_error: 8.7271 - val_loss: 32.5719 - val_mean_absolute_error: 4.1685\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 22.42083\n",
      "Epoch 237/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.7519 - mean_absolute_error: 9.1663 - val_loss: 32.4936 - val_mean_absolute_error: 4.1621\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 22.42083\n",
      "Epoch 238/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.4997 - mean_absolute_error: 9.2593 - val_loss: 35.8701 - val_mean_absolute_error: 4.5284\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 22.42083\n",
      "Epoch 239/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.2502 - mean_absolute_error: 9.2132 - val_loss: 29.9963 - val_mean_absolute_error: 4.0194\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 22.42083\n",
      "Epoch 240/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.4740 - mean_absolute_error: 8.8746 - val_loss: 29.0471 - val_mean_absolute_error: 3.9113\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 22.42083\n",
      "Epoch 241/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.4009 - mean_absolute_error: 8.9805 - val_loss: 29.0613 - val_mean_absolute_error: 3.8945\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 22.42083\n",
      "Epoch 242/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 141.3247 - mean_absolute_error: 9.2740 - val_loss: 25.5140 - val_mean_absolute_error: 3.6061\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 22.42083\n",
      "Epoch 243/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.1627 - mean_absolute_error: 9.6360 - val_loss: 23.2560 - val_mean_absolute_error: 3.4798\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 22.42083\n",
      "Epoch 244/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.5281 - mean_absolute_error: 9.2019 - val_loss: 22.9121 - val_mean_absolute_error: 3.5293\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 22.42083\n",
      "Epoch 245/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.8206 - mean_absolute_error: 9.1817 - val_loss: 24.5615 - val_mean_absolute_error: 3.6934\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 22.42083\n",
      "Epoch 246/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.1415 - mean_absolute_error: 9.9764 - val_loss: 26.2799 - val_mean_absolute_error: 3.7944\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 22.42083\n",
      "Epoch 247/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.1546 - mean_absolute_error: 9.5683 - val_loss: 25.7397 - val_mean_absolute_error: 3.6855\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 22.42083\n",
      "Epoch 248/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.0948 - mean_absolute_error: 9.7535 - val_loss: 34.7288 - val_mean_absolute_error: 4.3243\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 22.42083\n",
      "Epoch 249/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.0731 - mean_absolute_error: 9.4658 - val_loss: 23.4206 - val_mean_absolute_error: 3.4616\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 22.42083\n",
      "Epoch 250/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.8777 - mean_absolute_error: 9.3844 - val_loss: 26.3239 - val_mean_absolute_error: 3.6708\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 22.42083\n",
      "Epoch 251/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.9285 - mean_absolute_error: 9.1195 - val_loss: 29.0939 - val_mean_absolute_error: 4.0747\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 22.42083\n",
      "Epoch 252/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.8434 - mean_absolute_error: 8.8949 - val_loss: 22.3101 - val_mean_absolute_error: 3.3377\n",
      "\n",
      "Epoch 00252: val_loss improved from 22.42083 to 22.31011, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 253/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.0247 - mean_absolute_error: 9.8385 - val_loss: 26.0070 - val_mean_absolute_error: 3.7551\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 22.31011\n",
      "Epoch 254/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.4083 - mean_absolute_error: 9.2163 - val_loss: 26.1301 - val_mean_absolute_error: 3.6971\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 22.31011\n",
      "Epoch 255/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.3525 - mean_absolute_error: 9.4220 - val_loss: 28.8283 - val_mean_absolute_error: 3.9536\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 22.31011\n",
      "Epoch 256/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.4275 - mean_absolute_error: 9.5504 - val_loss: 26.9101 - val_mean_absolute_error: 3.7913\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 22.31011\n",
      "Epoch 257/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5101 - mean_absolute_error: 9.6993 - val_loss: 25.5044 - val_mean_absolute_error: 3.6663\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 22.31011\n",
      "Epoch 258/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.1223 - mean_absolute_error: 9.5903 - val_loss: 26.5462 - val_mean_absolute_error: 3.8409\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 22.31011\n",
      "Epoch 259/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.3694 - mean_absolute_error: 9.5509 - val_loss: 23.4500 - val_mean_absolute_error: 3.5095\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 22.31011\n",
      "Epoch 260/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.7128 - mean_absolute_error: 9.4910 - val_loss: 31.8095 - val_mean_absolute_error: 4.2952\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 22.31011\n",
      "Epoch 261/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.5990 - mean_absolute_error: 9.5082 - val_loss: 24.0604 - val_mean_absolute_error: 3.5403\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 22.31011\n",
      "Epoch 262/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.5155 - mean_absolute_error: 9.3703 - val_loss: 28.6378 - val_mean_absolute_error: 3.8409\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 22.31011\n",
      "Epoch 263/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.8584 - mean_absolute_error: 9.3882 - val_loss: 48.3415 - val_mean_absolute_error: 5.5972\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 22.31011\n",
      "Epoch 264/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.6986 - mean_absolute_error: 9.1510 - val_loss: 32.0361 - val_mean_absolute_error: 4.2224\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 22.31011\n",
      "Epoch 265/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.4812 - mean_absolute_error: 9.2240 - val_loss: 31.4290 - val_mean_absolute_error: 4.1876\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 22.31011\n",
      "Epoch 266/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.2413 - mean_absolute_error: 8.5593 - val_loss: 25.1243 - val_mean_absolute_error: 3.6607\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 22.31011\n",
      "Epoch 267/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.7872 - mean_absolute_error: 8.8536 - val_loss: 32.5506 - val_mean_absolute_error: 4.2907\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 22.31011\n",
      "Epoch 268/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.6186 - mean_absolute_error: 9.3723 - val_loss: 31.5309 - val_mean_absolute_error: 4.1852\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 22.31011\n",
      "Epoch 269/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.3634 - mean_absolute_error: 9.4015 - val_loss: 35.0852 - val_mean_absolute_error: 4.5463\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 22.31011\n",
      "Epoch 270/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.6955 - mean_absolute_error: 9.6601 - val_loss: 34.9319 - val_mean_absolute_error: 4.4496\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 22.31011\n",
      "Epoch 271/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.8970 - mean_absolute_error: 9.9505 - val_loss: 34.7658 - val_mean_absolute_error: 4.3768\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 22.31011\n",
      "Epoch 272/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.7665 - mean_absolute_error: 9.5780 - val_loss: 34.6365 - val_mean_absolute_error: 4.4356\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 22.31011\n",
      "Epoch 273/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.1730 - mean_absolute_error: 8.5862 - val_loss: 25.4210 - val_mean_absolute_error: 3.7115\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 22.31011\n",
      "Epoch 274/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.9314 - mean_absolute_error: 8.9557 - val_loss: 28.2919 - val_mean_absolute_error: 3.8105\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 22.31011\n",
      "Epoch 275/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.1987 - mean_absolute_error: 9.4842 - val_loss: 25.3648 - val_mean_absolute_error: 3.6157\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 22.31011\n",
      "Epoch 276/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.4025 - mean_absolute_error: 8.8703 - val_loss: 27.0023 - val_mean_absolute_error: 3.8638\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 22.31011\n",
      "Epoch 277/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.2732 - mean_absolute_error: 9.2105 - val_loss: 24.1453 - val_mean_absolute_error: 3.5914\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 22.31011\n",
      "Epoch 278/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.2046 - mean_absolute_error: 9.3249 - val_loss: 38.5083 - val_mean_absolute_error: 4.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00278: val_loss did not improve from 22.31011\n",
      "Epoch 279/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.8052 - mean_absolute_error: 9.1501 - val_loss: 38.9777 - val_mean_absolute_error: 4.8063\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 22.31011\n",
      "Epoch 280/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.3403 - mean_absolute_error: 9.4381 - val_loss: 36.8608 - val_mean_absolute_error: 4.7547\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 22.31011\n",
      "Epoch 281/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.3556 - mean_absolute_error: 9.3630 - val_loss: 23.7396 - val_mean_absolute_error: 3.5395\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 22.31011\n",
      "Epoch 282/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.1989 - mean_absolute_error: 8.9971 - val_loss: 26.4496 - val_mean_absolute_error: 3.7269\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 22.31011\n",
      "Epoch 283/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.5667 - mean_absolute_error: 9.4095 - val_loss: 26.7874 - val_mean_absolute_error: 3.7615\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 22.31011\n",
      "Epoch 284/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.4389 - mean_absolute_error: 8.8176 - val_loss: 24.1343 - val_mean_absolute_error: 3.4854\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 22.31011\n",
      "Epoch 285/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.6398 - mean_absolute_error: 9.4221 - val_loss: 24.8911 - val_mean_absolute_error: 3.6524\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 22.31011\n",
      "Epoch 286/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.8372 - mean_absolute_error: 9.4577 - val_loss: 23.2584 - val_mean_absolute_error: 3.5362\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 22.31011\n",
      "Epoch 287/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.4507 - mean_absolute_error: 8.7197 - val_loss: 30.6516 - val_mean_absolute_error: 4.0476\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 22.31011\n",
      "Epoch 288/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.5339 - mean_absolute_error: 9.1647 - val_loss: 45.0228 - val_mean_absolute_error: 5.3853\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 22.31011\n",
      "Epoch 289/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.4965 - mean_absolute_error: 9.2949 - val_loss: 23.6078 - val_mean_absolute_error: 3.4768\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 22.31011\n",
      "Epoch 290/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5890 - mean_absolute_error: 9.3979 - val_loss: 26.1831 - val_mean_absolute_error: 3.7412\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 22.31011\n",
      "Epoch 291/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.7357 - mean_absolute_error: 9.2102 - val_loss: 23.8785 - val_mean_absolute_error: 3.5113\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 22.31011\n",
      "Epoch 292/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.0592 - mean_absolute_error: 9.0375 - val_loss: 25.4684 - val_mean_absolute_error: 3.7173\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 22.31011\n",
      "Epoch 293/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.2999 - mean_absolute_error: 9.0789 - val_loss: 22.3804 - val_mean_absolute_error: 3.3726\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 22.31011\n",
      "Epoch 294/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.6740 - mean_absolute_error: 9.0584 - val_loss: 24.5660 - val_mean_absolute_error: 3.7202\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 22.31011\n",
      "Epoch 295/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.1465 - mean_absolute_error: 9.1122 - val_loss: 24.8298 - val_mean_absolute_error: 3.6460\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 22.31011\n",
      "Epoch 296/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.4517 - mean_absolute_error: 9.6368 - val_loss: 25.9039 - val_mean_absolute_error: 3.8547\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 22.31011\n",
      "Epoch 297/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.0257 - mean_absolute_error: 9.4130 - val_loss: 34.2213 - val_mean_absolute_error: 4.4849\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 22.31011\n",
      "Epoch 298/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.2929 - mean_absolute_error: 8.9613 - val_loss: 34.7068 - val_mean_absolute_error: 4.5427\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 22.31011\n",
      "Epoch 299/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.3458 - mean_absolute_error: 9.1665 - val_loss: 34.1320 - val_mean_absolute_error: 4.4419\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 22.31011\n",
      "Epoch 300/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.2058 - mean_absolute_error: 9.0622 - val_loss: 33.4275 - val_mean_absolute_error: 4.4305\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 22.31011\n",
      "Epoch 301/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.0304 - mean_absolute_error: 9.4661 - val_loss: 20.8579 - val_mean_absolute_error: 3.2501\n",
      "\n",
      "Epoch 00301: val_loss improved from 22.31011 to 20.85791, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 302/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.1796 - mean_absolute_error: 8.8805 - val_loss: 22.0704 - val_mean_absolute_error: 3.4190\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 20.85791\n",
      "Epoch 303/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.3683 - mean_absolute_error: 8.8416 - val_loss: 24.9036 - val_mean_absolute_error: 3.7275\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 20.85791\n",
      "Epoch 304/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.2959 - mean_absolute_error: 9.3503 - val_loss: 22.4575 - val_mean_absolute_error: 3.3346\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 20.85791\n",
      "Epoch 305/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.0517 - mean_absolute_error: 9.1616 - val_loss: 23.4901 - val_mean_absolute_error: 3.3261\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 20.85791\n",
      "Epoch 306/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.7093 - mean_absolute_error: 9.2769 - val_loss: 23.0871 - val_mean_absolute_error: 3.5107\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 20.85791\n",
      "Epoch 307/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.9780 - mean_absolute_error: 8.8754 - val_loss: 27.3472 - val_mean_absolute_error: 3.8747\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 20.85791\n",
      "Epoch 308/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.5847 - mean_absolute_error: 9.1308 - val_loss: 29.0799 - val_mean_absolute_error: 4.1139\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 20.85791\n",
      "Epoch 309/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.0984 - mean_absolute_error: 8.9298 - val_loss: 22.7893 - val_mean_absolute_error: 3.4363\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 20.85791\n",
      "Epoch 310/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.9474 - mean_absolute_error: 8.7283 - val_loss: 22.4758 - val_mean_absolute_error: 3.4246\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 20.85791\n",
      "Epoch 311/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.3770 - mean_absolute_error: 9.4008 - val_loss: 30.2518 - val_mean_absolute_error: 4.2792\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 20.85791\n",
      "Epoch 312/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.3102 - mean_absolute_error: 9.6662 - val_loss: 37.1906 - val_mean_absolute_error: 4.4788\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 20.85791\n",
      "Epoch 313/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.9821 - mean_absolute_error: 9.8378 - val_loss: 24.8545 - val_mean_absolute_error: 3.5107\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 20.85791\n",
      "Epoch 314/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.8158 - mean_absolute_error: 9.2939 - val_loss: 32.4373 - val_mean_absolute_error: 4.2583\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 20.85791\n",
      "Epoch 315/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 127.6402 - mean_absolute_error: 8.9213 - val_loss: 27.9596 - val_mean_absolute_error: 3.8459\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 20.85791\n",
      "Epoch 316/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.8633 - mean_absolute_error: 9.0936 - val_loss: 30.2113 - val_mean_absolute_error: 4.0297\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 20.85791\n",
      "Epoch 317/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.9451 - mean_absolute_error: 9.1198 - val_loss: 24.1314 - val_mean_absolute_error: 3.6803\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 20.85791\n",
      "Epoch 318/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.0778 - mean_absolute_error: 9.2935 - val_loss: 28.7066 - val_mean_absolute_error: 4.1455\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 20.85791\n",
      "Epoch 319/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.1395 - mean_absolute_error: 9.3457 - val_loss: 23.4257 - val_mean_absolute_error: 3.4799\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 20.85791\n",
      "Epoch 320/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.0818 - mean_absolute_error: 9.1962 - val_loss: 31.4966 - val_mean_absolute_error: 4.0878\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 20.85791\n",
      "Epoch 321/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.3531 - mean_absolute_error: 8.9816 - val_loss: 29.5797 - val_mean_absolute_error: 3.7858\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 20.85791\n",
      "Epoch 322/500\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 124.3666 - mean_absolute_error: 8.7997 - val_loss: 22.9261 - val_mean_absolute_error: 3.3980\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 20.85791\n",
      "Epoch 323/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.9788 - mean_absolute_error: 9.0854 - val_loss: 23.8531 - val_mean_absolute_error: 3.5919\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 20.85791\n",
      "Epoch 324/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.3565 - mean_absolute_error: 9.3892 - val_loss: 26.3917 - val_mean_absolute_error: 3.6709\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 20.85791\n",
      "Epoch 325/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.8929 - mean_absolute_error: 9.1029 - val_loss: 22.2143 - val_mean_absolute_error: 3.4510\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 20.85791\n",
      "Epoch 326/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.7998 - mean_absolute_error: 9.1465 - val_loss: 23.6681 - val_mean_absolute_error: 3.5983\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 20.85791\n",
      "Epoch 327/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.2865 - mean_absolute_error: 9.2119 - val_loss: 29.2442 - val_mean_absolute_error: 3.9785\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 20.85791\n",
      "Epoch 328/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.1854 - mean_absolute_error: 9.3947 - val_loss: 22.7774 - val_mean_absolute_error: 3.3010\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 20.85791\n",
      "Epoch 329/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.7760 - mean_absolute_error: 9.4461 - val_loss: 25.6112 - val_mean_absolute_error: 3.6939\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 20.85791\n",
      "Epoch 330/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.8739 - mean_absolute_error: 8.6481 - val_loss: 24.4711 - val_mean_absolute_error: 3.5511\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 20.85791\n",
      "Epoch 331/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.7884 - mean_absolute_error: 9.2576 - val_loss: 20.5896 - val_mean_absolute_error: 3.3061\n",
      "\n",
      "Epoch 00331: val_loss improved from 20.85791 to 20.58959, saving model to LSTM_Interval_best_RNFL.hdf5\n",
      "Epoch 332/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.9271 - mean_absolute_error: 9.0536 - val_loss: 22.2343 - val_mean_absolute_error: 3.5702\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 20.58959\n",
      "Epoch 333/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.9075 - mean_absolute_error: 9.1199 - val_loss: 23.2307 - val_mean_absolute_error: 3.4894\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 20.58959\n",
      "Epoch 334/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.2339 - mean_absolute_error: 9.4178 - val_loss: 24.1835 - val_mean_absolute_error: 3.4837\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 20.58959\n",
      "Epoch 335/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 121.5771 - mean_absolute_error: 8.8238 - val_loss: 24.9318 - val_mean_absolute_error: 3.5262\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 20.58959\n",
      "Epoch 336/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.3387 - mean_absolute_error: 9.1348 - val_loss: 24.3754 - val_mean_absolute_error: 3.6022\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 20.58959\n",
      "Epoch 337/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.0006 - mean_absolute_error: 9.7498 - val_loss: 24.4712 - val_mean_absolute_error: 3.5458\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 20.58959\n",
      "Epoch 338/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.7666 - mean_absolute_error: 8.7036 - val_loss: 24.9987 - val_mean_absolute_error: 3.6280\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 20.58959\n",
      "Epoch 339/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.9860 - mean_absolute_error: 9.5217 - val_loss: 32.1762 - val_mean_absolute_error: 4.2213\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 20.58959\n",
      "Epoch 340/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.1305 - mean_absolute_error: 9.4841 - val_loss: 24.8414 - val_mean_absolute_error: 3.4245\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 20.58959\n",
      "Epoch 341/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.4111 - mean_absolute_error: 9.2318 - val_loss: 24.1560 - val_mean_absolute_error: 3.5141\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 20.58959\n",
      "Epoch 342/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.0696 - mean_absolute_error: 8.9379 - val_loss: 24.3030 - val_mean_absolute_error: 3.5967\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 20.58959\n",
      "Epoch 343/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.9781 - mean_absolute_error: 9.1904 - val_loss: 31.6451 - val_mean_absolute_error: 4.0743\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 20.58959\n",
      "Epoch 344/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.4202 - mean_absolute_error: 9.1891 - val_loss: 40.8746 - val_mean_absolute_error: 4.7894\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 20.58959\n",
      "Epoch 345/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.3426 - mean_absolute_error: 8.7644 - val_loss: 28.5315 - val_mean_absolute_error: 3.7816\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 20.58959\n",
      "Epoch 346/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.9117 - mean_absolute_error: 8.8189 - val_loss: 31.1509 - val_mean_absolute_error: 4.0728\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 20.58959\n",
      "Epoch 347/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5742 - mean_absolute_error: 9.5165 - val_loss: 25.4911 - val_mean_absolute_error: 3.6590\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 20.58959\n",
      "Epoch 348/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.9678 - mean_absolute_error: 8.7178 - val_loss: 24.5713 - val_mean_absolute_error: 3.5513\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 20.58959\n",
      "Epoch 349/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.4105 - mean_absolute_error: 9.6687 - val_loss: 25.3749 - val_mean_absolute_error: 3.6967\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 20.58959\n",
      "Epoch 350/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 118.2797 - mean_absolute_error: 8.3267 - val_loss: 20.6198 - val_mean_absolute_error: 3.2920\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 20.58959\n",
      "Epoch 351/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.4061 - mean_absolute_error: 9.1292 - val_loss: 31.7835 - val_mean_absolute_error: 4.0277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00351: val_loss did not improve from 20.58959\n",
      "Epoch 352/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.2162 - mean_absolute_error: 9.4354 - val_loss: 28.5429 - val_mean_absolute_error: 3.8647\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 20.58959\n",
      "Epoch 353/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.7680 - mean_absolute_error: 9.1593 - val_loss: 29.5868 - val_mean_absolute_error: 3.9079\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 20.58959\n",
      "Epoch 354/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.3868 - mean_absolute_error: 9.0664 - val_loss: 23.0017 - val_mean_absolute_error: 3.5167\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 20.58959\n",
      "Epoch 355/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.6583 - mean_absolute_error: 9.2327 - val_loss: 27.2508 - val_mean_absolute_error: 3.9089\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 20.58959\n",
      "Epoch 356/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.9872 - mean_absolute_error: 8.9968 - val_loss: 32.8842 - val_mean_absolute_error: 4.2972\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 20.58959\n",
      "Epoch 357/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.1957 - mean_absolute_error: 9.5439 - val_loss: 37.1236 - val_mean_absolute_error: 4.7557\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 20.58959\n",
      "Epoch 358/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.3256 - mean_absolute_error: 8.9566 - val_loss: 30.3071 - val_mean_absolute_error: 4.1730\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 20.58959\n",
      "Epoch 359/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.1309 - mean_absolute_error: 9.2767 - val_loss: 32.1521 - val_mean_absolute_error: 4.3851\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 20.58959\n",
      "Epoch 360/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.2924 - mean_absolute_error: 9.2743 - val_loss: 36.6542 - val_mean_absolute_error: 4.5896\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 20.58959\n",
      "Epoch 361/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.4893 - mean_absolute_error: 9.2357 - val_loss: 29.4978 - val_mean_absolute_error: 4.0165\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 20.58959\n",
      "Epoch 362/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.4348 - mean_absolute_error: 9.1149 - val_loss: 22.8217 - val_mean_absolute_error: 3.4835\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 20.58959\n",
      "Epoch 363/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.4910 - mean_absolute_error: 8.8047 - val_loss: 25.6632 - val_mean_absolute_error: 3.5915\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 20.58959\n",
      "Epoch 364/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.6352 - mean_absolute_error: 8.9357 - val_loss: 31.0213 - val_mean_absolute_error: 4.1449\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 20.58959\n",
      "Epoch 365/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.1238 - mean_absolute_error: 9.3836 - val_loss: 21.9408 - val_mean_absolute_error: 3.3998\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 20.58959\n",
      "Epoch 366/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.0991 - mean_absolute_error: 9.7394 - val_loss: 26.5859 - val_mean_absolute_error: 3.9684\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 20.58959\n",
      "Epoch 367/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.7272 - mean_absolute_error: 9.4214 - val_loss: 22.1337 - val_mean_absolute_error: 3.5584\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 20.58959\n",
      "Epoch 368/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.2144 - mean_absolute_error: 8.5864 - val_loss: 29.0516 - val_mean_absolute_error: 4.1588\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 20.58959\n",
      "Epoch 369/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.1545 - mean_absolute_error: 9.4811 - val_loss: 25.3552 - val_mean_absolute_error: 3.7768\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 20.58959\n",
      "Epoch 370/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.2434 - mean_absolute_error: 9.0540 - val_loss: 22.7563 - val_mean_absolute_error: 3.4095\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 20.58959\n",
      "Epoch 371/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.0641 - mean_absolute_error: 8.9117 - val_loss: 23.4949 - val_mean_absolute_error: 3.4998\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 20.58959\n",
      "Epoch 372/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.1483 - mean_absolute_error: 8.8462 - val_loss: 51.6226 - val_mean_absolute_error: 5.8290\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 20.58959\n",
      "Epoch 373/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.7460 - mean_absolute_error: 8.9504 - val_loss: 26.5494 - val_mean_absolute_error: 3.7241\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 20.58959\n",
      "Epoch 374/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.2843 - mean_absolute_error: 9.1198 - val_loss: 24.0580 - val_mean_absolute_error: 3.6378\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 20.58959\n",
      "Epoch 375/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.9844 - mean_absolute_error: 9.6141 - val_loss: 23.4044 - val_mean_absolute_error: 3.6027\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 20.58959\n",
      "Epoch 376/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 117.1265 - mean_absolute_error: 8.4970 - val_loss: 25.0722 - val_mean_absolute_error: 3.8259\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 20.58959\n",
      "Epoch 377/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.2518 - mean_absolute_error: 9.9507 - val_loss: 26.7693 - val_mean_absolute_error: 3.7739\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 20.58959\n",
      "Epoch 378/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.3035 - mean_absolute_error: 9.1629 - val_loss: 31.3759 - val_mean_absolute_error: 4.1707\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 20.58959\n",
      "Epoch 379/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.5585 - mean_absolute_error: 9.1838 - val_loss: 29.9575 - val_mean_absolute_error: 4.0563\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 20.58959\n",
      "Epoch 380/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 140.6168 - mean_absolute_error: 9.3635 - val_loss: 23.8263 - val_mean_absolute_error: 3.4853\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 20.58959\n",
      "Epoch 381/500\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.7601 - mean_absolute_error: 9.6385 - val_loss: 24.9088 - val_mean_absolute_error: 3.7139\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 20.58959\n",
      "Epoch 00381: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0xb237f2b70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_baseline(X,y_RNFL,\"RNFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_RNFL, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/1000\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 5813.8772 - mean_absolute_error: 75.0465 - val_loss: 5468.1655 - val_mean_absolute_error: 72.6578\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5468.16552, saving model to RNN_best.hdf5\n",
      "Epoch 2/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 5039.4006 - mean_absolute_error: 69.6668 - val_loss: 4599.2280 - val_mean_absolute_error: 66.4152\n",
      "\n",
      "Epoch 00002: val_loss improved from 5468.16552 to 4599.22805, saving model to RNN_best.hdf5\n",
      "Epoch 3/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 4304.7142 - mean_absolute_error: 64.1807 - val_loss: 3925.7089 - val_mean_absolute_error: 61.1230\n",
      "\n",
      "Epoch 00003: val_loss improved from 4599.22805 to 3925.70890, saving model to RNN_best.hdf5\n",
      "Epoch 4/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 3650.6117 - mean_absolute_error: 58.7828 - val_loss: 3246.7211 - val_mean_absolute_error: 55.2911\n",
      "\n",
      "Epoch 00004: val_loss improved from 3925.70890 to 3246.72109, saving model to RNN_best.hdf5\n",
      "Epoch 5/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2943.3012 - mean_absolute_error: 52.3678 - val_loss: 2584.5468 - val_mean_absolute_error: 48.9377\n",
      "\n",
      "Epoch 00005: val_loss improved from 3246.72109 to 2584.54682, saving model to RNN_best.hdf5\n",
      "Epoch 6/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 2320.1783 - mean_absolute_error: 45.8463 - val_loss: 1972.5519 - val_mean_absolute_error: 42.2231\n",
      "\n",
      "Epoch 00006: val_loss improved from 2584.54682 to 1972.55190, saving model to RNN_best.hdf5\n",
      "Epoch 7/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1722.0734 - mean_absolute_error: 38.8335 - val_loss: 1440.7803 - val_mean_absolute_error: 35.3688\n",
      "\n",
      "Epoch 00007: val_loss improved from 1972.55190 to 1440.78033, saving model to RNN_best.hdf5\n",
      "Epoch 8/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 1286.0297 - mean_absolute_error: 32.5023 - val_loss: 1008.4358 - val_mean_absolute_error: 28.6105\n",
      "\n",
      "Epoch 00008: val_loss improved from 1440.78033 to 1008.43576, saving model to RNN_best.hdf5\n",
      "Epoch 9/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 918.8606 - mean_absolute_error: 26.3178 - val_loss: 686.6447 - val_mean_absolute_error: 22.6474\n",
      "\n",
      "Epoch 00009: val_loss improved from 1008.43576 to 686.64467, saving model to RNN_best.hdf5\n",
      "Epoch 10/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 601.6999 - mean_absolute_error: 20.4905 - val_loss: 469.8452 - val_mean_absolute_error: 17.8324\n",
      "\n",
      "Epoch 00010: val_loss improved from 686.64467 to 469.84522, saving model to RNN_best.hdf5\n",
      "Epoch 11/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 502.1321 - mean_absolute_error: 17.9518 - val_loss: 336.3452 - val_mean_absolute_error: 14.5492\n",
      "\n",
      "Epoch 00011: val_loss improved from 469.84522 to 336.34516, saving model to RNN_best.hdf5\n",
      "Epoch 12/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 380.6785 - mean_absolute_error: 15.5969 - val_loss: 259.6461 - val_mean_absolute_error: 12.6850\n",
      "\n",
      "Epoch 00012: val_loss improved from 336.34516 to 259.64615, saving model to RNN_best.hdf5\n",
      "Epoch 13/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 367.4903 - mean_absolute_error: 15.0949 - val_loss: 223.9264 - val_mean_absolute_error: 11.8578\n",
      "\n",
      "Epoch 00013: val_loss improved from 259.64615 to 223.92641, saving model to RNN_best.hdf5\n",
      "Epoch 14/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 324.3787 - mean_absolute_error: 14.5012 - val_loss: 208.0251 - val_mean_absolute_error: 11.5078\n",
      "\n",
      "Epoch 00014: val_loss improved from 223.92641 to 208.02510, saving model to RNN_best.hdf5\n",
      "Epoch 15/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 310.0982 - mean_absolute_error: 14.1557 - val_loss: 200.1028 - val_mean_absolute_error: 11.3732\n",
      "\n",
      "Epoch 00015: val_loss improved from 208.02510 to 200.10280, saving model to RNN_best.hdf5\n",
      "Epoch 16/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 351.8745 - mean_absolute_error: 14.8341 - val_loss: 196.3147 - val_mean_absolute_error: 11.3222\n",
      "\n",
      "Epoch 00016: val_loss improved from 200.10280 to 196.31474, saving model to RNN_best.hdf5\n",
      "Epoch 17/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 286.4932 - mean_absolute_error: 13.3451 - val_loss: 194.8117 - val_mean_absolute_error: 11.3035\n",
      "\n",
      "Epoch 00017: val_loss improved from 196.31474 to 194.81165, saving model to RNN_best.hdf5\n",
      "Epoch 18/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 307.9283 - mean_absolute_error: 14.0148 - val_loss: 193.4595 - val_mean_absolute_error: 11.2839\n",
      "\n",
      "Epoch 00018: val_loss improved from 194.81165 to 193.45952, saving model to RNN_best.hdf5\n",
      "Epoch 19/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 327.7703 - mean_absolute_error: 14.2217 - val_loss: 193.0982 - val_mean_absolute_error: 11.2769\n",
      "\n",
      "Epoch 00019: val_loss improved from 193.45952 to 193.09823, saving model to RNN_best.hdf5\n",
      "Epoch 20/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 290.0568 - mean_absolute_error: 13.8138 - val_loss: 193.2877 - val_mean_absolute_error: 11.2730\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 193.09823\n",
      "Epoch 21/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 320.2151 - mean_absolute_error: 14.2846 - val_loss: 193.3892 - val_mean_absolute_error: 11.2694\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 193.09823\n",
      "Epoch 22/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 296.8433 - mean_absolute_error: 13.7082 - val_loss: 193.8000 - val_mean_absolute_error: 11.2687\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 193.09823\n",
      "Epoch 23/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 312.3912 - mean_absolute_error: 14.0400 - val_loss: 192.8144 - val_mean_absolute_error: 11.2479\n",
      "\n",
      "Epoch 00023: val_loss improved from 193.09823 to 192.81436, saving model to RNN_best.hdf5\n",
      "Epoch 24/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 313.2099 - mean_absolute_error: 14.1081 - val_loss: 192.1715 - val_mean_absolute_error: 11.2321\n",
      "\n",
      "Epoch 00024: val_loss improved from 192.81436 to 192.17152, saving model to RNN_best.hdf5\n",
      "Epoch 25/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 307.9287 - mean_absolute_error: 14.2991 - val_loss: 191.9680 - val_mean_absolute_error: 11.2222\n",
      "\n",
      "Epoch 00025: val_loss improved from 192.17152 to 191.96802, saving model to RNN_best.hdf5\n",
      "Epoch 26/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 313.6685 - mean_absolute_error: 14.1529 - val_loss: 191.8602 - val_mean_absolute_error: 11.2115\n",
      "\n",
      "Epoch 00026: val_loss improved from 191.96802 to 191.86018, saving model to RNN_best.hdf5\n",
      "Epoch 27/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 294.3847 - mean_absolute_error: 13.5733 - val_loss: 190.3309 - val_mean_absolute_error: 11.1822\n",
      "\n",
      "Epoch 00027: val_loss improved from 191.86018 to 190.33089, saving model to RNN_best.hdf5\n",
      "Epoch 28/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 276.2908 - mean_absolute_error: 13.2967 - val_loss: 189.9150 - val_mean_absolute_error: 11.1698\n",
      "\n",
      "Epoch 00028: val_loss improved from 190.33089 to 189.91495, saving model to RNN_best.hdf5\n",
      "Epoch 29/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 308.3554 - mean_absolute_error: 13.8638 - val_loss: 190.6205 - val_mean_absolute_error: 11.1660\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 189.91495\n",
      "Epoch 30/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 302.1504 - mean_absolute_error: 13.8157 - val_loss: 191.4736 - val_mean_absolute_error: 11.1523\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 189.91495\n",
      "Epoch 31/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 310.5330 - mean_absolute_error: 14.2043 - val_loss: 191.2663 - val_mean_absolute_error: 11.1288\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 189.91495\n",
      "Epoch 32/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 305.0765 - mean_absolute_error: 14.0945 - val_loss: 189.0717 - val_mean_absolute_error: 11.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_loss improved from 189.91495 to 189.07174, saving model to RNN_best.hdf5\n",
      "Epoch 33/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 324.1885 - mean_absolute_error: 14.4010 - val_loss: 188.5587 - val_mean_absolute_error: 11.0065\n",
      "\n",
      "Epoch 00033: val_loss improved from 189.07174 to 188.55870, saving model to RNN_best.hdf5\n",
      "Epoch 34/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 300.5899 - mean_absolute_error: 13.5613 - val_loss: 212.3413 - val_mean_absolute_error: 11.6109\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 188.55870\n",
      "Epoch 35/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 311.2172 - mean_absolute_error: 13.7559 - val_loss: 210.6241 - val_mean_absolute_error: 11.5360\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 188.55870\n",
      "Epoch 36/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 301.9850 - mean_absolute_error: 13.7315 - val_loss: 174.8403 - val_mean_absolute_error: 10.3691\n",
      "\n",
      "Epoch 00036: val_loss improved from 188.55870 to 174.84035, saving model to RNN_best.hdf5\n",
      "Epoch 37/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 271.2854 - mean_absolute_error: 12.8593 - val_loss: 142.2378 - val_mean_absolute_error: 9.1823\n",
      "\n",
      "Epoch 00037: val_loss improved from 174.84035 to 142.23784, saving model to RNN_best.hdf5\n",
      "Epoch 38/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 243.6397 - mean_absolute_error: 12.0594 - val_loss: 119.7061 - val_mean_absolute_error: 8.4787\n",
      "\n",
      "Epoch 00038: val_loss improved from 142.23784 to 119.70605, saving model to RNN_best.hdf5\n",
      "Epoch 39/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 247.7593 - mean_absolute_error: 12.0656 - val_loss: 106.5142 - val_mean_absolute_error: 7.6477\n",
      "\n",
      "Epoch 00039: val_loss improved from 119.70605 to 106.51423, saving model to RNN_best.hdf5\n",
      "Epoch 40/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.2704 - mean_absolute_error: 11.2419 - val_loss: 121.6356 - val_mean_absolute_error: 8.4809\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 106.51423\n",
      "Epoch 41/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 219.0438 - mean_absolute_error: 11.4831 - val_loss: 130.1562 - val_mean_absolute_error: 8.9016\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 106.51423\n",
      "Epoch 42/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 271.1145 - mean_absolute_error: 13.1689 - val_loss: 116.1813 - val_mean_absolute_error: 8.2637\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 106.51423\n",
      "Epoch 43/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 229.7826 - mean_absolute_error: 11.7298 - val_loss: 104.8637 - val_mean_absolute_error: 7.7183\n",
      "\n",
      "Epoch 00043: val_loss improved from 106.51423 to 104.86374, saving model to RNN_best.hdf5\n",
      "Epoch 44/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 229.6157 - mean_absolute_error: 12.2423 - val_loss: 102.3087 - val_mean_absolute_error: 7.5300\n",
      "\n",
      "Epoch 00044: val_loss improved from 104.86374 to 102.30872, saving model to RNN_best.hdf5\n",
      "Epoch 45/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 220.7831 - mean_absolute_error: 11.6047 - val_loss: 142.4763 - val_mean_absolute_error: 9.4616\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 102.30872\n",
      "Epoch 46/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 224.2976 - mean_absolute_error: 11.5674 - val_loss: 93.9320 - val_mean_absolute_error: 7.1481\n",
      "\n",
      "Epoch 00046: val_loss improved from 102.30872 to 93.93203, saving model to RNN_best.hdf5\n",
      "Epoch 47/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 194.4470 - mean_absolute_error: 10.6192 - val_loss: 88.6164 - val_mean_absolute_error: 6.8208\n",
      "\n",
      "Epoch 00047: val_loss improved from 93.93203 to 88.61645, saving model to RNN_best.hdf5\n",
      "Epoch 48/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.9628 - mean_absolute_error: 10.6336 - val_loss: 88.7277 - val_mean_absolute_error: 6.9726\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 88.61645\n",
      "Epoch 49/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.2334 - mean_absolute_error: 10.5790 - val_loss: 96.7293 - val_mean_absolute_error: 7.4931\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 88.61645\n",
      "Epoch 50/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 211.9620 - mean_absolute_error: 11.4634 - val_loss: 88.3071 - val_mean_absolute_error: 6.8816\n",
      "\n",
      "Epoch 00050: val_loss improved from 88.61645 to 88.30709, saving model to RNN_best.hdf5\n",
      "Epoch 51/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 204.0511 - mean_absolute_error: 11.1270 - val_loss: 138.3501 - val_mean_absolute_error: 9.2291\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 88.30709\n",
      "Epoch 52/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 225.8444 - mean_absolute_error: 11.8821 - val_loss: 91.9927 - val_mean_absolute_error: 7.0762\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 88.30709\n",
      "Epoch 53/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 195.5012 - mean_absolute_error: 10.8453 - val_loss: 67.2398 - val_mean_absolute_error: 5.9518\n",
      "\n",
      "Epoch 00053: val_loss improved from 88.30709 to 67.23981, saving model to RNN_best.hdf5\n",
      "Epoch 54/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 196.4988 - mean_absolute_error: 11.0508 - val_loss: 76.2786 - val_mean_absolute_error: 6.4462\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 67.23981\n",
      "Epoch 55/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 191.3539 - mean_absolute_error: 10.7234 - val_loss: 69.8309 - val_mean_absolute_error: 6.1298\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 67.23981\n",
      "Epoch 56/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.7402 - mean_absolute_error: 10.5841 - val_loss: 63.2060 - val_mean_absolute_error: 5.7409\n",
      "\n",
      "Epoch 00056: val_loss improved from 67.23981 to 63.20600, saving model to RNN_best.hdf5\n",
      "Epoch 57/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.7581 - mean_absolute_error: 10.3878 - val_loss: 60.6415 - val_mean_absolute_error: 5.5653\n",
      "\n",
      "Epoch 00057: val_loss improved from 63.20600 to 60.64151, saving model to RNN_best.hdf5\n",
      "Epoch 58/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.3357 - mean_absolute_error: 9.9222 - val_loss: 64.1671 - val_mean_absolute_error: 5.7897\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 60.64151\n",
      "Epoch 59/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.0643 - mean_absolute_error: 10.4956 - val_loss: 57.8696 - val_mean_absolute_error: 5.4309\n",
      "\n",
      "Epoch 00059: val_loss improved from 60.64151 to 57.86956, saving model to RNN_best.hdf5\n",
      "Epoch 60/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.7756 - mean_absolute_error: 11.1225 - val_loss: 53.4158 - val_mean_absolute_error: 5.1996\n",
      "\n",
      "Epoch 00060: val_loss improved from 57.86956 to 53.41582, saving model to RNN_best.hdf5\n",
      "Epoch 61/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.8704 - mean_absolute_error: 10.7949 - val_loss: 71.9927 - val_mean_absolute_error: 6.2933\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 53.41582\n",
      "Epoch 62/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.3370 - mean_absolute_error: 10.0184 - val_loss: 53.8348 - val_mean_absolute_error: 5.2937\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 53.41582\n",
      "Epoch 63/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.1038 - mean_absolute_error: 10.6333 - val_loss: 78.2207 - val_mean_absolute_error: 6.8352\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 53.41582\n",
      "Epoch 64/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.8173 - mean_absolute_error: 10.4688 - val_loss: 57.9614 - val_mean_absolute_error: 5.5305\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 53.41582\n",
      "Epoch 65/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.9614 - mean_absolute_error: 9.7985 - val_loss: 45.0046 - val_mean_absolute_error: 4.6226\n",
      "\n",
      "Epoch 00065: val_loss improved from 53.41582 to 45.00458, saving model to RNN_best.hdf5\n",
      "Epoch 66/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.4921 - mean_absolute_error: 10.0124 - val_loss: 45.9993 - val_mean_absolute_error: 4.7176\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 45.00458\n",
      "Epoch 67/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.6994 - mean_absolute_error: 9.5475 - val_loss: 51.3201 - val_mean_absolute_error: 5.1762\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 45.00458\n",
      "Epoch 68/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.5526 - mean_absolute_error: 9.9979 - val_loss: 47.8322 - val_mean_absolute_error: 4.7923\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 45.00458\n",
      "Epoch 69/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.9423 - mean_absolute_error: 10.2416 - val_loss: 62.8236 - val_mean_absolute_error: 6.0812\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 45.00458\n",
      "Epoch 70/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.6803 - mean_absolute_error: 9.9728 - val_loss: 55.3939 - val_mean_absolute_error: 5.3184\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 45.00458\n",
      "Epoch 71/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 180.6811 - mean_absolute_error: 10.3908 - val_loss: 55.1451 - val_mean_absolute_error: 5.3036\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 45.00458\n",
      "Epoch 72/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.7627 - mean_absolute_error: 9.4573 - val_loss: 55.1212 - val_mean_absolute_error: 5.3253\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 45.00458\n",
      "Epoch 73/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.2223 - mean_absolute_error: 10.1024 - val_loss: 43.5749 - val_mean_absolute_error: 4.5339\n",
      "\n",
      "Epoch 00073: val_loss improved from 45.00458 to 43.57489, saving model to RNN_best.hdf5\n",
      "Epoch 74/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.5524 - mean_absolute_error: 10.3512 - val_loss: 40.3836 - val_mean_absolute_error: 4.3744\n",
      "\n",
      "Epoch 00074: val_loss improved from 43.57489 to 40.38359, saving model to RNN_best.hdf5\n",
      "Epoch 75/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 176.8429 - mean_absolute_error: 10.3229 - val_loss: 40.1610 - val_mean_absolute_error: 4.4581\n",
      "\n",
      "Epoch 00075: val_loss improved from 40.38359 to 40.16099, saving model to RNN_best.hdf5\n",
      "Epoch 76/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.5994 - mean_absolute_error: 9.9465 - val_loss: 38.3618 - val_mean_absolute_error: 4.3059\n",
      "\n",
      "Epoch 00076: val_loss improved from 40.16099 to 38.36177, saving model to RNN_best.hdf5\n",
      "Epoch 77/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 180.2607 - mean_absolute_error: 10.3467 - val_loss: 41.0329 - val_mean_absolute_error: 4.6662\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 38.36177\n",
      "Epoch 78/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.1457 - mean_absolute_error: 9.3804 - val_loss: 44.0791 - val_mean_absolute_error: 4.9433\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 38.36177\n",
      "Epoch 79/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.7773 - mean_absolute_error: 9.8055 - val_loss: 41.8623 - val_mean_absolute_error: 4.6288\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 38.36177\n",
      "Epoch 80/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.2516 - mean_absolute_error: 10.5839 - val_loss: 47.0614 - val_mean_absolute_error: 5.0489\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 38.36177\n",
      "Epoch 81/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 164.1051 - mean_absolute_error: 10.2374 - val_loss: 49.6303 - val_mean_absolute_error: 5.1682\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 38.36177\n",
      "Epoch 82/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.5074 - mean_absolute_error: 10.0955 - val_loss: 46.6239 - val_mean_absolute_error: 5.0148\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 38.36177\n",
      "Epoch 83/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.5273 - mean_absolute_error: 9.7326 - val_loss: 45.2943 - val_mean_absolute_error: 4.9557\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 38.36177\n",
      "Epoch 84/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 163.6866 - mean_absolute_error: 10.0643 - val_loss: 42.8277 - val_mean_absolute_error: 4.6154\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 38.36177\n",
      "Epoch 85/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.6228 - mean_absolute_error: 10.1390 - val_loss: 37.0506 - val_mean_absolute_error: 4.3584\n",
      "\n",
      "Epoch 00085: val_loss improved from 38.36177 to 37.05062, saving model to RNN_best.hdf5\n",
      "Epoch 86/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.5915 - mean_absolute_error: 9.2612 - val_loss: 33.4370 - val_mean_absolute_error: 4.0899\n",
      "\n",
      "Epoch 00086: val_loss improved from 37.05062 to 33.43697, saving model to RNN_best.hdf5\n",
      "Epoch 87/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.6515 - mean_absolute_error: 10.1087 - val_loss: 34.1341 - val_mean_absolute_error: 4.2311\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 33.43697\n",
      "Epoch 88/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.6810 - mean_absolute_error: 10.3252 - val_loss: 36.6944 - val_mean_absolute_error: 4.3789\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 33.43697\n",
      "Epoch 89/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.0766 - mean_absolute_error: 10.5700 - val_loss: 39.7462 - val_mean_absolute_error: 4.3128\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 33.43697\n",
      "Epoch 90/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.4027 - mean_absolute_error: 9.7837 - val_loss: 35.3785 - val_mean_absolute_error: 4.1877\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 33.43697\n",
      "Epoch 91/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.6591 - mean_absolute_error: 10.2582 - val_loss: 34.0489 - val_mean_absolute_error: 4.0710\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 33.43697\n",
      "Epoch 92/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 171.4118 - mean_absolute_error: 10.1980 - val_loss: 34.6143 - val_mean_absolute_error: 4.1610\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 33.43697\n",
      "Epoch 93/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.4172 - mean_absolute_error: 9.3341 - val_loss: 31.8200 - val_mean_absolute_error: 3.9756\n",
      "\n",
      "Epoch 00093: val_loss improved from 33.43697 to 31.81996, saving model to RNN_best.hdf5\n",
      "Epoch 94/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 163.4051 - mean_absolute_error: 10.0950 - val_loss: 29.9785 - val_mean_absolute_error: 3.7873\n",
      "\n",
      "Epoch 00094: val_loss improved from 31.81996 to 29.97851, saving model to RNN_best.hdf5\n",
      "Epoch 95/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.5485 - mean_absolute_error: 9.7593 - val_loss: 34.2516 - val_mean_absolute_error: 4.1531\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 29.97851\n",
      "Epoch 96/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.9896 - mean_absolute_error: 9.1977 - val_loss: 38.5175 - val_mean_absolute_error: 4.5750\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 29.97851\n",
      "Epoch 97/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.9032 - mean_absolute_error: 9.4864 - val_loss: 30.5719 - val_mean_absolute_error: 3.9108\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 29.97851\n",
      "Epoch 98/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 176.2330 - mean_absolute_error: 10.3405 - val_loss: 30.9322 - val_mean_absolute_error: 3.8449\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 29.97851\n",
      "Epoch 99/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.6565 - mean_absolute_error: 9.9683 - val_loss: 30.6763 - val_mean_absolute_error: 3.9231\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 29.97851\n",
      "Epoch 100/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.7933 - mean_absolute_error: 9.9989 - val_loss: 32.9812 - val_mean_absolute_error: 4.0854\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 29.97851\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5212 - mean_absolute_error: 9.4713 - val_loss: 40.9333 - val_mean_absolute_error: 4.8300\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 29.97851\n",
      "Epoch 102/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.7779 - mean_absolute_error: 9.1268 - val_loss: 32.4588 - val_mean_absolute_error: 4.0474\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 29.97851\n",
      "Epoch 103/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.2907 - mean_absolute_error: 9.1944 - val_loss: 31.0096 - val_mean_absolute_error: 3.8377\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 29.97851\n",
      "Epoch 104/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.4215 - mean_absolute_error: 9.6282 - val_loss: 31.4352 - val_mean_absolute_error: 3.8785\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 29.97851\n",
      "Epoch 105/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.1180 - mean_absolute_error: 9.7984 - val_loss: 60.5570 - val_mean_absolute_error: 6.3285\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 29.97851\n",
      "Epoch 106/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.6124 - mean_absolute_error: 9.5541 - val_loss: 39.3935 - val_mean_absolute_error: 4.5119\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 29.97851\n",
      "Epoch 107/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.9939 - mean_absolute_error: 9.5875 - val_loss: 40.1020 - val_mean_absolute_error: 4.5842\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 29.97851\n",
      "Epoch 108/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.4783 - mean_absolute_error: 9.9825 - val_loss: 42.5514 - val_mean_absolute_error: 4.8982\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 29.97851\n",
      "Epoch 109/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.7979 - mean_absolute_error: 9.2238 - val_loss: 32.5043 - val_mean_absolute_error: 4.0943\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 29.97851\n",
      "Epoch 110/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.7902 - mean_absolute_error: 10.0427 - val_loss: 29.7318 - val_mean_absolute_error: 3.9610\n",
      "\n",
      "Epoch 00110: val_loss improved from 29.97851 to 29.73179, saving model to RNN_best.hdf5\n",
      "Epoch 111/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 180.1018 - mean_absolute_error: 10.6049 - val_loss: 33.5178 - val_mean_absolute_error: 4.3192\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 29.73179\n",
      "Epoch 112/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.6950 - mean_absolute_error: 9.4532 - val_loss: 31.5536 - val_mean_absolute_error: 3.9513\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 29.73179\n",
      "Epoch 113/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.5621 - mean_absolute_error: 9.9834 - val_loss: 27.5653 - val_mean_absolute_error: 3.7493\n",
      "\n",
      "Epoch 00113: val_loss improved from 29.73179 to 27.56534, saving model to RNN_best.hdf5\n",
      "Epoch 114/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.5285 - mean_absolute_error: 8.7194 - val_loss: 31.0696 - val_mean_absolute_error: 4.0828\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 27.56534\n",
      "Epoch 115/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.6854 - mean_absolute_error: 9.7032 - val_loss: 40.4058 - val_mean_absolute_error: 4.7820\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 27.56534\n",
      "Epoch 116/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.7682 - mean_absolute_error: 8.8596 - val_loss: 40.4105 - val_mean_absolute_error: 4.8754\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 27.56534\n",
      "Epoch 117/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 147.0141 - mean_absolute_error: 9.4475 - val_loss: 32.2899 - val_mean_absolute_error: 4.3327\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 27.56534\n",
      "Epoch 118/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 166.3924 - mean_absolute_error: 10.1701 - val_loss: 36.8438 - val_mean_absolute_error: 4.4655\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 27.56534\n",
      "Epoch 119/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.3764 - mean_absolute_error: 10.0544 - val_loss: 30.3118 - val_mean_absolute_error: 4.0120\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 27.56534\n",
      "Epoch 120/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.5830 - mean_absolute_error: 9.7404 - val_loss: 31.6412 - val_mean_absolute_error: 4.0727\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 27.56534\n",
      "Epoch 121/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.7845 - mean_absolute_error: 9.1033 - val_loss: 37.4960 - val_mean_absolute_error: 4.7328\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 27.56534\n",
      "Epoch 122/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.2734 - mean_absolute_error: 9.1627 - val_loss: 38.7986 - val_mean_absolute_error: 4.8713\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 27.56534\n",
      "Epoch 123/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.5397 - mean_absolute_error: 9.0557 - val_loss: 33.1334 - val_mean_absolute_error: 4.2687\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 27.56534\n",
      "Epoch 124/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.0473 - mean_absolute_error: 8.9186 - val_loss: 29.0229 - val_mean_absolute_error: 3.8126\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 27.56534\n",
      "Epoch 125/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.4468 - mean_absolute_error: 8.7699 - val_loss: 28.5698 - val_mean_absolute_error: 3.7746\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 27.56534\n",
      "Epoch 126/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.7857 - mean_absolute_error: 9.0178 - val_loss: 28.1148 - val_mean_absolute_error: 3.7615\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 27.56534\n",
      "Epoch 127/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.5932 - mean_absolute_error: 9.2052 - val_loss: 34.3290 - val_mean_absolute_error: 4.4384\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 27.56534\n",
      "Epoch 128/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.1573 - mean_absolute_error: 9.1323 - val_loss: 28.1943 - val_mean_absolute_error: 3.8950\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 27.56534\n",
      "Epoch 129/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.8197 - mean_absolute_error: 9.2626 - val_loss: 30.3630 - val_mean_absolute_error: 4.0780\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 27.56534\n",
      "Epoch 130/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.6877 - mean_absolute_error: 9.2876 - val_loss: 34.2869 - val_mean_absolute_error: 4.4948\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 27.56534\n",
      "Epoch 131/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.6208 - mean_absolute_error: 10.3531 - val_loss: 34.3547 - val_mean_absolute_error: 4.2860\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 27.56534\n",
      "Epoch 132/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.3696 - mean_absolute_error: 9.7063 - val_loss: 40.2768 - val_mean_absolute_error: 4.8195\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 27.56534\n",
      "Epoch 133/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.0013 - mean_absolute_error: 8.9798 - val_loss: 46.2510 - val_mean_absolute_error: 5.2899\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 27.56534\n",
      "Epoch 134/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.7898 - mean_absolute_error: 8.9062 - val_loss: 29.9193 - val_mean_absolute_error: 3.9962\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 27.56534\n",
      "Epoch 135/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.7713 - mean_absolute_error: 10.1051 - val_loss: 26.0896 - val_mean_absolute_error: 3.6166\n",
      "\n",
      "Epoch 00135: val_loss improved from 27.56534 to 26.08963, saving model to RNN_best.hdf5\n",
      "Epoch 136/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.4865 - mean_absolute_error: 9.5164 - val_loss: 29.8676 - val_mean_absolute_error: 4.0158\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 26.08963\n",
      "Epoch 137/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.7779 - mean_absolute_error: 9.9637 - val_loss: 34.1898 - val_mean_absolute_error: 4.4036\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 26.08963\n",
      "Epoch 138/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.4089 - mean_absolute_error: 9.0270 - val_loss: 28.8048 - val_mean_absolute_error: 3.8067\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 26.08963\n",
      "Epoch 139/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.0370 - mean_absolute_error: 9.4701 - val_loss: 28.2085 - val_mean_absolute_error: 3.7647\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 26.08963\n",
      "Epoch 140/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.4646 - mean_absolute_error: 9.3210 - val_loss: 27.7770 - val_mean_absolute_error: 3.7684\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 26.08963\n",
      "Epoch 141/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.8809 - mean_absolute_error: 9.0334 - val_loss: 27.2258 - val_mean_absolute_error: 3.6753\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 26.08963\n",
      "Epoch 142/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.7215 - mean_absolute_error: 9.3988 - val_loss: 28.2814 - val_mean_absolute_error: 3.7807\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 26.08963\n",
      "Epoch 143/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.6528 - mean_absolute_error: 9.5035 - val_loss: 32.5020 - val_mean_absolute_error: 4.1814\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 26.08963\n",
      "Epoch 144/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.2212 - mean_absolute_error: 9.0085 - val_loss: 27.2390 - val_mean_absolute_error: 3.7221\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 26.08963\n",
      "Epoch 145/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.5270 - mean_absolute_error: 8.8456 - val_loss: 30.8370 - val_mean_absolute_error: 4.0727\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 26.08963\n",
      "Epoch 146/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 126.3633 - mean_absolute_error: 8.8644 - val_loss: 29.2752 - val_mean_absolute_error: 3.7757\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 26.08963\n",
      "Epoch 147/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.4099 - mean_absolute_error: 9.1624 - val_loss: 28.7341 - val_mean_absolute_error: 3.9150\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 26.08963\n",
      "Epoch 148/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.3260 - mean_absolute_error: 9.2133 - val_loss: 26.0864 - val_mean_absolute_error: 3.5897\n",
      "\n",
      "Epoch 00148: val_loss improved from 26.08963 to 26.08636, saving model to RNN_best.hdf5\n",
      "Epoch 149/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.1506 - mean_absolute_error: 9.6920 - val_loss: 38.5886 - val_mean_absolute_error: 4.6102\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 26.08636\n",
      "Epoch 150/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.2219 - mean_absolute_error: 9.5160 - val_loss: 49.0556 - val_mean_absolute_error: 5.4976\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 26.08636\n",
      "Epoch 151/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.9101 - mean_absolute_error: 9.0552 - val_loss: 51.0425 - val_mean_absolute_error: 5.7611\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 26.08636\n",
      "Epoch 152/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.9601 - mean_absolute_error: 9.0990 - val_loss: 36.8777 - val_mean_absolute_error: 4.4638\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 26.08636\n",
      "Epoch 153/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.0660 - mean_absolute_error: 9.1548 - val_loss: 26.1708 - val_mean_absolute_error: 3.5854\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 26.08636\n",
      "Epoch 154/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.5348 - mean_absolute_error: 9.3312 - val_loss: 26.8939 - val_mean_absolute_error: 3.6749\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 26.08636\n",
      "Epoch 155/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.6533 - mean_absolute_error: 9.1420 - val_loss: 25.9006 - val_mean_absolute_error: 3.6339\n",
      "\n",
      "Epoch 00155: val_loss improved from 26.08636 to 25.90062, saving model to RNN_best.hdf5\n",
      "Epoch 156/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.5136 - mean_absolute_error: 8.9532 - val_loss: 35.3423 - val_mean_absolute_error: 4.4376\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 25.90062\n",
      "Epoch 157/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.3451 - mean_absolute_error: 9.0346 - val_loss: 45.7168 - val_mean_absolute_error: 5.2337\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 25.90062\n",
      "Epoch 158/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.2357 - mean_absolute_error: 9.9377 - val_loss: 34.4664 - val_mean_absolute_error: 4.2750\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 25.90062\n",
      "Epoch 159/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.2288 - mean_absolute_error: 9.1607 - val_loss: 29.8555 - val_mean_absolute_error: 3.8860\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 25.90062\n",
      "Epoch 160/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.6096 - mean_absolute_error: 8.9280 - val_loss: 28.1028 - val_mean_absolute_error: 3.7606\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 25.90062\n",
      "Epoch 161/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.6255 - mean_absolute_error: 9.3693 - val_loss: 29.6760 - val_mean_absolute_error: 4.0061\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 25.90062\n",
      "Epoch 162/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.0315 - mean_absolute_error: 9.0929 - val_loss: 28.2285 - val_mean_absolute_error: 3.7780\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 25.90062\n",
      "Epoch 163/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.2372 - mean_absolute_error: 9.8343 - val_loss: 42.6563 - val_mean_absolute_error: 5.0147\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 25.90062\n",
      "Epoch 164/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 140.3529 - mean_absolute_error: 9.2149 - val_loss: 43.9592 - val_mean_absolute_error: 5.1871\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 25.90062\n",
      "Epoch 165/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.0451 - mean_absolute_error: 9.3311 - val_loss: 44.1161 - val_mean_absolute_error: 5.2233\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 25.90062\n",
      "Epoch 166/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.5964 - mean_absolute_error: 9.5302 - val_loss: 33.0486 - val_mean_absolute_error: 4.3295\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 25.90062\n",
      "Epoch 167/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.7674 - mean_absolute_error: 9.4290 - val_loss: 26.1686 - val_mean_absolute_error: 3.6697\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 25.90062\n",
      "Epoch 168/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.0156 - mean_absolute_error: 9.6280 - val_loss: 31.0079 - val_mean_absolute_error: 3.9288\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 25.90062\n",
      "Epoch 169/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 135.6523 - mean_absolute_error: 9.3096 - val_loss: 29.5800 - val_mean_absolute_error: 3.9158\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 25.90062\n",
      "Epoch 170/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.4866 - mean_absolute_error: 8.9073 - val_loss: 33.6980 - val_mean_absolute_error: 4.1860\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 25.90062\n",
      "Epoch 171/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 134.5155 - mean_absolute_error: 8.9387 - val_loss: 33.8644 - val_mean_absolute_error: 4.3385\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 25.90062\n",
      "Epoch 172/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.8745 - mean_absolute_error: 9.4532 - val_loss: 33.8346 - val_mean_absolute_error: 4.3267\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 25.90062\n",
      "Epoch 173/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 143.5175 - mean_absolute_error: 9.3071 - val_loss: 26.2237 - val_mean_absolute_error: 3.6440\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 25.90062\n",
      "Epoch 174/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 127.1983 - mean_absolute_error: 8.8627 - val_loss: 26.8642 - val_mean_absolute_error: 3.8140\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 25.90062\n",
      "Epoch 175/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 123.7518 - mean_absolute_error: 8.6741 - val_loss: 24.2504 - val_mean_absolute_error: 3.5964\n",
      "\n",
      "Epoch 00175: val_loss improved from 25.90062 to 24.25038, saving model to RNN_best.hdf5\n",
      "Epoch 176/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.4335 - mean_absolute_error: 9.1138 - val_loss: 24.2132 - val_mean_absolute_error: 3.5164\n",
      "\n",
      "Epoch 00176: val_loss improved from 24.25038 to 24.21320, saving model to RNN_best.hdf5\n",
      "Epoch 177/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.8013 - mean_absolute_error: 9.0900 - val_loss: 26.5752 - val_mean_absolute_error: 3.7079\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 24.21320\n",
      "Epoch 178/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.2794 - mean_absolute_error: 9.0898 - val_loss: 26.1753 - val_mean_absolute_error: 3.7153\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 24.21320\n",
      "Epoch 179/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.4051 - mean_absolute_error: 9.1395 - val_loss: 39.3536 - val_mean_absolute_error: 4.6914\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 24.21320\n",
      "Epoch 180/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.5295 - mean_absolute_error: 9.3703 - val_loss: 38.7409 - val_mean_absolute_error: 4.5220\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 24.21320\n",
      "Epoch 181/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.1936 - mean_absolute_error: 9.2979 - val_loss: 34.0261 - val_mean_absolute_error: 4.2140\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 24.21320\n",
      "Epoch 182/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 119.4818 - mean_absolute_error: 8.5366 - val_loss: 27.1252 - val_mean_absolute_error: 3.8053\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 24.21320\n",
      "Epoch 183/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.9145 - mean_absolute_error: 9.1894 - val_loss: 27.5511 - val_mean_absolute_error: 3.7595\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 24.21320\n",
      "Epoch 184/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.5143 - mean_absolute_error: 9.9588 - val_loss: 25.7716 - val_mean_absolute_error: 3.6090\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 24.21320\n",
      "Epoch 185/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 130.6726 - mean_absolute_error: 9.0246 - val_loss: 27.5951 - val_mean_absolute_error: 3.7825\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 24.21320\n",
      "Epoch 186/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 126.6902 - mean_absolute_error: 8.7026 - val_loss: 31.7598 - val_mean_absolute_error: 4.0821\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 24.21320\n",
      "Epoch 187/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.7219 - mean_absolute_error: 9.6786 - val_loss: 28.2495 - val_mean_absolute_error: 3.7399\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 24.21320\n",
      "Epoch 188/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 134.4246 - mean_absolute_error: 9.1857 - val_loss: 27.7107 - val_mean_absolute_error: 3.8701\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 24.21320\n",
      "Epoch 189/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 132.1762 - mean_absolute_error: 9.2161 - val_loss: 43.4471 - val_mean_absolute_error: 5.1791\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 24.21320\n",
      "Epoch 190/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 132.4777 - mean_absolute_error: 9.0524 - val_loss: 28.0754 - val_mean_absolute_error: 3.8523\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 24.21320\n",
      "Epoch 191/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.9113 - mean_absolute_error: 9.1502 - val_loss: 25.2758 - val_mean_absolute_error: 3.6542\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 24.21320\n",
      "Epoch 192/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 128.6150 - mean_absolute_error: 8.9544 - val_loss: 32.4828 - val_mean_absolute_error: 4.3267\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 24.21320\n",
      "Epoch 193/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 148.0071 - mean_absolute_error: 9.3723 - val_loss: 29.2719 - val_mean_absolute_error: 3.9064\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 24.21320\n",
      "Epoch 194/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 127.8956 - mean_absolute_error: 8.7701 - val_loss: 49.8804 - val_mean_absolute_error: 5.5461\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 24.21320\n",
      "Epoch 195/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 138.6329 - mean_absolute_error: 9.1350 - val_loss: 31.0583 - val_mean_absolute_error: 3.9313\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 24.21320\n",
      "Epoch 196/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.4610 - mean_absolute_error: 9.4008 - val_loss: 25.9234 - val_mean_absolute_error: 3.5565\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 24.21320\n",
      "Epoch 197/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 129.6423 - mean_absolute_error: 9.0686 - val_loss: 29.1583 - val_mean_absolute_error: 3.8182\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 24.21320\n",
      "Epoch 198/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 120.0834 - mean_absolute_error: 8.7587 - val_loss: 27.1427 - val_mean_absolute_error: 3.7914\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 24.21320\n",
      "Epoch 199/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.1193 - mean_absolute_error: 9.3347 - val_loss: 27.5412 - val_mean_absolute_error: 3.6578\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 24.21320\n",
      "Epoch 200/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.8052 - mean_absolute_error: 9.4999 - val_loss: 30.3146 - val_mean_absolute_error: 4.0019\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 24.21320\n",
      "Epoch 201/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.0295 - mean_absolute_error: 9.3580 - val_loss: 26.8612 - val_mean_absolute_error: 3.7438\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 24.21320\n",
      "Epoch 202/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.8817 - mean_absolute_error: 9.2471 - val_loss: 27.8437 - val_mean_absolute_error: 3.7399\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 24.21320\n",
      "Epoch 203/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.5364 - mean_absolute_error: 9.5376 - val_loss: 27.9186 - val_mean_absolute_error: 3.8188\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 24.21320\n",
      "Epoch 204/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 127.0971 - mean_absolute_error: 8.8270 - val_loss: 25.4965 - val_mean_absolute_error: 3.5312\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 24.21320\n",
      "Epoch 205/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.2234 - mean_absolute_error: 9.5003 - val_loss: 28.2553 - val_mean_absolute_error: 3.7776\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 24.21320\n",
      "Epoch 206/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 137.0858 - mean_absolute_error: 9.0134 - val_loss: 27.0707 - val_mean_absolute_error: 3.6783\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 24.21320\n",
      "Epoch 207/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 137.1100 - mean_absolute_error: 9.3041 - val_loss: 26.1171 - val_mean_absolute_error: 3.6947\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 24.21320\n",
      "Epoch 208/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 132.9984 - mean_absolute_error: 8.9890 - val_loss: 25.8696 - val_mean_absolute_error: 3.6342\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 24.21320\n",
      "Epoch 209/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.4063 - mean_absolute_error: 9.1514 - val_loss: 25.2434 - val_mean_absolute_error: 3.6090\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 24.21320\n",
      "Epoch 210/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 123.3566 - mean_absolute_error: 8.9365 - val_loss: 27.4522 - val_mean_absolute_error: 3.8038\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 24.21320\n",
      "Epoch 211/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.6258 - mean_absolute_error: 8.9143 - val_loss: 29.0099 - val_mean_absolute_error: 3.9226\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 24.21320\n",
      "Epoch 212/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.1879 - mean_absolute_error: 9.0049 - val_loss: 39.1447 - val_mean_absolute_error: 4.7570\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 24.21320\n",
      "Epoch 213/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.8211 - mean_absolute_error: 9.3230 - val_loss: 27.8676 - val_mean_absolute_error: 3.8147\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 24.21320\n",
      "Epoch 214/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 134.6483 - mean_absolute_error: 9.2075 - val_loss: 33.1982 - val_mean_absolute_error: 4.3112\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 24.21320\n",
      "Epoch 215/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 157.7952 - mean_absolute_error: 9.7762 - val_loss: 33.6515 - val_mean_absolute_error: 4.3682\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 24.21320\n",
      "Epoch 216/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 154.3133 - mean_absolute_error: 9.8667 - val_loss: 39.3623 - val_mean_absolute_error: 4.7767\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 24.21320\n",
      "Epoch 217/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 124.1649 - mean_absolute_error: 8.7197 - val_loss: 26.2865 - val_mean_absolute_error: 3.5751\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 24.21320\n",
      "Epoch 218/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.5698 - mean_absolute_error: 9.0924 - val_loss: 29.3165 - val_mean_absolute_error: 4.0693\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 24.21320\n",
      "Epoch 219/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 121.8627 - mean_absolute_error: 8.5233 - val_loss: 22.5922 - val_mean_absolute_error: 3.3905\n",
      "\n",
      "Epoch 00219: val_loss improved from 24.21320 to 22.59217, saving model to RNN_best.hdf5\n",
      "Epoch 220/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 127.4439 - mean_absolute_error: 8.8106 - val_loss: 23.9440 - val_mean_absolute_error: 3.5621\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 22.59217\n",
      "Epoch 221/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.6962 - mean_absolute_error: 9.2900 - val_loss: 34.2432 - val_mean_absolute_error: 4.3615\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 22.59217\n",
      "Epoch 222/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 137.8127 - mean_absolute_error: 9.3800 - val_loss: 25.9678 - val_mean_absolute_error: 3.5903\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 22.59217\n",
      "Epoch 223/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 137.3217 - mean_absolute_error: 9.3645 - val_loss: 27.2584 - val_mean_absolute_error: 3.7944\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 22.59217\n",
      "Epoch 224/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.4035 - mean_absolute_error: 9.3826 - val_loss: 26.5340 - val_mean_absolute_error: 3.7627\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 22.59217\n",
      "Epoch 225/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.9517 - mean_absolute_error: 9.3536 - val_loss: 31.6030 - val_mean_absolute_error: 4.3261\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 22.59217\n",
      "Epoch 226/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.7000 - mean_absolute_error: 9.5403 - val_loss: 34.0271 - val_mean_absolute_error: 4.3733\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 22.59217\n",
      "Epoch 227/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.7257 - mean_absolute_error: 9.4433 - val_loss: 37.7345 - val_mean_absolute_error: 4.6401\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 22.59217\n",
      "Epoch 228/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.5545 - mean_absolute_error: 9.4538 - val_loss: 30.0729 - val_mean_absolute_error: 4.0342\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 22.59217\n",
      "Epoch 229/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 129.4166 - mean_absolute_error: 8.8879 - val_loss: 29.2052 - val_mean_absolute_error: 3.9783\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 22.59217\n",
      "Epoch 230/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 128.4407 - mean_absolute_error: 8.7981 - val_loss: 29.7759 - val_mean_absolute_error: 3.9488\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 22.59217\n",
      "Epoch 231/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.9743 - mean_absolute_error: 8.8582 - val_loss: 26.3001 - val_mean_absolute_error: 3.6895\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 22.59217\n",
      "Epoch 232/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 140.4481 - mean_absolute_error: 9.2330 - val_loss: 31.5204 - val_mean_absolute_error: 4.1231\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 22.59217\n",
      "Epoch 233/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 128.8376 - mean_absolute_error: 9.0066 - val_loss: 29.0988 - val_mean_absolute_error: 3.8354\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 22.59217\n",
      "Epoch 234/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 112.4291 - mean_absolute_error: 8.4700 - val_loss: 32.6381 - val_mean_absolute_error: 4.3256\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 22.59217\n",
      "Epoch 235/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 127.8415 - mean_absolute_error: 8.8576 - val_loss: 25.0085 - val_mean_absolute_error: 3.6994\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 22.59217\n",
      "Epoch 236/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.6600 - mean_absolute_error: 9.1845 - val_loss: 26.6958 - val_mean_absolute_error: 3.7276\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 22.59217\n",
      "Epoch 237/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.0845 - mean_absolute_error: 8.8219 - val_loss: 26.2463 - val_mean_absolute_error: 3.7008\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 22.59217\n",
      "Epoch 238/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 139.9689 - mean_absolute_error: 9.2340 - val_loss: 26.5578 - val_mean_absolute_error: 3.6700\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 22.59217\n",
      "Epoch 239/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.3065 - mean_absolute_error: 9.2548 - val_loss: 25.4790 - val_mean_absolute_error: 3.7906\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 22.59217\n",
      "Epoch 240/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.6376 - mean_absolute_error: 9.4492 - val_loss: 27.1758 - val_mean_absolute_error: 3.8663\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 22.59217\n",
      "Epoch 241/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.7514 - mean_absolute_error: 9.2861 - val_loss: 35.7080 - val_mean_absolute_error: 4.4816\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 22.59217\n",
      "Epoch 242/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.3425 - mean_absolute_error: 9.0433 - val_loss: 40.3337 - val_mean_absolute_error: 4.7157\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 22.59217\n",
      "Epoch 243/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.7941 - mean_absolute_error: 9.4733 - val_loss: 39.8468 - val_mean_absolute_error: 4.8574\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 22.59217\n",
      "Epoch 244/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 129.9507 - mean_absolute_error: 8.9925 - val_loss: 24.9733 - val_mean_absolute_error: 3.4608\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 22.59217\n",
      "Epoch 245/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 133.6472 - mean_absolute_error: 9.0380 - val_loss: 31.1433 - val_mean_absolute_error: 4.2220\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 22.59217\n",
      "Epoch 246/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 143.7319 - mean_absolute_error: 9.3668 - val_loss: 31.7831 - val_mean_absolute_error: 4.3124\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 22.59217\n",
      "Epoch 247/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.2677 - mean_absolute_error: 9.3342 - val_loss: 25.0393 - val_mean_absolute_error: 3.6124\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 22.59217\n",
      "Epoch 248/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.3307 - mean_absolute_error: 9.8566 - val_loss: 25.3280 - val_mean_absolute_error: 3.5851\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 22.59217\n",
      "Epoch 249/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.8369 - mean_absolute_error: 9.6230 - val_loss: 34.7397 - val_mean_absolute_error: 4.5310\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 22.59217\n",
      "Epoch 250/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.8233 - mean_absolute_error: 9.6641 - val_loss: 39.3778 - val_mean_absolute_error: 4.9182\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 22.59217\n",
      "Epoch 251/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.6337 - mean_absolute_error: 9.0701 - val_loss: 32.6538 - val_mean_absolute_error: 4.3164\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 22.59217\n",
      "Epoch 252/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 125.4764 - mean_absolute_error: 8.8492 - val_loss: 42.8411 - val_mean_absolute_error: 5.0881\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 22.59217\n",
      "Epoch 253/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 145.1970 - mean_absolute_error: 9.3049 - val_loss: 26.8466 - val_mean_absolute_error: 3.7493\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 22.59217\n",
      "Epoch 254/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.2901 - mean_absolute_error: 9.4006 - val_loss: 25.8794 - val_mean_absolute_error: 3.7866\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 22.59217\n",
      "Epoch 255/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.3708 - mean_absolute_error: 9.1888 - val_loss: 24.8497 - val_mean_absolute_error: 3.5763\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 22.59217\n",
      "Epoch 256/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 131.9204 - mean_absolute_error: 9.0514 - val_loss: 41.1020 - val_mean_absolute_error: 5.0257\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 22.59217\n",
      "Epoch 257/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 133.8324 - mean_absolute_error: 9.0947 - val_loss: 27.1006 - val_mean_absolute_error: 3.7597\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 22.59217\n",
      "Epoch 258/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.2368 - mean_absolute_error: 9.1192 - val_loss: 30.0848 - val_mean_absolute_error: 4.0791\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 22.59217\n",
      "Epoch 259/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.3356 - mean_absolute_error: 9.5091 - val_loss: 27.4270 - val_mean_absolute_error: 3.8600\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 22.59217\n",
      "Epoch 260/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.0104 - mean_absolute_error: 8.8582 - val_loss: 32.1628 - val_mean_absolute_error: 4.2816\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 22.59217\n",
      "Epoch 261/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 146.5384 - mean_absolute_error: 9.4697 - val_loss: 26.8659 - val_mean_absolute_error: 3.8231\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 22.59217\n",
      "Epoch 262/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 136.9920 - mean_absolute_error: 9.0590 - val_loss: 27.1168 - val_mean_absolute_error: 3.8586\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 22.59217\n",
      "Epoch 263/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.5414 - mean_absolute_error: 9.2727 - val_loss: 36.0582 - val_mean_absolute_error: 4.6688\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 22.59217\n",
      "Epoch 264/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 123.5621 - mean_absolute_error: 8.8476 - val_loss: 34.2352 - val_mean_absolute_error: 4.4628\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 22.59217\n",
      "Epoch 265/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 130.0853 - mean_absolute_error: 9.0223 - val_loss: 27.5351 - val_mean_absolute_error: 3.9280\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 22.59217\n",
      "Epoch 266/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.9355 - mean_absolute_error: 9.5826 - val_loss: 25.4433 - val_mean_absolute_error: 3.7257\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 22.59217\n",
      "Epoch 267/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 136.9899 - mean_absolute_error: 9.0970 - val_loss: 31.3626 - val_mean_absolute_error: 4.1912\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 22.59217\n",
      "Epoch 268/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 134.1563 - mean_absolute_error: 9.0136 - val_loss: 46.4731 - val_mean_absolute_error: 5.4961\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 22.59217\n",
      "Epoch 269/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.0677 - mean_absolute_error: 9.5280 - val_loss: 39.0753 - val_mean_absolute_error: 4.8698\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 22.59217\n",
      "Epoch 00269: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_RNFL = lstm_baseline(X_train, y_train)\n",
    "outputs = [layer.output for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model_RNFL.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = y_pred.tolist()\n",
    "dictionary_DF = {'predicted':y_pred, 'actual':y_test}\n",
    "y_RNFL_DF = pd.DataFrame(dictionary_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.303093</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.612396</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.395008</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82.982697</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81.850433</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71.222908</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.775566</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73.106834</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>82.103714</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94.369431</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84.290535</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75.818901</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>87.226982</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>65.873711</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>74.332970</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>93.878418</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56.224892</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>96.826477</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>107.671776</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>62.551289</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>86.929611</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83.766602</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>69.685463</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>81.624947</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>93.270737</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>85.593384</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>53.084858</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>94.964470</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>82.771713</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>72.639481</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>100.470024</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>64.569626</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>69.636894</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>85.964638</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>64.303055</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>75.384956</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>60.957458</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>80.297691</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>71.974144</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>71.223419</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>84.860359</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>94.865273</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>77.883118</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>76.700645</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>51.895576</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>81.283882</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>56.898197</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>85.169853</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>68.929794</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>87.861748</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>75.126091</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>69.505409</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>59.956028</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>65.712990</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>76.081734</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>103.951195</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>81.262970</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>103.530388</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>59.059700</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>67.913544</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted  actual\n",
       "0     79.303093    74.0\n",
       "1     98.612396    93.0\n",
       "2     51.395008    54.0\n",
       "3     82.982697    84.0\n",
       "4     81.850433    81.0\n",
       "5     71.222908    69.0\n",
       "6     86.775566    87.0\n",
       "7     73.106834    64.0\n",
       "8     82.103714    76.0\n",
       "9     94.369431    92.0\n",
       "10    84.290535    85.0\n",
       "11    75.818901    79.0\n",
       "12    87.226982    95.0\n",
       "13    65.873711    73.0\n",
       "14    74.332970    68.0\n",
       "15    93.878418    93.0\n",
       "16    56.224892    58.0\n",
       "17    96.826477    93.0\n",
       "18   107.671776   105.0\n",
       "19    62.551289    56.0\n",
       "20    86.929611    87.0\n",
       "21    83.766602    97.0\n",
       "22    69.685463    64.0\n",
       "23    81.624947    80.0\n",
       "24    93.270737    90.0\n",
       "25    85.593384    87.0\n",
       "26    53.084858    56.0\n",
       "27    94.964470    99.0\n",
       "28    82.771713    87.0\n",
       "29    72.639481    70.0\n",
       "..          ...     ...\n",
       "134  100.470024   102.0\n",
       "135   64.569626    67.0\n",
       "136   69.636894    68.0\n",
       "137   85.964638    94.0\n",
       "138   64.303055    63.0\n",
       "139   75.384956    72.0\n",
       "140   60.957458    64.0\n",
       "141   80.297691    74.0\n",
       "142   71.974144    61.0\n",
       "143   71.223419    72.0\n",
       "144   84.860359    87.0\n",
       "145   94.865273    95.0\n",
       "146   77.883118    80.0\n",
       "147   76.700645    78.0\n",
       "148   51.895576    52.0\n",
       "149   81.283882    84.0\n",
       "150   56.898197    60.0\n",
       "151   85.169853    78.0\n",
       "152   68.929794    66.0\n",
       "153   87.861748    78.0\n",
       "154   75.126091    73.0\n",
       "155   69.505409    71.0\n",
       "156   59.956028    61.0\n",
       "157   65.712990    65.0\n",
       "158   76.081734    74.0\n",
       "159  103.951195   104.0\n",
       "160   81.262970    78.0\n",
       "161  103.530388   108.0\n",
       "162   59.059700    61.0\n",
       "163   67.913544    66.0\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_RNFL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    164.000000\n",
       "mean       3.566009\n",
       "std        3.033462\n",
       "min        0.010735\n",
       "25%        1.517568\n",
       "50%        2.732998\n",
       "75%        5.307162\n",
       "max       18.885056\n",
       "dtype: float64"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_RNFL = abs(y_RNFL_DF['predicted']-y_RNFL_DF['actual'])\n",
    "difference_RNFL.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_MD, test_size = 0.2, random_state = 200)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/1000\n",
      "522/522 [==============================] - 14s 27ms/step - loss: 0.2388 - mean_absolute_error: 0.3890 - val_loss: 0.1609 - val_mean_absolute_error: 0.3190\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16087, saving model to RNN_best\n",
      "Epoch 2/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1707 - mean_absolute_error: 0.3315 - val_loss: 0.1353 - val_mean_absolute_error: 0.2800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16087 to 0.13532, saving model to RNN_best\n",
      "Epoch 3/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1364 - mean_absolute_error: 0.2882 - val_loss: 0.0989 - val_mean_absolute_error: 0.2286\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13532 to 0.09887, saving model to RNN_best\n",
      "Epoch 4/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1255 - mean_absolute_error: 0.2701 - val_loss: 0.0994 - val_mean_absolute_error: 0.2256\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09887\n",
      "Epoch 5/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1103 - mean_absolute_error: 0.2521 - val_loss: 0.1020 - val_mean_absolute_error: 0.2224\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09887\n",
      "Epoch 6/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1048 - mean_absolute_error: 0.2447 - val_loss: 0.0902 - val_mean_absolute_error: 0.2096\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09887 to 0.09019, saving model to RNN_best\n",
      "Epoch 7/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0991 - mean_absolute_error: 0.2316 - val_loss: 0.0900 - val_mean_absolute_error: 0.2063\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09019 to 0.09001, saving model to RNN_best\n",
      "Epoch 8/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0939 - mean_absolute_error: 0.2282 - val_loss: 0.0904 - val_mean_absolute_error: 0.2056\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09001\n",
      "Epoch 9/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0941 - mean_absolute_error: 0.2263 - val_loss: 0.0853 - val_mean_absolute_error: 0.1992\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09001 to 0.08527, saving model to RNN_best\n",
      "Epoch 10/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.1019 - mean_absolute_error: 0.2366 - val_loss: 0.1052 - val_mean_absolute_error: 0.2182\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08527\n",
      "Epoch 11/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0993 - mean_absolute_error: 0.2274 - val_loss: 0.0869 - val_mean_absolute_error: 0.2014\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08527\n",
      "Epoch 12/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0949 - mean_absolute_error: 0.2272 - val_loss: 0.0992 - val_mean_absolute_error: 0.2127\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08527\n",
      "Epoch 13/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0990 - mean_absolute_error: 0.2306 - val_loss: 0.0842 - val_mean_absolute_error: 0.1973\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08527 to 0.08419, saving model to RNN_best\n",
      "Epoch 14/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0888 - mean_absolute_error: 0.2212 - val_loss: 0.0861 - val_mean_absolute_error: 0.1985\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08419\n",
      "Epoch 15/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0898 - mean_absolute_error: 0.2193 - val_loss: 0.0829 - val_mean_absolute_error: 0.1945\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08419 to 0.08287, saving model to RNN_best\n",
      "Epoch 16/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0864 - mean_absolute_error: 0.2127 - val_loss: 0.0768 - val_mean_absolute_error: 0.1873\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08287 to 0.07678, saving model to RNN_best\n",
      "Epoch 17/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0860 - mean_absolute_error: 0.2112 - val_loss: 0.0978 - val_mean_absolute_error: 0.2089\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.07678\n",
      "Epoch 18/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0874 - mean_absolute_error: 0.2146 - val_loss: 0.0785 - val_mean_absolute_error: 0.1895\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.07678\n",
      "Epoch 19/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0876 - mean_absolute_error: 0.2171 - val_loss: 0.0939 - val_mean_absolute_error: 0.2043\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07678\n",
      "Epoch 20/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0864 - mean_absolute_error: 0.2127 - val_loss: 0.0871 - val_mean_absolute_error: 0.1975\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.07678\n",
      "Epoch 21/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0840 - mean_absolute_error: 0.2116 - val_loss: 0.0736 - val_mean_absolute_error: 0.1825\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.07678 to 0.07359, saving model to RNN_best\n",
      "Epoch 22/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0820 - mean_absolute_error: 0.2120 - val_loss: 0.0807 - val_mean_absolute_error: 0.1895\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07359\n",
      "Epoch 23/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0837 - mean_absolute_error: 0.2133 - val_loss: 0.0843 - val_mean_absolute_error: 0.1919\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07359\n",
      "Epoch 24/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0813 - mean_absolute_error: 0.2085 - val_loss: 0.0742 - val_mean_absolute_error: 0.1820\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07359\n",
      "Epoch 25/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0805 - mean_absolute_error: 0.2083 - val_loss: 0.0955 - val_mean_absolute_error: 0.2054\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07359\n",
      "Epoch 26/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0849 - mean_absolute_error: 0.2106 - val_loss: 0.0733 - val_mean_absolute_error: 0.1821\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07359 to 0.07333, saving model to RNN_best\n",
      "Epoch 27/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0816 - mean_absolute_error: 0.2067 - val_loss: 0.0970 - val_mean_absolute_error: 0.2057\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07333\n",
      "Epoch 28/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0839 - mean_absolute_error: 0.2101 - val_loss: 0.0769 - val_mean_absolute_error: 0.1823\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07333\n",
      "Epoch 29/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0743 - mean_absolute_error: 0.1967 - val_loss: 0.0806 - val_mean_absolute_error: 0.1873\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.07333\n",
      "Epoch 30/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0718 - mean_absolute_error: 0.1929 - val_loss: 0.0712 - val_mean_absolute_error: 0.1777\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07333 to 0.07120, saving model to RNN_best\n",
      "Epoch 31/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0773 - mean_absolute_error: 0.2009 - val_loss: 0.0801 - val_mean_absolute_error: 0.1855\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07120\n",
      "Epoch 32/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0777 - mean_absolute_error: 0.2024 - val_loss: 0.0745 - val_mean_absolute_error: 0.1815\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.07120\n",
      "Epoch 33/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0742 - mean_absolute_error: 0.1943 - val_loss: 0.0797 - val_mean_absolute_error: 0.1844\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07120\n",
      "Epoch 34/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0786 - mean_absolute_error: 0.2016 - val_loss: 0.0797 - val_mean_absolute_error: 0.1879\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.07120\n",
      "Epoch 35/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0800 - mean_absolute_error: 0.2077 - val_loss: 0.0736 - val_mean_absolute_error: 0.1772\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07120\n",
      "Epoch 36/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0691 - mean_absolute_error: 0.1893 - val_loss: 0.0661 - val_mean_absolute_error: 0.1684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss improved from 0.07120 to 0.06615, saving model to RNN_best\n",
      "Epoch 37/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0780 - mean_absolute_error: 0.2026 - val_loss: 0.0944 - val_mean_absolute_error: 0.1988\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.06615\n",
      "Epoch 38/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0826 - mean_absolute_error: 0.2072 - val_loss: 0.0899 - val_mean_absolute_error: 0.1946\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.06615\n",
      "Epoch 39/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0779 - mean_absolute_error: 0.1974 - val_loss: 0.0715 - val_mean_absolute_error: 0.1758\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.06615\n",
      "Epoch 40/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0725 - mean_absolute_error: 0.1952 - val_loss: 0.0721 - val_mean_absolute_error: 0.1792\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.06615\n",
      "Epoch 41/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0753 - mean_absolute_error: 0.1994 - val_loss: 0.0763 - val_mean_absolute_error: 0.1811\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.06615\n",
      "Epoch 42/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0745 - mean_absolute_error: 0.1964 - val_loss: 0.0825 - val_mean_absolute_error: 0.1888\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.06615\n",
      "Epoch 43/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0690 - mean_absolute_error: 0.1850 - val_loss: 0.0667 - val_mean_absolute_error: 0.1733\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.06615\n",
      "Epoch 44/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0707 - mean_absolute_error: 0.1918 - val_loss: 0.0914 - val_mean_absolute_error: 0.1987\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.06615\n",
      "Epoch 45/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0754 - mean_absolute_error: 0.1956 - val_loss: 0.0788 - val_mean_absolute_error: 0.1833\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.06615\n",
      "Epoch 46/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0707 - mean_absolute_error: 0.1909 - val_loss: 0.0687 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.06615\n",
      "Epoch 47/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0706 - mean_absolute_error: 0.1909 - val_loss: 0.0898 - val_mean_absolute_error: 0.1960\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.06615\n",
      "Epoch 48/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0715 - mean_absolute_error: 0.1945 - val_loss: 0.0665 - val_mean_absolute_error: 0.1711\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.06615\n",
      "Epoch 49/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0708 - mean_absolute_error: 0.1931 - val_loss: 0.0735 - val_mean_absolute_error: 0.1790\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.06615\n",
      "Epoch 50/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0714 - mean_absolute_error: 0.1915 - val_loss: 0.0767 - val_mean_absolute_error: 0.1802\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.06615\n",
      "Epoch 51/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0701 - mean_absolute_error: 0.1833 - val_loss: 0.0671 - val_mean_absolute_error: 0.1706\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.06615\n",
      "Epoch 52/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0675 - mean_absolute_error: 0.1841 - val_loss: 0.0736 - val_mean_absolute_error: 0.1788\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.06615\n",
      "Epoch 53/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0710 - mean_absolute_error: 0.1907 - val_loss: 0.0844 - val_mean_absolute_error: 0.1905\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.06615\n",
      "Epoch 54/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0705 - mean_absolute_error: 0.1865 - val_loss: 0.0703 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.06615\n",
      "Epoch 55/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0657 - mean_absolute_error: 0.1856 - val_loss: 0.0711 - val_mean_absolute_error: 0.1749\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.06615\n",
      "Epoch 56/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0693 - mean_absolute_error: 0.1894 - val_loss: 0.0831 - val_mean_absolute_error: 0.1900\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.06615\n",
      "Epoch 57/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0676 - mean_absolute_error: 0.1865 - val_loss: 0.0700 - val_mean_absolute_error: 0.1727\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.06615\n",
      "Epoch 58/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0622 - mean_absolute_error: 0.1728 - val_loss: 0.0676 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.06615\n",
      "Epoch 59/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0656 - mean_absolute_error: 0.1809 - val_loss: 0.0756 - val_mean_absolute_error: 0.1814\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.06615\n",
      "Epoch 60/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0629 - mean_absolute_error: 0.1794 - val_loss: 0.0771 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.06615\n",
      "Epoch 61/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0655 - mean_absolute_error: 0.1829 - val_loss: 0.0624 - val_mean_absolute_error: 0.1664\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.06615 to 0.06244, saving model to RNN_best\n",
      "Epoch 62/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0614 - mean_absolute_error: 0.1777 - val_loss: 0.0724 - val_mean_absolute_error: 0.1776\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.06244\n",
      "Epoch 63/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0658 - mean_absolute_error: 0.1844 - val_loss: 0.0707 - val_mean_absolute_error: 0.1723\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.06244\n",
      "Epoch 64/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0622 - mean_absolute_error: 0.1794 - val_loss: 0.0627 - val_mean_absolute_error: 0.1634\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.06244\n",
      "Epoch 65/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0601 - mean_absolute_error: 0.1768 - val_loss: 0.0741 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.06244\n",
      "Epoch 66/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0589 - mean_absolute_error: 0.1747 - val_loss: 0.0658 - val_mean_absolute_error: 0.1669\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.06244\n",
      "Epoch 67/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0624 - mean_absolute_error: 0.1833 - val_loss: 0.0831 - val_mean_absolute_error: 0.1908\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.06244\n",
      "Epoch 68/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0658 - mean_absolute_error: 0.1821 - val_loss: 0.0799 - val_mean_absolute_error: 0.1844\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.06244\n",
      "Epoch 69/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0622 - mean_absolute_error: 0.1729 - val_loss: 0.0684 - val_mean_absolute_error: 0.1735\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.06244\n",
      "Epoch 70/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0632 - mean_absolute_error: 0.1777 - val_loss: 0.0718 - val_mean_absolute_error: 0.1802\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.06244\n",
      "Epoch 71/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0647 - mean_absolute_error: 0.1771 - val_loss: 0.0647 - val_mean_absolute_error: 0.1704\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.06244\n",
      "Epoch 72/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0572 - mean_absolute_error: 0.1715 - val_loss: 0.0888 - val_mean_absolute_error: 0.1948\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.06244\n",
      "Epoch 73/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0639 - mean_absolute_error: 0.1799 - val_loss: 0.0634 - val_mean_absolute_error: 0.1664\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.06244\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0619 - mean_absolute_error: 0.1766 - val_loss: 0.0721 - val_mean_absolute_error: 0.1754\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.06244\n",
      "Epoch 75/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0622 - mean_absolute_error: 0.1755 - val_loss: 0.0704 - val_mean_absolute_error: 0.1736\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.06244\n",
      "Epoch 76/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0652 - mean_absolute_error: 0.1768 - val_loss: 0.0723 - val_mean_absolute_error: 0.1793\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.06244\n",
      "Epoch 77/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0556 - mean_absolute_error: 0.1625 - val_loss: 0.0617 - val_mean_absolute_error: 0.1657\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.06244 to 0.06168, saving model to RNN_best\n",
      "Epoch 78/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0534 - mean_absolute_error: 0.1672 - val_loss: 0.0682 - val_mean_absolute_error: 0.1736\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.06168\n",
      "Epoch 79/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0575 - mean_absolute_error: 0.1710 - val_loss: 0.0801 - val_mean_absolute_error: 0.1848\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.06168\n",
      "Epoch 80/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0567 - mean_absolute_error: 0.1684 - val_loss: 0.0729 - val_mean_absolute_error: 0.1843\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.06168\n",
      "Epoch 81/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0565 - mean_absolute_error: 0.1706 - val_loss: 0.0718 - val_mean_absolute_error: 0.1798\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.06168\n",
      "Epoch 82/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0577 - mean_absolute_error: 0.1679 - val_loss: 0.0696 - val_mean_absolute_error: 0.1771\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.06168\n",
      "Epoch 83/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0543 - mean_absolute_error: 0.1632 - val_loss: 0.0666 - val_mean_absolute_error: 0.1718\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.06168\n",
      "Epoch 84/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0531 - mean_absolute_error: 0.1598 - val_loss: 0.0669 - val_mean_absolute_error: 0.1701\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.06168\n",
      "Epoch 85/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0545 - mean_absolute_error: 0.1662 - val_loss: 0.0704 - val_mean_absolute_error: 0.1758\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.06168\n",
      "Epoch 86/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0561 - mean_absolute_error: 0.1647 - val_loss: 0.0713 - val_mean_absolute_error: 0.1751\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.06168\n",
      "Epoch 87/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0573 - mean_absolute_error: 0.1686 - val_loss: 0.0600 - val_mean_absolute_error: 0.1659\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.06168 to 0.05998, saving model to RNN_best\n",
      "Epoch 88/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0539 - mean_absolute_error: 0.1623 - val_loss: 0.0650 - val_mean_absolute_error: 0.1699\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.05998\n",
      "Epoch 89/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0563 - mean_absolute_error: 0.1633 - val_loss: 0.0749 - val_mean_absolute_error: 0.1809\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.05998\n",
      "Epoch 90/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0561 - mean_absolute_error: 0.1693 - val_loss: 0.0887 - val_mean_absolute_error: 0.1974\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.05998\n",
      "Epoch 91/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0563 - mean_absolute_error: 0.1707 - val_loss: 0.0737 - val_mean_absolute_error: 0.1824\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.05998\n",
      "Epoch 92/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0504 - mean_absolute_error: 0.1613 - val_loss: 0.0686 - val_mean_absolute_error: 0.1759\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.05998\n",
      "Epoch 93/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0542 - mean_absolute_error: 0.1661 - val_loss: 0.0644 - val_mean_absolute_error: 0.1697\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.05998\n",
      "Epoch 94/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0601 - mean_absolute_error: 0.1713 - val_loss: 0.0807 - val_mean_absolute_error: 0.1893\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.05998\n",
      "Epoch 95/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0568 - mean_absolute_error: 0.1639 - val_loss: 0.0675 - val_mean_absolute_error: 0.1742\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.05998\n",
      "Epoch 96/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0505 - mean_absolute_error: 0.1603 - val_loss: 0.0647 - val_mean_absolute_error: 0.1714\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.05998\n",
      "Epoch 97/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0508 - mean_absolute_error: 0.1590 - val_loss: 0.0674 - val_mean_absolute_error: 0.1752\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.05998\n",
      "Epoch 98/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0514 - mean_absolute_error: 0.1600 - val_loss: 0.0650 - val_mean_absolute_error: 0.1773\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.05998\n",
      "Epoch 99/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0483 - mean_absolute_error: 0.1561 - val_loss: 0.0661 - val_mean_absolute_error: 0.1739\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.05998\n",
      "Epoch 100/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0556 - mean_absolute_error: 0.1626 - val_loss: 0.0743 - val_mean_absolute_error: 0.1796\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.05998\n",
      "Epoch 101/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0508 - mean_absolute_error: 0.1561 - val_loss: 0.0648 - val_mean_absolute_error: 0.1761\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.05998\n",
      "Epoch 102/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0512 - mean_absolute_error: 0.1596 - val_loss: 0.0681 - val_mean_absolute_error: 0.1812\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.05998\n",
      "Epoch 103/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0503 - mean_absolute_error: 0.1569 - val_loss: 0.0659 - val_mean_absolute_error: 0.1762\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.05998\n",
      "Epoch 104/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0519 - mean_absolute_error: 0.1591 - val_loss: 0.0740 - val_mean_absolute_error: 0.1805\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.05998\n",
      "Epoch 105/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0496 - mean_absolute_error: 0.1591 - val_loss: 0.0654 - val_mean_absolute_error: 0.1761\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.05998\n",
      "Epoch 106/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0494 - mean_absolute_error: 0.1573 - val_loss: 0.0680 - val_mean_absolute_error: 0.1769\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.05998\n",
      "Epoch 107/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0469 - mean_absolute_error: 0.1509 - val_loss: 0.0687 - val_mean_absolute_error: 0.1806\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.05998\n",
      "Epoch 108/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0501 - mean_absolute_error: 0.1564 - val_loss: 0.0735 - val_mean_absolute_error: 0.1851\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.05998\n",
      "Epoch 109/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0521 - mean_absolute_error: 0.1565 - val_loss: 0.0642 - val_mean_absolute_error: 0.1761\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.05998\n",
      "Epoch 110/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0397 - mean_absolute_error: 0.1412 - val_loss: 0.0628 - val_mean_absolute_error: 0.1783\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.05998\n",
      "Epoch 111/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0466 - mean_absolute_error: 0.1531 - val_loss: 0.0613 - val_mean_absolute_error: 0.1743\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.05998\n",
      "Epoch 112/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0503 - mean_absolute_error: 0.1563 - val_loss: 0.0630 - val_mean_absolute_error: 0.1712\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.05998\n",
      "Epoch 113/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0525 - mean_absolute_error: 0.1604 - val_loss: 0.0789 - val_mean_absolute_error: 0.1941\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.05998\n",
      "Epoch 114/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0524 - mean_absolute_error: 0.1591 - val_loss: 0.0653 - val_mean_absolute_error: 0.1720\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.05998\n",
      "Epoch 115/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0462 - mean_absolute_error: 0.1489 - val_loss: 0.0642 - val_mean_absolute_error: 0.1759\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.05998\n",
      "Epoch 116/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0484 - mean_absolute_error: 0.1512 - val_loss: 0.0681 - val_mean_absolute_error: 0.1732\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.05998\n",
      "Epoch 117/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0481 - mean_absolute_error: 0.1499 - val_loss: 0.0676 - val_mean_absolute_error: 0.1724\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.05998\n",
      "Epoch 118/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0457 - mean_absolute_error: 0.1495 - val_loss: 0.0673 - val_mean_absolute_error: 0.1758\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.05998\n",
      "Epoch 119/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0441 - mean_absolute_error: 0.1475 - val_loss: 0.0621 - val_mean_absolute_error: 0.1759\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.05998\n",
      "Epoch 120/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0465 - mean_absolute_error: 0.1510 - val_loss: 0.0637 - val_mean_absolute_error: 0.1736\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.05998\n",
      "Epoch 121/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0424 - mean_absolute_error: 0.1467 - val_loss: 0.0606 - val_mean_absolute_error: 0.1676\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.05998\n",
      "Epoch 122/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0441 - mean_absolute_error: 0.1480 - val_loss: 0.0687 - val_mean_absolute_error: 0.1753\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.05998\n",
      "Epoch 123/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0438 - mean_absolute_error: 0.1460 - val_loss: 0.0638 - val_mean_absolute_error: 0.1792\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.05998\n",
      "Epoch 124/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0441 - mean_absolute_error: 0.1458 - val_loss: 0.0625 - val_mean_absolute_error: 0.1725\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.05998\n",
      "Epoch 125/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0422 - mean_absolute_error: 0.1436 - val_loss: 0.0695 - val_mean_absolute_error: 0.1725\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.05998\n",
      "Epoch 126/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0412 - mean_absolute_error: 0.1390 - val_loss: 0.0680 - val_mean_absolute_error: 0.1792\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.05998\n",
      "Epoch 127/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0407 - mean_absolute_error: 0.1429 - val_loss: 0.0653 - val_mean_absolute_error: 0.1780\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.05998\n",
      "Epoch 128/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0401 - mean_absolute_error: 0.1380 - val_loss: 0.0599 - val_mean_absolute_error: 0.1631\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.05998 to 0.05991, saving model to RNN_best\n",
      "Epoch 129/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0436 - mean_absolute_error: 0.1441 - val_loss: 0.0626 - val_mean_absolute_error: 0.1710\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.05991\n",
      "Epoch 130/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0473 - mean_absolute_error: 0.1531 - val_loss: 0.0659 - val_mean_absolute_error: 0.1800\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.05991\n",
      "Epoch 131/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0413 - mean_absolute_error: 0.1424 - val_loss: 0.0720 - val_mean_absolute_error: 0.1789\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.05991\n",
      "Epoch 132/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0455 - mean_absolute_error: 0.1515 - val_loss: 0.0610 - val_mean_absolute_error: 0.1671\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.05991\n",
      "Epoch 133/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0410 - mean_absolute_error: 0.1394 - val_loss: 0.0746 - val_mean_absolute_error: 0.1866\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.05991\n",
      "Epoch 134/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0412 - mean_absolute_error: 0.1444 - val_loss: 0.0658 - val_mean_absolute_error: 0.1828\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.05991\n",
      "Epoch 135/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0380 - mean_absolute_error: 0.1376 - val_loss: 0.0658 - val_mean_absolute_error: 0.1746\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.05991\n",
      "Epoch 136/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0388 - mean_absolute_error: 0.1380 - val_loss: 0.0738 - val_mean_absolute_error: 0.1941\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.05991\n",
      "Epoch 137/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0423 - mean_absolute_error: 0.1426 - val_loss: 0.0704 - val_mean_absolute_error: 0.1872\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.05991\n",
      "Epoch 138/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0447 - mean_absolute_error: 0.1459 - val_loss: 0.0687 - val_mean_absolute_error: 0.1742\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.05991\n",
      "Epoch 139/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0404 - mean_absolute_error: 0.1413 - val_loss: 0.0676 - val_mean_absolute_error: 0.1830\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.05991\n",
      "Epoch 140/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0406 - mean_absolute_error: 0.1404 - val_loss: 0.0676 - val_mean_absolute_error: 0.1866\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.05991\n",
      "Epoch 141/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0359 - mean_absolute_error: 0.1327 - val_loss: 0.0659 - val_mean_absolute_error: 0.1793\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.05991\n",
      "Epoch 142/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0358 - mean_absolute_error: 0.1317 - val_loss: 0.0648 - val_mean_absolute_error: 0.1769\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.05991\n",
      "Epoch 143/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0387 - mean_absolute_error: 0.1372 - val_loss: 0.0651 - val_mean_absolute_error: 0.1780\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.05991\n",
      "Epoch 144/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0408 - mean_absolute_error: 0.1424 - val_loss: 0.0736 - val_mean_absolute_error: 0.1848\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.05991\n",
      "Epoch 145/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0396 - mean_absolute_error: 0.1380 - val_loss: 0.0683 - val_mean_absolute_error: 0.1781\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.05991\n",
      "Epoch 146/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0397 - mean_absolute_error: 0.1410 - val_loss: 0.0754 - val_mean_absolute_error: 0.1891\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.05991\n",
      "Epoch 147/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0419 - mean_absolute_error: 0.1438 - val_loss: 0.0684 - val_mean_absolute_error: 0.1780\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.05991\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0357 - mean_absolute_error: 0.1334 - val_loss: 0.0792 - val_mean_absolute_error: 0.1981\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.05991\n",
      "Epoch 149/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0397 - mean_absolute_error: 0.1380 - val_loss: 0.0720 - val_mean_absolute_error: 0.1933\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.05991\n",
      "Epoch 150/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0401 - mean_absolute_error: 0.1437 - val_loss: 0.0636 - val_mean_absolute_error: 0.1734\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.05991\n",
      "Epoch 151/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0406 - mean_absolute_error: 0.1383 - val_loss: 0.0800 - val_mean_absolute_error: 0.1894\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.05991\n",
      "Epoch 152/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0365 - mean_absolute_error: 0.1348 - val_loss: 0.0713 - val_mean_absolute_error: 0.1836\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.05991\n",
      "Epoch 153/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0396 - mean_absolute_error: 0.1378 - val_loss: 0.0613 - val_mean_absolute_error: 0.1714\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.05991\n",
      "Epoch 154/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0371 - mean_absolute_error: 0.1353 - val_loss: 0.0652 - val_mean_absolute_error: 0.1832\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.05991\n",
      "Epoch 155/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0331 - mean_absolute_error: 0.1256 - val_loss: 0.0651 - val_mean_absolute_error: 0.1854\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.05991\n",
      "Epoch 156/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0363 - mean_absolute_error: 0.1336 - val_loss: 0.0570 - val_mean_absolute_error: 0.1676\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.05991 to 0.05701, saving model to RNN_best\n",
      "Epoch 157/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0420 - mean_absolute_error: 0.1442 - val_loss: 0.0795 - val_mean_absolute_error: 0.1853\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.05701\n",
      "Epoch 158/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0428 - mean_absolute_error: 0.1470 - val_loss: 0.0646 - val_mean_absolute_error: 0.1832\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.05701\n",
      "Epoch 159/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0385 - mean_absolute_error: 0.1351 - val_loss: 0.0747 - val_mean_absolute_error: 0.1905\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.05701\n",
      "Epoch 160/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0323 - mean_absolute_error: 0.1282 - val_loss: 0.0630 - val_mean_absolute_error: 0.1799\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.05701\n",
      "Epoch 161/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0294 - mean_absolute_error: 0.1210 - val_loss: 0.0665 - val_mean_absolute_error: 0.1841\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.05701\n",
      "Epoch 162/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0296 - mean_absolute_error: 0.1221 - val_loss: 0.0661 - val_mean_absolute_error: 0.1826\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.05701\n",
      "Epoch 163/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0306 - mean_absolute_error: 0.1250 - val_loss: 0.0732 - val_mean_absolute_error: 0.1906\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.05701\n",
      "Epoch 164/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0368 - mean_absolute_error: 0.1342 - val_loss: 0.0650 - val_mean_absolute_error: 0.1747\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.05701\n",
      "Epoch 165/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0387 - mean_absolute_error: 0.1361 - val_loss: 0.0617 - val_mean_absolute_error: 0.1729\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.05701\n",
      "Epoch 166/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0328 - mean_absolute_error: 0.1278 - val_loss: 0.0743 - val_mean_absolute_error: 0.1884\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.05701\n",
      "Epoch 167/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0361 - mean_absolute_error: 0.1337 - val_loss: 0.0632 - val_mean_absolute_error: 0.1712\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.05701\n",
      "Epoch 168/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0357 - mean_absolute_error: 0.1333 - val_loss: 0.0611 - val_mean_absolute_error: 0.1738\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.05701\n",
      "Epoch 169/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0268 - mean_absolute_error: 0.1162 - val_loss: 0.0668 - val_mean_absolute_error: 0.1774\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.05701\n",
      "Epoch 170/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0300 - mean_absolute_error: 0.1231 - val_loss: 0.0678 - val_mean_absolute_error: 0.1833\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.05701\n",
      "Epoch 171/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0329 - mean_absolute_error: 0.1288 - val_loss: 0.0699 - val_mean_absolute_error: 0.1909\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.05701\n",
      "Epoch 172/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0338 - mean_absolute_error: 0.1299 - val_loss: 0.0659 - val_mean_absolute_error: 0.1750\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.05701\n",
      "Epoch 173/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0355 - mean_absolute_error: 0.1339 - val_loss: 0.0702 - val_mean_absolute_error: 0.1826\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.05701\n",
      "Epoch 174/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0312 - mean_absolute_error: 0.1241 - val_loss: 0.0695 - val_mean_absolute_error: 0.1822\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.05701\n",
      "Epoch 175/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0255 - mean_absolute_error: 0.1124 - val_loss: 0.0695 - val_mean_absolute_error: 0.1876\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.05701\n",
      "Epoch 176/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0293 - mean_absolute_error: 0.1204 - val_loss: 0.0693 - val_mean_absolute_error: 0.1876\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.05701\n",
      "Epoch 177/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0308 - mean_absolute_error: 0.1243 - val_loss: 0.0701 - val_mean_absolute_error: 0.1836\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.05701\n",
      "Epoch 178/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0353 - mean_absolute_error: 0.1279 - val_loss: 0.0777 - val_mean_absolute_error: 0.1935\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.05701\n",
      "Epoch 179/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0290 - mean_absolute_error: 0.1201 - val_loss: 0.0782 - val_mean_absolute_error: 0.1954\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.05701\n",
      "Epoch 180/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0279 - mean_absolute_error: 0.1154 - val_loss: 0.0707 - val_mean_absolute_error: 0.1878\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.05701\n",
      "Epoch 181/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0283 - mean_absolute_error: 0.1188 - val_loss: 0.0658 - val_mean_absolute_error: 0.1742\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.05701\n",
      "Epoch 182/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0272 - mean_absolute_error: 0.1151 - val_loss: 0.0634 - val_mean_absolute_error: 0.1712\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.05701\n",
      "Epoch 183/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0273 - mean_absolute_error: 0.1188 - val_loss: 0.0643 - val_mean_absolute_error: 0.1760\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.05701\n",
      "Epoch 184/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0253 - mean_absolute_error: 0.1076 - val_loss: 0.0752 - val_mean_absolute_error: 0.1893\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.05701\n",
      "Epoch 185/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0250 - mean_absolute_error: 0.1074 - val_loss: 0.0666 - val_mean_absolute_error: 0.1826\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.05701\n",
      "Epoch 186/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0256 - mean_absolute_error: 0.1104 - val_loss: 0.0674 - val_mean_absolute_error: 0.1777\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.05701\n",
      "Epoch 187/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0228 - mean_absolute_error: 0.1065 - val_loss: 0.0726 - val_mean_absolute_error: 0.1842\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.05701\n",
      "Epoch 188/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0240 - mean_absolute_error: 0.1080 - val_loss: 0.0682 - val_mean_absolute_error: 0.1816\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.05701\n",
      "Epoch 189/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0284 - mean_absolute_error: 0.1201 - val_loss: 0.0694 - val_mean_absolute_error: 0.1801\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.05701\n",
      "Epoch 190/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0254 - mean_absolute_error: 0.1158 - val_loss: 0.0696 - val_mean_absolute_error: 0.1878\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.05701\n",
      "Epoch 191/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0236 - mean_absolute_error: 0.1061 - val_loss: 0.0676 - val_mean_absolute_error: 0.1859\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.05701\n",
      "Epoch 192/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0276 - mean_absolute_error: 0.1186 - val_loss: 0.0581 - val_mean_absolute_error: 0.1688\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.05701\n",
      "Epoch 193/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0281 - mean_absolute_error: 0.1183 - val_loss: 0.0701 - val_mean_absolute_error: 0.1806\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.05701\n",
      "Epoch 194/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0234 - mean_absolute_error: 0.1065 - val_loss: 0.0668 - val_mean_absolute_error: 0.1821\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.05701\n",
      "Epoch 195/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0214 - mean_absolute_error: 0.1005 - val_loss: 0.0770 - val_mean_absolute_error: 0.1886\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.05701\n",
      "Epoch 196/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0190 - mean_absolute_error: 0.0988 - val_loss: 0.0696 - val_mean_absolute_error: 0.1885\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.05701\n",
      "Epoch 197/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0236 - mean_absolute_error: 0.1066 - val_loss: 0.0729 - val_mean_absolute_error: 0.1815\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.05701\n",
      "Epoch 198/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0249 - mean_absolute_error: 0.1102 - val_loss: 0.0666 - val_mean_absolute_error: 0.1815\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.05701\n",
      "Epoch 199/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0226 - mean_absolute_error: 0.1032 - val_loss: 0.0692 - val_mean_absolute_error: 0.1834\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.05701\n",
      "Epoch 200/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1040 - val_loss: 0.0796 - val_mean_absolute_error: 0.1976\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.05701\n",
      "Epoch 201/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0353 - mean_absolute_error: 0.1315 - val_loss: 0.0696 - val_mean_absolute_error: 0.1823\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.05701\n",
      "Epoch 202/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0284 - mean_absolute_error: 0.1207 - val_loss: 0.0769 - val_mean_absolute_error: 0.1917\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.05701\n",
      "Epoch 203/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0233 - mean_absolute_error: 0.1102 - val_loss: 0.0772 - val_mean_absolute_error: 0.1893\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.05701\n",
      "Epoch 204/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0239 - mean_absolute_error: 0.1070 - val_loss: 0.0715 - val_mean_absolute_error: 0.1860\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.05701\n",
      "Epoch 205/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0255 - mean_absolute_error: 0.1064 - val_loss: 0.0685 - val_mean_absolute_error: 0.1861\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.05701\n",
      "Epoch 206/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0210 - mean_absolute_error: 0.1011 - val_loss: 0.0707 - val_mean_absolute_error: 0.1906\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.05701\n",
      "Epoch 207/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0238 - mean_absolute_error: 0.1084 - val_loss: 0.0711 - val_mean_absolute_error: 0.1889\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.05701\n",
      "Epoch 208/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0193 - mean_absolute_error: 0.0997 - val_loss: 0.0723 - val_mean_absolute_error: 0.1931\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.05701\n",
      "Epoch 209/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0197 - mean_absolute_error: 0.0980 - val_loss: 0.0669 - val_mean_absolute_error: 0.1849\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.05701\n",
      "Epoch 210/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0204 - mean_absolute_error: 0.1047 - val_loss: 0.0726 - val_mean_absolute_error: 0.1950\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.05701\n",
      "Epoch 211/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0268 - mean_absolute_error: 0.1142 - val_loss: 0.0682 - val_mean_absolute_error: 0.1866\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.05701\n",
      "Epoch 212/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0301 - mean_absolute_error: 0.1228 - val_loss: 0.0752 - val_mean_absolute_error: 0.1861\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.05701\n",
      "Epoch 213/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0246 - mean_absolute_error: 0.1053 - val_loss: 0.0683 - val_mean_absolute_error: 0.1806\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.05701\n",
      "Epoch 214/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0232 - mean_absolute_error: 0.1103 - val_loss: 0.0740 - val_mean_absolute_error: 0.1901\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.05701\n",
      "Epoch 215/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1061 - val_loss: 0.0617 - val_mean_absolute_error: 0.1764\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.05701\n",
      "Epoch 216/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0212 - mean_absolute_error: 0.1072 - val_loss: 0.0643 - val_mean_absolute_error: 0.1790\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.05701\n",
      "Epoch 217/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0182 - mean_absolute_error: 0.0954 - val_loss: 0.0741 - val_mean_absolute_error: 0.1930\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.05701\n",
      "Epoch 218/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0187 - mean_absolute_error: 0.0989 - val_loss: 0.0721 - val_mean_absolute_error: 0.1909\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.05701\n",
      "Epoch 219/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0201 - mean_absolute_error: 0.0988 - val_loss: 0.0686 - val_mean_absolute_error: 0.1791\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.05701\n",
      "Epoch 220/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0246 - mean_absolute_error: 0.1083 - val_loss: 0.0746 - val_mean_absolute_error: 0.1842\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.05701\n",
      "Epoch 221/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0255 - mean_absolute_error: 0.1077 - val_loss: 0.0909 - val_mean_absolute_error: 0.2065\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.05701\n",
      "Epoch 222/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0245 - mean_absolute_error: 0.1132 - val_loss: 0.0781 - val_mean_absolute_error: 0.1975\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.05701\n",
      "Epoch 223/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0222 - mean_absolute_error: 0.1025 - val_loss: 0.0796 - val_mean_absolute_error: 0.1946\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.05701\n",
      "Epoch 224/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0202 - mean_absolute_error: 0.1007 - val_loss: 0.0642 - val_mean_absolute_error: 0.1786\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.05701\n",
      "Epoch 225/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0175 - mean_absolute_error: 0.0938 - val_loss: 0.0685 - val_mean_absolute_error: 0.1882\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.05701\n",
      "Epoch 226/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0204 - mean_absolute_error: 0.1013 - val_loss: 0.0764 - val_mean_absolute_error: 0.1955\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.05701\n",
      "Epoch 227/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0213 - mean_absolute_error: 0.1052 - val_loss: 0.0730 - val_mean_absolute_error: 0.1874\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.05701\n",
      "Epoch 228/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0179 - mean_absolute_error: 0.0984 - val_loss: 0.0813 - val_mean_absolute_error: 0.1958\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.05701\n",
      "Epoch 229/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0202 - mean_absolute_error: 0.1013 - val_loss: 0.0796 - val_mean_absolute_error: 0.1913\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.05701\n",
      "Epoch 230/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0205 - mean_absolute_error: 0.1006 - val_loss: 0.0679 - val_mean_absolute_error: 0.1835\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.05701\n",
      "Epoch 231/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0217 - mean_absolute_error: 0.0990 - val_loss: 0.0681 - val_mean_absolute_error: 0.1796\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.05701\n",
      "Epoch 232/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0200 - mean_absolute_error: 0.0963 - val_loss: 0.0714 - val_mean_absolute_error: 0.1859\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.05701\n",
      "Epoch 233/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0172 - mean_absolute_error: 0.0898 - val_loss: 0.0754 - val_mean_absolute_error: 0.1910\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.05701\n",
      "Epoch 234/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0172 - mean_absolute_error: 0.0900 - val_loss: 0.0708 - val_mean_absolute_error: 0.1879\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.05701\n",
      "Epoch 235/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0162 - mean_absolute_error: 0.0926 - val_loss: 0.0687 - val_mean_absolute_error: 0.1852\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.05701\n",
      "Epoch 236/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0156 - mean_absolute_error: 0.0861 - val_loss: 0.0797 - val_mean_absolute_error: 0.1971\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.05701\n",
      "Epoch 237/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0152 - mean_absolute_error: 0.0898 - val_loss: 0.0807 - val_mean_absolute_error: 0.1999\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.05701\n",
      "Epoch 238/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0175 - mean_absolute_error: 0.0917 - val_loss: 0.0686 - val_mean_absolute_error: 0.1847\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.05701\n",
      "Epoch 239/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0175 - mean_absolute_error: 0.0912 - val_loss: 0.0724 - val_mean_absolute_error: 0.1864\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.05701\n",
      "Epoch 240/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0173 - mean_absolute_error: 0.0919 - val_loss: 0.0643 - val_mean_absolute_error: 0.1733\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.05701\n",
      "Epoch 241/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0164 - mean_absolute_error: 0.0925 - val_loss: 0.0669 - val_mean_absolute_error: 0.1772\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.05701\n",
      "Epoch 242/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 0.0141 - mean_absolute_error: 0.0843 - val_loss: 0.0763 - val_mean_absolute_error: 0.1948\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.05701\n",
      "Epoch 243/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0165 - mean_absolute_error: 0.0899 - val_loss: 0.0790 - val_mean_absolute_error: 0.1962\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.05701\n",
      "Epoch 244/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0152 - mean_absolute_error: 0.0890 - val_loss: 0.0688 - val_mean_absolute_error: 0.1802\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.05701\n",
      "Epoch 245/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0175 - mean_absolute_error: 0.0935 - val_loss: 0.0717 - val_mean_absolute_error: 0.1876\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.05701\n",
      "Epoch 246/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0199 - mean_absolute_error: 0.1014 - val_loss: 0.0691 - val_mean_absolute_error: 0.1822\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.05701\n",
      "Epoch 247/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0182 - mean_absolute_error: 0.0962 - val_loss: 0.0785 - val_mean_absolute_error: 0.1968\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.05701\n",
      "Epoch 248/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0171 - mean_absolute_error: 0.0937 - val_loss: 0.0757 - val_mean_absolute_error: 0.1995\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.05701\n",
      "Epoch 249/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0177 - mean_absolute_error: 0.0969 - val_loss: 0.0800 - val_mean_absolute_error: 0.1886\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.05701\n",
      "Epoch 250/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0181 - mean_absolute_error: 0.0968 - val_loss: 0.0773 - val_mean_absolute_error: 0.1930\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.05701\n",
      "Epoch 251/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0148 - mean_absolute_error: 0.0858 - val_loss: 0.0788 - val_mean_absolute_error: 0.1913\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.05701\n",
      "Epoch 252/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0173 - mean_absolute_error: 0.0940 - val_loss: 0.0711 - val_mean_absolute_error: 0.1875\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.05701\n",
      "Epoch 253/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0197 - mean_absolute_error: 0.1036 - val_loss: 0.0718 - val_mean_absolute_error: 0.1863\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.05701\n",
      "Epoch 254/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0161 - mean_absolute_error: 0.0893 - val_loss: 0.0702 - val_mean_absolute_error: 0.1864\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.05701\n",
      "Epoch 255/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0150 - mean_absolute_error: 0.0880 - val_loss: 0.0749 - val_mean_absolute_error: 0.1870\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.05701\n",
      "Epoch 256/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 0.0150 - mean_absolute_error: 0.0855 - val_loss: 0.0726 - val_mean_absolute_error: 0.1900\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.05701\n",
      "Epoch 00256: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_MD = lstm_baseline(X_train, y_train)\n",
    "outputs = [layer.output for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.724535</td>\n",
       "      <td>0.641210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.966149</td>\n",
       "      <td>0.709578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.161104</td>\n",
       "      <td>0.995405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445089</td>\n",
       "      <td>0.503501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.430304</td>\n",
       "      <td>0.317687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.772315</td>\n",
       "      <td>0.719449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.918608</td>\n",
       "      <td>1.233105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.421411</td>\n",
       "      <td>0.070632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.829184</td>\n",
       "      <td>0.944061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.301577</td>\n",
       "      <td>0.266686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.720044</td>\n",
       "      <td>0.810961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.131030</td>\n",
       "      <td>1.174898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.787556</td>\n",
       "      <td>1.081434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.195228</td>\n",
       "      <td>0.788860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.600352</td>\n",
       "      <td>0.765597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.701512</td>\n",
       "      <td>1.330454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.052332</td>\n",
       "      <td>1.425608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.077584</td>\n",
       "      <td>1.006932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.492963</td>\n",
       "      <td>0.824138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.033000</td>\n",
       "      <td>0.709578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.584818</td>\n",
       "      <td>0.480839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.104976</td>\n",
       "      <td>1.124605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.388268</td>\n",
       "      <td>0.501187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.316164</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.838337</td>\n",
       "      <td>1.177606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.915255</td>\n",
       "      <td>0.458142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.160945</td>\n",
       "      <td>1.285287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.917704</td>\n",
       "      <td>1.545254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.920417</td>\n",
       "      <td>0.903649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.203415</td>\n",
       "      <td>0.274157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.199810</td>\n",
       "      <td>0.124451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.488245</td>\n",
       "      <td>0.302691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.380054</td>\n",
       "      <td>0.357273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.819899</td>\n",
       "      <td>0.952796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.518961</td>\n",
       "      <td>0.446684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.803118</td>\n",
       "      <td>0.835603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.978450</td>\n",
       "      <td>0.146218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.733761</td>\n",
       "      <td>0.625173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.085578</td>\n",
       "      <td>0.163305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.726317</td>\n",
       "      <td>0.952796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.669704</td>\n",
       "      <td>1.233105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.121522</td>\n",
       "      <td>0.787046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.062288</td>\n",
       "      <td>0.403645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.562721</td>\n",
       "      <td>0.474242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.041363</td>\n",
       "      <td>0.203704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.216395</td>\n",
       "      <td>0.410204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1.104089</td>\n",
       "      <td>0.781628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.168120</td>\n",
       "      <td>0.756833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.708411</td>\n",
       "      <td>0.887156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.497244</td>\n",
       "      <td>0.781628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.605341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.553243</td>\n",
       "      <td>0.182810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted    actual\n",
       "0     0.724535  0.641210\n",
       "1     0.966149  0.709578\n",
       "2     0.000000  0.021429\n",
       "3     1.161104  0.995405\n",
       "4     0.445089  0.503501\n",
       "5     0.430304  0.317687\n",
       "6     0.772315  0.719449\n",
       "7     0.918608  1.233105\n",
       "8     0.421411  0.070632\n",
       "9     0.829184  0.944061\n",
       "10    0.301577  0.266686\n",
       "11    0.720044  0.810961\n",
       "12    1.131030  1.174898\n",
       "13    0.000000  0.035156\n",
       "14    0.787556  1.081434\n",
       "15    1.195228  0.788860\n",
       "16    0.000000  0.101859\n",
       "17    0.600352  0.765597\n",
       "18    0.701512  1.330454\n",
       "19    0.000000  0.047753\n",
       "20    1.052332  1.425608\n",
       "21    1.077584  1.006932\n",
       "22    0.492963  0.824138\n",
       "23    1.033000  0.709578\n",
       "24    0.584818  0.480839\n",
       "25    1.104976  1.124605\n",
       "26    0.388268  0.501187\n",
       "27    1.316164  1.000000\n",
       "28    0.838337  1.177606\n",
       "29    0.915255  0.458142\n",
       "..         ...       ...\n",
       "134   1.160945  1.285287\n",
       "135   0.917704  1.545254\n",
       "136   0.000000  0.135207\n",
       "137   0.920417  0.903649\n",
       "138   0.203415  0.274157\n",
       "139   0.199810  0.124451\n",
       "140   0.488245  0.302691\n",
       "141   0.380054  0.357273\n",
       "142   0.000000  0.058345\n",
       "143   0.000000  0.084333\n",
       "144   0.819899  0.952796\n",
       "145   0.518961  0.446684\n",
       "146   0.803118  0.835603\n",
       "147   0.978450  0.146218\n",
       "148   0.000000  0.032137\n",
       "149   0.733761  0.625173\n",
       "150   0.085578  0.163305\n",
       "151   0.726317  0.952796\n",
       "152   1.669704  1.233105\n",
       "153   1.121522  0.787046\n",
       "154   0.062288  0.403645\n",
       "155   0.562721  0.474242\n",
       "156   0.041363  0.203704\n",
       "157   0.216395  0.410204\n",
       "158   1.104089  0.781628\n",
       "159   1.168120  0.756833\n",
       "160   0.708411  0.887156\n",
       "161   0.497244  0.781628\n",
       "162   0.513330  0.605341\n",
       "163   0.553243  0.182810\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_MD.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = y_pred.tolist()\n",
    "dictionary_DF = {'predicted':y_pred, 'actual':y_test}\n",
    "y_MD_DF = pd.DataFrame(dictionary_DF)\n",
    "y_MD_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    164.000000\n",
       "mean       0.198822\n",
       "std        0.181547\n",
       "min        0.001040\n",
       "25%        0.057838\n",
       "50%        0.157118\n",
       "75%        0.315746\n",
       "max        0.832233\n",
       "dtype: float64"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_MD = pd.Series(abs(y_test-y_pred))\n",
    "difference_MD.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_VFI, test_size = 0.2, random_state = 200)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/1000\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 8076.0442 - mean_absolute_error: 87.8789 - val_loss: 7290.1289 - val_mean_absolute_error: 82.4523\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7290.12886, saving model to RNN_best\n",
      "Epoch 2/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 6967.7201 - mean_absolute_error: 81.4028 - val_loss: 6107.6505 - val_mean_absolute_error: 74.9731\n",
      "\n",
      "Epoch 00002: val_loss improved from 7290.12886 to 6107.65048, saving model to RNN_best\n",
      "Epoch 3/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 5920.6131 - mean_absolute_error: 74.8351 - val_loss: 5223.3969 - val_mean_absolute_error: 69.1647\n",
      "\n",
      "Epoch 00003: val_loss improved from 6107.65048 to 5223.39685, saving model to RNN_best\n",
      "Epoch 4/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 5017.0579 - mean_absolute_error: 68.7914 - val_loss: 4371.9909 - val_mean_absolute_error: 63.2445\n",
      "\n",
      "Epoch 00004: val_loss improved from 5223.39685 to 4371.99088, saving model to RNN_best\n",
      "Epoch 5/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 4151.0622 - mean_absolute_error: 62.4027 - val_loss: 3536.4559 - val_mean_absolute_error: 56.8492\n",
      "\n",
      "Epoch 00005: val_loss improved from 4371.99088 to 3536.45590, saving model to RNN_best\n",
      "Epoch 6/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3334.1522 - mean_absolute_error: 55.7915 - val_loss: 2767.0166 - val_mean_absolute_error: 50.5086\n",
      "\n",
      "Epoch 00006: val_loss improved from 3536.45590 to 2767.01664, saving model to RNN_best\n",
      "Epoch 7/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 2522.1559 - mean_absolute_error: 48.0681 - val_loss: 2097.1848 - val_mean_absolute_error: 44.1369\n",
      "\n",
      "Epoch 00007: val_loss improved from 2767.01664 to 2097.18478, saving model to RNN_best\n",
      "Epoch 8/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 1950.8740 - mean_absolute_error: 41.9321 - val_loss: 1553.1079 - val_mean_absolute_error: 37.9780\n",
      "\n",
      "Epoch 00008: val_loss improved from 2097.18478 to 1553.10793, saving model to RNN_best\n",
      "Epoch 9/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1389.5571 - mean_absolute_error: 34.9419 - val_loss: 1141.6036 - val_mean_absolute_error: 32.3553\n",
      "\n",
      "Epoch 00009: val_loss improved from 1553.10793 to 1141.60359, saving model to RNN_best\n",
      "Epoch 10/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1044.9582 - mean_absolute_error: 29.4881 - val_loss: 858.1512 - val_mean_absolute_error: 27.5459\n",
      "\n",
      "Epoch 00010: val_loss improved from 1141.60359 to 858.15120, saving model to RNN_best\n",
      "Epoch 11/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 813.7234 - mean_absolute_error: 25.1114 - val_loss: 681.4267 - val_mean_absolute_error: 23.6847\n",
      "\n",
      "Epoch 00011: val_loss improved from 858.15120 to 681.42670, saving model to RNN_best\n",
      "Epoch 12/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 669.2591 - mean_absolute_error: 21.8258 - val_loss: 577.5345 - val_mean_absolute_error: 20.6896\n",
      "\n",
      "Epoch 00012: val_loss improved from 681.42670 to 577.53447, saving model to RNN_best\n",
      "Epoch 13/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 559.7813 - mean_absolute_error: 19.0773 - val_loss: 527.5391 - val_mean_absolute_error: 18.7363\n",
      "\n",
      "Epoch 00013: val_loss improved from 577.53447 to 527.53908, saving model to RNN_best\n",
      "Epoch 14/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 518.8632 - mean_absolute_error: 17.4403 - val_loss: 503.9737 - val_mean_absolute_error: 17.3477\n",
      "\n",
      "Epoch 00014: val_loss improved from 527.53908 to 503.97369, saving model to RNN_best\n",
      "Epoch 15/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 543.6445 - mean_absolute_error: 17.3491 - val_loss: 494.3439 - val_mean_absolute_error: 16.5048\n",
      "\n",
      "Epoch 00015: val_loss improved from 503.97369 to 494.34390, saving model to RNN_best\n",
      "Epoch 16/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 511.2487 - mean_absolute_error: 16.6763 - val_loss: 488.4045 - val_mean_absolute_error: 15.9315\n",
      "\n",
      "Epoch 00016: val_loss improved from 494.34390 to 488.40448, saving model to RNN_best\n",
      "Epoch 17/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 493.9548 - mean_absolute_error: 16.2073 - val_loss: 481.5193 - val_mean_absolute_error: 15.5973\n",
      "\n",
      "Epoch 00017: val_loss improved from 488.40448 to 481.51928, saving model to RNN_best\n",
      "Epoch 18/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 502.6211 - mean_absolute_error: 16.0715 - val_loss: 442.8642 - val_mean_absolute_error: 15.1453\n",
      "\n",
      "Epoch 00018: val_loss improved from 481.51928 to 442.86417, saving model to RNN_best\n",
      "Epoch 19/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 447.6161 - mean_absolute_error: 15.9351 - val_loss: 248.8542 - val_mean_absolute_error: 12.7021\n",
      "\n",
      "Epoch 00019: val_loss improved from 442.86417 to 248.85418, saving model to RNN_best\n",
      "Epoch 20/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 412.7230 - mean_absolute_error: 15.2008 - val_loss: 217.2010 - val_mean_absolute_error: 11.6328\n",
      "\n",
      "Epoch 00020: val_loss improved from 248.85418 to 217.20095, saving model to RNN_best\n",
      "Epoch 21/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 309.1083 - mean_absolute_error: 13.4244 - val_loss: 178.4966 - val_mean_absolute_error: 9.7676\n",
      "\n",
      "Epoch 00021: val_loss improved from 217.20095 to 178.49660, saving model to RNN_best\n",
      "Epoch 22/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 246.2788 - mean_absolute_error: 11.8666 - val_loss: 133.2603 - val_mean_absolute_error: 9.0447\n",
      "\n",
      "Epoch 00022: val_loss improved from 178.49660 to 133.26027, saving model to RNN_best\n",
      "Epoch 23/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 266.8664 - mean_absolute_error: 12.4458 - val_loss: 111.3255 - val_mean_absolute_error: 7.2875\n",
      "\n",
      "Epoch 00023: val_loss improved from 133.26027 to 111.32548, saving model to RNN_best\n",
      "Epoch 24/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 244.7641 - mean_absolute_error: 11.8055 - val_loss: 111.4370 - val_mean_absolute_error: 6.7523\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 111.32548\n",
      "Epoch 25/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 253.5944 - mean_absolute_error: 12.0237 - val_loss: 100.3543 - val_mean_absolute_error: 6.2115\n",
      "\n",
      "Epoch 00025: val_loss improved from 111.32548 to 100.35432, saving model to RNN_best\n",
      "Epoch 26/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 234.9955 - mean_absolute_error: 11.5392 - val_loss: 100.1965 - val_mean_absolute_error: 6.5811\n",
      "\n",
      "Epoch 00026: val_loss improved from 100.35432 to 100.19647, saving model to RNN_best\n",
      "Epoch 27/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 252.5873 - mean_absolute_error: 12.1528 - val_loss: 94.9631 - val_mean_absolute_error: 5.5714\n",
      "\n",
      "Epoch 00027: val_loss improved from 100.19647 to 94.96309, saving model to RNN_best\n",
      "Epoch 28/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 249.0679 - mean_absolute_error: 12.0628 - val_loss: 89.5192 - val_mean_absolute_error: 5.5952\n",
      "\n",
      "Epoch 00028: val_loss improved from 94.96309 to 89.51921, saving model to RNN_best\n",
      "Epoch 29/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 216.3645 - mean_absolute_error: 11.3847 - val_loss: 86.1903 - val_mean_absolute_error: 5.7876\n",
      "\n",
      "Epoch 00029: val_loss improved from 89.51921 to 86.19032, saving model to RNN_best\n",
      "Epoch 30/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 220.3549 - mean_absolute_error: 11.3732 - val_loss: 96.8752 - val_mean_absolute_error: 5.2466\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 86.19032\n",
      "Epoch 31/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 253.3360 - mean_absolute_error: 12.1167 - val_loss: 98.5294 - val_mean_absolute_error: 6.1420\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 86.19032\n",
      "Epoch 32/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 238.5803 - mean_absolute_error: 11.7092 - val_loss: 86.2441 - val_mean_absolute_error: 5.4391\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 86.19032\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 205.5234 - mean_absolute_error: 11.0469 - val_loss: 83.4170 - val_mean_absolute_error: 6.0130\n",
      "\n",
      "Epoch 00033: val_loss improved from 86.19032 to 83.41697, saving model to RNN_best\n",
      "Epoch 34/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 196.4889 - mean_absolute_error: 10.7550 - val_loss: 95.9889 - val_mean_absolute_error: 7.1757\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 83.41697\n",
      "Epoch 35/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 221.7964 - mean_absolute_error: 11.6863 - val_loss: 87.1457 - val_mean_absolute_error: 5.7316\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 83.41697\n",
      "Epoch 36/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 225.5998 - mean_absolute_error: 11.5937 - val_loss: 82.5609 - val_mean_absolute_error: 6.1756\n",
      "\n",
      "Epoch 00036: val_loss improved from 83.41697 to 82.56086, saving model to RNN_best\n",
      "Epoch 37/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 250.0563 - mean_absolute_error: 11.9367 - val_loss: 82.1379 - val_mean_absolute_error: 5.2477\n",
      "\n",
      "Epoch 00037: val_loss improved from 82.56086 to 82.13792, saving model to RNN_best\n",
      "Epoch 38/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 229.7028 - mean_absolute_error: 11.6851 - val_loss: 81.9094 - val_mean_absolute_error: 5.2043\n",
      "\n",
      "Epoch 00038: val_loss improved from 82.13792 to 81.90939, saving model to RNN_best\n",
      "Epoch 39/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 234.0704 - mean_absolute_error: 11.4408 - val_loss: 113.4308 - val_mean_absolute_error: 8.2515\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 81.90939\n",
      "Epoch 40/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 230.8342 - mean_absolute_error: 11.3531 - val_loss: 89.8939 - val_mean_absolute_error: 5.7305\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 81.90939\n",
      "Epoch 41/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.3285 - mean_absolute_error: 10.9359 - val_loss: 76.8368 - val_mean_absolute_error: 4.7957\n",
      "\n",
      "Epoch 00041: val_loss improved from 81.90939 to 76.83675, saving model to RNN_best\n",
      "Epoch 42/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 216.0338 - mean_absolute_error: 11.0999 - val_loss: 76.6085 - val_mean_absolute_error: 4.9972\n",
      "\n",
      "Epoch 00042: val_loss improved from 76.83675 to 76.60853, saving model to RNN_best\n",
      "Epoch 43/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 229.2544 - mean_absolute_error: 11.4605 - val_loss: 86.3749 - val_mean_absolute_error: 5.7371\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 76.60853\n",
      "Epoch 44/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 204.6372 - mean_absolute_error: 11.1979 - val_loss: 70.7170 - val_mean_absolute_error: 4.6128\n",
      "\n",
      "Epoch 00044: val_loss improved from 76.60853 to 70.71704, saving model to RNN_best\n",
      "Epoch 45/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 233.1938 - mean_absolute_error: 11.8385 - val_loss: 76.5046 - val_mean_absolute_error: 4.3411\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 70.71704\n",
      "Epoch 46/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 206.5310 - mean_absolute_error: 10.9346 - val_loss: 83.3949 - val_mean_absolute_error: 5.4105\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 70.71704\n",
      "Epoch 47/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 226.3231 - mean_absolute_error: 11.7784 - val_loss: 86.2849 - val_mean_absolute_error: 4.5169\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 70.71704\n",
      "Epoch 48/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 234.9713 - mean_absolute_error: 12.0102 - val_loss: 79.9507 - val_mean_absolute_error: 5.2995\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 70.71704\n",
      "Epoch 49/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 257.4951 - mean_absolute_error: 12.0807 - val_loss: 82.4082 - val_mean_absolute_error: 6.0834\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 70.71704\n",
      "Epoch 50/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 190.6366 - mean_absolute_error: 10.7217 - val_loss: 82.9660 - val_mean_absolute_error: 5.8837\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 70.71704\n",
      "Epoch 51/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 219.6977 - mean_absolute_error: 11.0274 - val_loss: 67.9491 - val_mean_absolute_error: 4.6870\n",
      "\n",
      "Epoch 00051: val_loss improved from 70.71704 to 67.94906, saving model to RNN_best\n",
      "Epoch 52/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 209.4104 - mean_absolute_error: 10.9841 - val_loss: 75.1901 - val_mean_absolute_error: 5.4278\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 67.94906\n",
      "Epoch 53/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 217.2839 - mean_absolute_error: 11.0443 - val_loss: 81.4034 - val_mean_absolute_error: 4.9685\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 67.94906\n",
      "Epoch 54/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 236.0876 - mean_absolute_error: 11.5678 - val_loss: 76.9920 - val_mean_absolute_error: 5.1051\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 67.94906\n",
      "Epoch 55/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 214.7760 - mean_absolute_error: 11.0228 - val_loss: 71.6299 - val_mean_absolute_error: 4.7435\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 67.94906\n",
      "Epoch 56/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 215.5594 - mean_absolute_error: 11.3830 - val_loss: 72.3237 - val_mean_absolute_error: 4.2609\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 67.94906\n",
      "Epoch 57/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 196.7237 - mean_absolute_error: 10.8994 - val_loss: 76.3900 - val_mean_absolute_error: 5.7896\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 67.94906\n",
      "Epoch 58/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 235.9523 - mean_absolute_error: 11.4476 - val_loss: 79.4346 - val_mean_absolute_error: 5.4789\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 67.94906\n",
      "Epoch 59/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.8362 - mean_absolute_error: 11.1856 - val_loss: 71.0283 - val_mean_absolute_error: 5.2268\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 67.94906\n",
      "Epoch 60/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 207.7227 - mean_absolute_error: 11.1636 - val_loss: 69.0638 - val_mean_absolute_error: 4.0026\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 67.94906\n",
      "Epoch 61/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.6725 - mean_absolute_error: 10.9089 - val_loss: 77.8784 - val_mean_absolute_error: 5.8800\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 67.94906\n",
      "Epoch 62/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 213.8774 - mean_absolute_error: 11.2338 - val_loss: 82.4839 - val_mean_absolute_error: 5.1803\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 67.94906\n",
      "Epoch 63/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 239.6912 - mean_absolute_error: 12.0833 - val_loss: 67.3417 - val_mean_absolute_error: 4.0747\n",
      "\n",
      "Epoch 00063: val_loss improved from 67.94906 to 67.34165, saving model to RNN_best\n",
      "Epoch 64/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 226.8480 - mean_absolute_error: 11.7496 - val_loss: 74.3606 - val_mean_absolute_error: 5.7170\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 67.34165\n",
      "Epoch 65/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 215.3343 - mean_absolute_error: 11.0036 - val_loss: 76.8120 - val_mean_absolute_error: 5.8752\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 67.34165\n",
      "Epoch 66/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 222.6482 - mean_absolute_error: 11.4752 - val_loss: 76.7005 - val_mean_absolute_error: 5.3701\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 67.34165\n",
      "Epoch 67/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 214.3523 - mean_absolute_error: 11.3781 - val_loss: 72.6870 - val_mean_absolute_error: 4.2308\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 67.34165\n",
      "Epoch 68/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.1096 - mean_absolute_error: 10.7782 - val_loss: 76.7405 - val_mean_absolute_error: 5.7568\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 67.34165\n",
      "Epoch 69/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 234.0582 - mean_absolute_error: 11.9054 - val_loss: 71.6443 - val_mean_absolute_error: 5.5130\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 67.34165\n",
      "Epoch 70/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.4383 - mean_absolute_error: 10.4799 - val_loss: 93.9101 - val_mean_absolute_error: 7.5195\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 67.34165\n",
      "Epoch 71/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 196.9593 - mean_absolute_error: 10.8217 - val_loss: 85.4646 - val_mean_absolute_error: 6.7563\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 67.34165\n",
      "Epoch 72/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 223.0652 - mean_absolute_error: 11.4049 - val_loss: 85.8474 - val_mean_absolute_error: 6.8286\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 67.34165\n",
      "Epoch 73/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 216.6146 - mean_absolute_error: 10.9870 - val_loss: 112.5576 - val_mean_absolute_error: 8.7195\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 67.34165\n",
      "Epoch 74/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 194.6770 - mean_absolute_error: 10.8571 - val_loss: 78.5379 - val_mean_absolute_error: 6.0933\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 67.34165\n",
      "Epoch 75/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 195.6905 - mean_absolute_error: 10.6795 - val_loss: 65.3389 - val_mean_absolute_error: 4.4236\n",
      "\n",
      "Epoch 00075: val_loss improved from 67.34165 to 65.33891, saving model to RNN_best\n",
      "Epoch 76/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 196.8047 - mean_absolute_error: 10.6917 - val_loss: 71.7095 - val_mean_absolute_error: 4.2196\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 65.33891\n",
      "Epoch 77/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 223.6484 - mean_absolute_error: 11.4445 - val_loss: 78.4616 - val_mean_absolute_error: 5.6810\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 65.33891\n",
      "Epoch 78/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 183.0641 - mean_absolute_error: 10.2315 - val_loss: 76.5794 - val_mean_absolute_error: 5.8804\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 65.33891\n",
      "Epoch 79/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 192.6103 - mean_absolute_error: 10.6549 - val_loss: 79.4015 - val_mean_absolute_error: 6.0381\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 65.33891\n",
      "Epoch 80/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.3097 - mean_absolute_error: 10.3505 - val_loss: 66.1082 - val_mean_absolute_error: 4.6156\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 65.33891\n",
      "Epoch 81/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 204.9488 - mean_absolute_error: 10.9845 - val_loss: 65.7638 - val_mean_absolute_error: 4.4499\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 65.33891\n",
      "Epoch 82/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.1751 - mean_absolute_error: 10.4303 - val_loss: 71.3842 - val_mean_absolute_error: 5.5680\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 65.33891\n",
      "Epoch 83/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 204.3578 - mean_absolute_error: 10.8296 - val_loss: 64.8966 - val_mean_absolute_error: 4.5600\n",
      "\n",
      "Epoch 00083: val_loss improved from 65.33891 to 64.89659, saving model to RNN_best\n",
      "Epoch 84/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 212.1385 - mean_absolute_error: 11.1336 - val_loss: 69.3741 - val_mean_absolute_error: 4.8948\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 64.89659\n",
      "Epoch 85/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.2559 - mean_absolute_error: 10.4360 - val_loss: 68.8997 - val_mean_absolute_error: 4.7400\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 64.89659\n",
      "Epoch 86/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.8736 - mean_absolute_error: 10.3573 - val_loss: 61.0426 - val_mean_absolute_error: 3.9533\n",
      "\n",
      "Epoch 00086: val_loss improved from 64.89659 to 61.04262, saving model to RNN_best\n",
      "Epoch 87/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.6989 - mean_absolute_error: 10.7095 - val_loss: 64.0155 - val_mean_absolute_error: 4.5200\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 61.04262\n",
      "Epoch 88/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.3846 - mean_absolute_error: 11.2129 - val_loss: 100.9227 - val_mean_absolute_error: 7.5475\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 61.04262\n",
      "Epoch 89/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.8237 - mean_absolute_error: 11.2712 - val_loss: 71.1203 - val_mean_absolute_error: 4.9054\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 61.04262\n",
      "Epoch 90/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 226.4312 - mean_absolute_error: 11.5778 - val_loss: 62.1765 - val_mean_absolute_error: 4.1168\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 61.04262\n",
      "Epoch 91/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.5695 - mean_absolute_error: 10.5954 - val_loss: 61.5528 - val_mean_absolute_error: 4.3649\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 61.04262\n",
      "Epoch 92/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 221.1935 - mean_absolute_error: 11.0554 - val_loss: 67.9156 - val_mean_absolute_error: 5.3153\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 61.04262\n",
      "Epoch 93/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 184.9259 - mean_absolute_error: 10.5429 - val_loss: 61.9060 - val_mean_absolute_error: 4.5130\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 61.04262\n",
      "Epoch 94/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 186.8004 - mean_absolute_error: 10.4482 - val_loss: 63.2167 - val_mean_absolute_error: 5.1013\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 61.04262\n",
      "Epoch 95/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 190.0334 - mean_absolute_error: 10.6432 - val_loss: 60.6558 - val_mean_absolute_error: 4.5553\n",
      "\n",
      "Epoch 00095: val_loss improved from 61.04262 to 60.65584, saving model to RNN_best\n",
      "Epoch 96/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 204.5383 - mean_absolute_error: 11.1954 - val_loss: 75.4838 - val_mean_absolute_error: 6.2660\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 60.65584\n",
      "Epoch 97/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 194.9401 - mean_absolute_error: 10.5631 - val_loss: 63.4360 - val_mean_absolute_error: 5.0779\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 60.65584\n",
      "Epoch 98/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 195.6592 - mean_absolute_error: 10.8247 - val_loss: 57.2528 - val_mean_absolute_error: 4.4867\n",
      "\n",
      "Epoch 00098: val_loss improved from 60.65584 to 57.25278, saving model to RNN_best\n",
      "Epoch 99/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 186.9322 - mean_absolute_error: 10.5740 - val_loss: 103.7989 - val_mean_absolute_error: 8.0154\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 57.25278\n",
      "Epoch 100/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 218.2366 - mean_absolute_error: 11.4385 - val_loss: 64.3614 - val_mean_absolute_error: 4.7414\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 57.25278\n",
      "Epoch 101/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 185.3324 - mean_absolute_error: 10.5633 - val_loss: 64.6540 - val_mean_absolute_error: 4.2459\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 57.25278\n",
      "Epoch 102/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.0191 - mean_absolute_error: 11.2127 - val_loss: 59.3359 - val_mean_absolute_error: 4.5925\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 57.25278\n",
      "Epoch 103/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 198.8171 - mean_absolute_error: 10.9394 - val_loss: 65.7060 - val_mean_absolute_error: 5.4057\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 57.25278\n",
      "Epoch 104/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 209.7478 - mean_absolute_error: 11.1742 - val_loss: 57.5178 - val_mean_absolute_error: 4.4102\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 57.25278\n",
      "Epoch 105/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 199.1259 - mean_absolute_error: 11.1234 - val_loss: 54.1409 - val_mean_absolute_error: 4.0095\n",
      "\n",
      "Epoch 00105: val_loss improved from 57.25278 to 54.14091, saving model to RNN_best\n",
      "Epoch 106/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.0694 - mean_absolute_error: 10.6244 - val_loss: 59.5047 - val_mean_absolute_error: 4.6907\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 54.14091\n",
      "Epoch 107/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 211.2797 - mean_absolute_error: 11.1987 - val_loss: 60.9771 - val_mean_absolute_error: 5.3088\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 54.14091\n",
      "Epoch 108/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 187.2452 - mean_absolute_error: 10.4288 - val_loss: 62.5528 - val_mean_absolute_error: 5.3818\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 54.14091\n",
      "Epoch 109/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 189.2961 - mean_absolute_error: 10.7758 - val_loss: 56.5597 - val_mean_absolute_error: 3.6195\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 54.14091\n",
      "Epoch 110/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.4974 - mean_absolute_error: 11.1236 - val_loss: 62.4231 - val_mean_absolute_error: 3.7602\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 54.14091\n",
      "Epoch 111/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 200.8123 - mean_absolute_error: 10.8750 - val_loss: 61.2050 - val_mean_absolute_error: 4.9975\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 54.14091\n",
      "Epoch 112/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.3036 - mean_absolute_error: 10.9263 - val_loss: 56.3011 - val_mean_absolute_error: 4.4908\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 54.14091\n",
      "Epoch 113/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.2292 - mean_absolute_error: 10.4006 - val_loss: 54.8173 - val_mean_absolute_error: 4.0630\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 54.14091\n",
      "Epoch 114/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 181.7122 - mean_absolute_error: 10.4263 - val_loss: 58.8758 - val_mean_absolute_error: 5.0674\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 54.14091\n",
      "Epoch 115/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 178.8550 - mean_absolute_error: 10.3504 - val_loss: 60.0306 - val_mean_absolute_error: 5.5361\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 54.14091\n",
      "Epoch 116/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 197.7736 - mean_absolute_error: 10.6610 - val_loss: 75.2345 - val_mean_absolute_error: 5.9781\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 54.14091\n",
      "Epoch 117/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 210.1225 - mean_absolute_error: 11.3316 - val_loss: 60.5563 - val_mean_absolute_error: 4.5558\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 54.14091\n",
      "Epoch 118/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 209.8934 - mean_absolute_error: 11.3177 - val_loss: 59.8615 - val_mean_absolute_error: 5.2207\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 54.14091\n",
      "Epoch 119/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 164.0995 - mean_absolute_error: 9.7689 - val_loss: 53.2373 - val_mean_absolute_error: 4.4665\n",
      "\n",
      "Epoch 00119: val_loss improved from 54.14091 to 53.23735, saving model to RNN_best\n",
      "Epoch 120/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 183.3360 - mean_absolute_error: 10.8132 - val_loss: 58.9834 - val_mean_absolute_error: 4.8916\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 53.23735\n",
      "Epoch 121/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 190.3215 - mean_absolute_error: 10.7133 - val_loss: 61.8779 - val_mean_absolute_error: 5.3993\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 53.23735\n",
      "Epoch 122/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 173.2085 - mean_absolute_error: 10.1285 - val_loss: 57.2238 - val_mean_absolute_error: 4.6677\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 53.23735\n",
      "Epoch 123/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 183.7100 - mean_absolute_error: 10.4976 - val_loss: 65.2000 - val_mean_absolute_error: 5.6374\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 53.23735\n",
      "Epoch 124/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.5474 - mean_absolute_error: 10.5881 - val_loss: 52.4873 - val_mean_absolute_error: 4.4761\n",
      "\n",
      "Epoch 00124: val_loss improved from 53.23735 to 52.48730, saving model to RNN_best\n",
      "Epoch 125/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 208.0180 - mean_absolute_error: 11.1964 - val_loss: 53.1138 - val_mean_absolute_error: 4.6944\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 52.48730\n",
      "Epoch 126/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 199.2564 - mean_absolute_error: 11.0202 - val_loss: 51.6965 - val_mean_absolute_error: 4.3754\n",
      "\n",
      "Epoch 00126: val_loss improved from 52.48730 to 51.69653, saving model to RNN_best\n",
      "Epoch 127/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 196.6685 - mean_absolute_error: 10.7613 - val_loss: 49.7052 - val_mean_absolute_error: 4.6709\n",
      "\n",
      "Epoch 00127: val_loss improved from 51.69653 to 49.70519, saving model to RNN_best\n",
      "Epoch 128/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.7582 - mean_absolute_error: 10.3775 - val_loss: 53.9483 - val_mean_absolute_error: 5.2450\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 49.70519\n",
      "Epoch 129/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.7287 - mean_absolute_error: 10.7055 - val_loss: 44.6522 - val_mean_absolute_error: 3.9853\n",
      "\n",
      "Epoch 00129: val_loss improved from 49.70519 to 44.65219, saving model to RNN_best\n",
      "Epoch 130/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.6824 - mean_absolute_error: 10.7570 - val_loss: 47.1587 - val_mean_absolute_error: 4.2034\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 44.65219\n",
      "Epoch 131/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.3883 - mean_absolute_error: 10.0204 - val_loss: 43.2950 - val_mean_absolute_error: 3.8842\n",
      "\n",
      "Epoch 00131: val_loss improved from 44.65219 to 43.29503, saving model to RNN_best\n",
      "Epoch 132/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 206.2164 - mean_absolute_error: 11.1812 - val_loss: 41.7503 - val_mean_absolute_error: 3.8599\n",
      "\n",
      "Epoch 00132: val_loss improved from 43.29503 to 41.75031, saving model to RNN_best\n",
      "Epoch 133/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.8751 - mean_absolute_error: 10.7523 - val_loss: 50.6980 - val_mean_absolute_error: 4.4364\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 41.75031\n",
      "Epoch 134/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 183.0141 - mean_absolute_error: 10.7045 - val_loss: 44.9173 - val_mean_absolute_error: 4.1503\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 41.75031\n",
      "Epoch 135/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 187.8250 - mean_absolute_error: 10.6509 - val_loss: 39.1346 - val_mean_absolute_error: 3.5065\n",
      "\n",
      "Epoch 00135: val_loss improved from 41.75031 to 39.13462, saving model to RNN_best\n",
      "Epoch 136/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.6641 - mean_absolute_error: 9.5827 - val_loss: 44.9835 - val_mean_absolute_error: 4.4108\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 39.13462\n",
      "Epoch 137/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.3614 - mean_absolute_error: 10.1536 - val_loss: 36.6149 - val_mean_absolute_error: 3.6816\n",
      "\n",
      "Epoch 00137: val_loss improved from 39.13462 to 36.61491, saving model to RNN_best\n",
      "Epoch 138/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 179.5012 - mean_absolute_error: 10.6816 - val_loss: 37.1019 - val_mean_absolute_error: 3.4150\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 36.61491\n",
      "Epoch 139/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 169.6750 - mean_absolute_error: 10.3290 - val_loss: 39.5162 - val_mean_absolute_error: 4.0314\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 36.61491\n",
      "Epoch 140/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 172.4994 - mean_absolute_error: 10.1568 - val_loss: 37.5151 - val_mean_absolute_error: 3.8457\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 36.61491\n",
      "Epoch 141/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.4104 - mean_absolute_error: 10.4139 - val_loss: 74.0640 - val_mean_absolute_error: 6.1070\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 36.61491\n",
      "Epoch 142/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 201.5496 - mean_absolute_error: 10.7357 - val_loss: 73.2422 - val_mean_absolute_error: 6.2928\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 36.61491\n",
      "Epoch 143/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.0284 - mean_absolute_error: 10.1719 - val_loss: 48.8414 - val_mean_absolute_error: 3.8407\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 36.61491\n",
      "Epoch 144/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 186.4778 - mean_absolute_error: 10.4460 - val_loss: 43.4180 - val_mean_absolute_error: 3.3724\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 36.61491\n",
      "Epoch 145/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.6364 - mean_absolute_error: 10.3551 - val_loss: 45.1487 - val_mean_absolute_error: 4.2208\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 36.61491\n",
      "Epoch 146/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 159.5880 - mean_absolute_error: 9.9545 - val_loss: 46.4434 - val_mean_absolute_error: 4.0406\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 36.61491\n",
      "Epoch 147/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 163.2784 - mean_absolute_error: 9.9944 - val_loss: 38.7772 - val_mean_absolute_error: 3.4812\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 36.61491\n",
      "Epoch 148/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 163.2464 - mean_absolute_error: 9.9577 - val_loss: 36.4471 - val_mean_absolute_error: 3.8310\n",
      "\n",
      "Epoch 00148: val_loss improved from 36.61491 to 36.44714, saving model to RNN_best\n",
      "Epoch 149/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 177.8218 - mean_absolute_error: 10.3013 - val_loss: 39.7469 - val_mean_absolute_error: 4.0485\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 36.44714\n",
      "Epoch 150/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 168.7762 - mean_absolute_error: 10.3058 - val_loss: 39.6044 - val_mean_absolute_error: 4.0238\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 36.44714\n",
      "Epoch 151/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 184.9892 - mean_absolute_error: 10.5819 - val_loss: 33.9653 - val_mean_absolute_error: 3.5110\n",
      "\n",
      "Epoch 00151: val_loss improved from 36.44714 to 33.96526, saving model to RNN_best\n",
      "Epoch 152/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.0244 - mean_absolute_error: 10.0869 - val_loss: 35.1532 - val_mean_absolute_error: 3.8362\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 33.96526\n",
      "Epoch 153/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.7877 - mean_absolute_error: 10.4583 - val_loss: 34.6415 - val_mean_absolute_error: 3.7692\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 33.96526\n",
      "Epoch 154/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.9369 - mean_absolute_error: 10.2619 - val_loss: 39.8386 - val_mean_absolute_error: 4.4508\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 33.96526\n",
      "Epoch 155/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.6151 - mean_absolute_error: 9.8410 - val_loss: 32.2832 - val_mean_absolute_error: 3.8446\n",
      "\n",
      "Epoch 00155: val_loss improved from 33.96526 to 32.28324, saving model to RNN_best\n",
      "Epoch 156/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 166.3662 - mean_absolute_error: 10.2122 - val_loss: 31.9533 - val_mean_absolute_error: 3.7958\n",
      "\n",
      "Epoch 00156: val_loss improved from 32.28324 to 31.95331, saving model to RNN_best\n",
      "Epoch 157/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 183.4401 - mean_absolute_error: 10.6085 - val_loss: 34.7497 - val_mean_absolute_error: 3.8292\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 31.95331\n",
      "Epoch 158/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 180.0963 - mean_absolute_error: 10.5474 - val_loss: 34.0424 - val_mean_absolute_error: 3.3748\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 31.95331\n",
      "Epoch 159/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 172.4927 - mean_absolute_error: 10.2439 - val_loss: 44.3822 - val_mean_absolute_error: 4.8928\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 31.95331\n",
      "Epoch 160/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.5399 - mean_absolute_error: 10.4496 - val_loss: 43.1042 - val_mean_absolute_error: 4.6088\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 31.95331\n",
      "Epoch 161/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.2803 - mean_absolute_error: 9.9950 - val_loss: 30.8533 - val_mean_absolute_error: 3.5564\n",
      "\n",
      "Epoch 00161: val_loss improved from 31.95331 to 30.85326, saving model to RNN_best\n",
      "Epoch 162/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.0988 - mean_absolute_error: 9.8510 - val_loss: 31.2193 - val_mean_absolute_error: 3.5060\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 30.85326\n",
      "Epoch 163/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.8612 - mean_absolute_error: 9.7821 - val_loss: 37.6307 - val_mean_absolute_error: 4.6290\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 30.85326\n",
      "Epoch 164/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 158.7975 - mean_absolute_error: 10.1572 - val_loss: 32.7336 - val_mean_absolute_error: 4.0948\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 30.85326\n",
      "Epoch 165/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.5092 - mean_absolute_error: 10.0233 - val_loss: 30.3232 - val_mean_absolute_error: 3.3973\n",
      "\n",
      "Epoch 00165: val_loss improved from 30.85326 to 30.32321, saving model to RNN_best\n",
      "Epoch 166/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.2959 - mean_absolute_error: 10.0543 - val_loss: 31.4713 - val_mean_absolute_error: 3.8951\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 30.32321\n",
      "Epoch 167/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.0622 - mean_absolute_error: 9.6584 - val_loss: 32.8523 - val_mean_absolute_error: 3.7912\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 30.32321\n",
      "Epoch 168/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 173.5899 - mean_absolute_error: 10.4365 - val_loss: 30.7873 - val_mean_absolute_error: 3.2238\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 30.32321\n",
      "Epoch 169/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.4782 - mean_absolute_error: 9.8179 - val_loss: 27.8939 - val_mean_absolute_error: 3.5544\n",
      "\n",
      "Epoch 00169: val_loss improved from 30.32321 to 27.89389, saving model to RNN_best\n",
      "Epoch 170/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.4967 - mean_absolute_error: 10.0524 - val_loss: 32.7388 - val_mean_absolute_error: 3.8449\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 27.89389\n",
      "Epoch 171/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 180.6187 - mean_absolute_error: 10.5084 - val_loss: 33.1666 - val_mean_absolute_error: 3.3080\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 27.89389\n",
      "Epoch 172/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 153.0277 - mean_absolute_error: 9.8205 - val_loss: 32.3673 - val_mean_absolute_error: 3.7518\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 27.89389\n",
      "Epoch 173/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.5942 - mean_absolute_error: 10.0128 - val_loss: 37.9462 - val_mean_absolute_error: 4.5411\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 27.89389\n",
      "Epoch 174/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.2120 - mean_absolute_error: 9.9441 - val_loss: 37.4115 - val_mean_absolute_error: 4.3274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00174: val_loss did not improve from 27.89389\n",
      "Epoch 175/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.3808 - mean_absolute_error: 9.6029 - val_loss: 34.7233 - val_mean_absolute_error: 4.1013\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 27.89389\n",
      "Epoch 176/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.3004 - mean_absolute_error: 9.6941 - val_loss: 27.4927 - val_mean_absolute_error: 3.1374\n",
      "\n",
      "Epoch 00176: val_loss improved from 27.89389 to 27.49274, saving model to RNN_best\n",
      "Epoch 177/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.9802 - mean_absolute_error: 10.2968 - val_loss: 31.1727 - val_mean_absolute_error: 3.9762\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 27.49274\n",
      "Epoch 178/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 162.7848 - mean_absolute_error: 10.1792 - val_loss: 35.7255 - val_mean_absolute_error: 4.2476\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 27.49274\n",
      "Epoch 179/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.2538 - mean_absolute_error: 9.5360 - val_loss: 30.9347 - val_mean_absolute_error: 4.0400\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 27.49274\n",
      "Epoch 180/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.3105 - mean_absolute_error: 10.0745 - val_loss: 66.1962 - val_mean_absolute_error: 5.7066\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 27.49274\n",
      "Epoch 181/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 203.2676 - mean_absolute_error: 11.1026 - val_loss: 74.2761 - val_mean_absolute_error: 5.3682\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 27.49274\n",
      "Epoch 182/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 188.2313 - mean_absolute_error: 10.2195 - val_loss: 51.5651 - val_mean_absolute_error: 4.3524\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 27.49274\n",
      "Epoch 183/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 194.3617 - mean_absolute_error: 10.5660 - val_loss: 55.8963 - val_mean_absolute_error: 5.7517\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 27.49274\n",
      "Epoch 184/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 171.5062 - mean_absolute_error: 10.1886 - val_loss: 38.9347 - val_mean_absolute_error: 3.7123\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 27.49274\n",
      "Epoch 185/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 185.4548 - mean_absolute_error: 10.3876 - val_loss: 33.4397 - val_mean_absolute_error: 3.4172\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 27.49274\n",
      "Epoch 186/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.2849 - mean_absolute_error: 10.0095 - val_loss: 37.9835 - val_mean_absolute_error: 4.4083\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 27.49274\n",
      "Epoch 187/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 182.0001 - mean_absolute_error: 10.6425 - val_loss: 53.0224 - val_mean_absolute_error: 5.1157\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 27.49274\n",
      "Epoch 188/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 182.9136 - mean_absolute_error: 10.3709 - val_loss: 52.3523 - val_mean_absolute_error: 5.1290\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 27.49274\n",
      "Epoch 189/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 164.7287 - mean_absolute_error: 9.8237 - val_loss: 36.7854 - val_mean_absolute_error: 3.5557\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 27.49274\n",
      "Epoch 190/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.4284 - mean_absolute_error: 10.3787 - val_loss: 35.6606 - val_mean_absolute_error: 4.2393\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 27.49274\n",
      "Epoch 191/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.0319 - mean_absolute_error: 9.7308 - val_loss: 42.7916 - val_mean_absolute_error: 5.1874\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 27.49274\n",
      "Epoch 192/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 166.3630 - mean_absolute_error: 10.0858 - val_loss: 37.5426 - val_mean_absolute_error: 4.2598\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 27.49274\n",
      "Epoch 193/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 174.3973 - mean_absolute_error: 10.3723 - val_loss: 84.9301 - val_mean_absolute_error: 7.4947\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 27.49274\n",
      "Epoch 194/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 205.2075 - mean_absolute_error: 10.7821 - val_loss: 38.1647 - val_mean_absolute_error: 4.0220\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 27.49274\n",
      "Epoch 195/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 157.6065 - mean_absolute_error: 10.0388 - val_loss: 35.8473 - val_mean_absolute_error: 4.1100\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 27.49274\n",
      "Epoch 196/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 165.9852 - mean_absolute_error: 10.3820 - val_loss: 28.6060 - val_mean_absolute_error: 3.6058\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 27.49274\n",
      "Epoch 197/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.9423 - mean_absolute_error: 10.3538 - val_loss: 30.8344 - val_mean_absolute_error: 4.1022\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 27.49274\n",
      "Epoch 198/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 137.2545 - mean_absolute_error: 9.2411 - val_loss: 38.4928 - val_mean_absolute_error: 4.4587\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 27.49274\n",
      "Epoch 199/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.2431 - mean_absolute_error: 10.1786 - val_loss: 26.8164 - val_mean_absolute_error: 3.0779\n",
      "\n",
      "Epoch 00199: val_loss improved from 27.49274 to 26.81637, saving model to RNN_best\n",
      "Epoch 200/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.7518 - mean_absolute_error: 9.6666 - val_loss: 31.8632 - val_mean_absolute_error: 3.3956\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 26.81637\n",
      "Epoch 201/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.1157 - mean_absolute_error: 9.8957 - val_loss: 48.8378 - val_mean_absolute_error: 5.1932\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 26.81637\n",
      "Epoch 202/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.5504 - mean_absolute_error: 10.0351 - val_loss: 30.4255 - val_mean_absolute_error: 3.6716\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 26.81637\n",
      "Epoch 203/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 154.0747 - mean_absolute_error: 9.8737 - val_loss: 37.5607 - val_mean_absolute_error: 4.4591\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 26.81637\n",
      "Epoch 204/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.8075 - mean_absolute_error: 10.0489 - val_loss: 38.2956 - val_mean_absolute_error: 4.5474\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 26.81637\n",
      "Epoch 205/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.1414 - mean_absolute_error: 9.6332 - val_loss: 32.6671 - val_mean_absolute_error: 4.2990\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 26.81637\n",
      "Epoch 206/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.1310 - mean_absolute_error: 9.5962 - val_loss: 31.7797 - val_mean_absolute_error: 4.0912\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 26.81637\n",
      "Epoch 207/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.1582 - mean_absolute_error: 9.9646 - val_loss: 32.3909 - val_mean_absolute_error: 3.4581\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 26.81637\n",
      "Epoch 208/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.3681 - mean_absolute_error: 10.2716 - val_loss: 26.2243 - val_mean_absolute_error: 3.1999\n",
      "\n",
      "Epoch 00208: val_loss improved from 26.81637 to 26.22433, saving model to RNN_best\n",
      "Epoch 209/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 157.9282 - mean_absolute_error: 10.0936 - val_loss: 41.1789 - val_mean_absolute_error: 4.8474\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 26.22433\n",
      "Epoch 210/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.7198 - mean_absolute_error: 9.8162 - val_loss: 32.3600 - val_mean_absolute_error: 4.2536\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 26.22433\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 167.5632 - mean_absolute_error: 10.1528 - val_loss: 26.3554 - val_mean_absolute_error: 3.1101\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 26.22433\n",
      "Epoch 212/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 151.6852 - mean_absolute_error: 9.5654 - val_loss: 27.1558 - val_mean_absolute_error: 3.5397\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 26.22433\n",
      "Epoch 213/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.7808 - mean_absolute_error: 10.0044 - val_loss: 40.6740 - val_mean_absolute_error: 4.3912\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 26.22433\n",
      "Epoch 214/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.5428 - mean_absolute_error: 10.1113 - val_loss: 22.5089 - val_mean_absolute_error: 2.9736\n",
      "\n",
      "Epoch 00214: val_loss improved from 26.22433 to 22.50893, saving model to RNN_best\n",
      "Epoch 215/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.5240 - mean_absolute_error: 9.5091 - val_loss: 28.4345 - val_mean_absolute_error: 4.1124\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 22.50893\n",
      "Epoch 216/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.2373 - mean_absolute_error: 9.8060 - val_loss: 31.4269 - val_mean_absolute_error: 4.0113\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 22.50893\n",
      "Epoch 217/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.0456 - mean_absolute_error: 10.1489 - val_loss: 26.8718 - val_mean_absolute_error: 3.7334\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 22.50893\n",
      "Epoch 218/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.5919 - mean_absolute_error: 10.1318 - val_loss: 26.3563 - val_mean_absolute_error: 3.6806\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 22.50893\n",
      "Epoch 219/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.8664 - mean_absolute_error: 9.3232 - val_loss: 29.4944 - val_mean_absolute_error: 3.7496\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 22.50893\n",
      "Epoch 220/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.5829 - mean_absolute_error: 9.6429 - val_loss: 32.7269 - val_mean_absolute_error: 4.0797\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 22.50893\n",
      "Epoch 221/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.4111 - mean_absolute_error: 9.3341 - val_loss: 28.5083 - val_mean_absolute_error: 3.7056\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 22.50893\n",
      "Epoch 222/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.5294 - mean_absolute_error: 9.9726 - val_loss: 31.0224 - val_mean_absolute_error: 3.7737\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 22.50893\n",
      "Epoch 223/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 156.2792 - mean_absolute_error: 9.6183 - val_loss: 29.7984 - val_mean_absolute_error: 3.8139\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 22.50893\n",
      "Epoch 224/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 138.6545 - mean_absolute_error: 9.2643 - val_loss: 41.6875 - val_mean_absolute_error: 5.2083\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 22.50893\n",
      "Epoch 225/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 164.9223 - mean_absolute_error: 10.1191 - val_loss: 49.5424 - val_mean_absolute_error: 5.2466\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 22.50893\n",
      "Epoch 226/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 210.5470 - mean_absolute_error: 11.1582 - val_loss: 47.9235 - val_mean_absolute_error: 4.0175\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 22.50893\n",
      "Epoch 227/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 193.9163 - mean_absolute_error: 10.8971 - val_loss: 48.3516 - val_mean_absolute_error: 3.7719\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 22.50893\n",
      "Epoch 228/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 196.2043 - mean_absolute_error: 10.6075 - val_loss: 50.3845 - val_mean_absolute_error: 4.9791\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 22.50893\n",
      "Epoch 229/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 188.5098 - mean_absolute_error: 10.4490 - val_loss: 40.8585 - val_mean_absolute_error: 4.0525\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 22.50893\n",
      "Epoch 230/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 162.8927 - mean_absolute_error: 9.7868 - val_loss: 55.9826 - val_mean_absolute_error: 5.7042\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 22.50893\n",
      "Epoch 231/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 178.8551 - mean_absolute_error: 10.4783 - val_loss: 36.4860 - val_mean_absolute_error: 3.7155\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 22.50893\n",
      "Epoch 232/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 175.8230 - mean_absolute_error: 10.3511 - val_loss: 33.4109 - val_mean_absolute_error: 3.4880\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 22.50893\n",
      "Epoch 233/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.5996 - mean_absolute_error: 10.1738 - val_loss: 42.3062 - val_mean_absolute_error: 4.9578\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 22.50893\n",
      "Epoch 234/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 148.4499 - mean_absolute_error: 9.4041 - val_loss: 32.5155 - val_mean_absolute_error: 3.3593\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 22.50893\n",
      "Epoch 235/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 161.4527 - mean_absolute_error: 9.9561 - val_loss: 33.7736 - val_mean_absolute_error: 3.4574\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 22.50893\n",
      "Epoch 236/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 159.7163 - mean_absolute_error: 10.2688 - val_loss: 32.9156 - val_mean_absolute_error: 3.9985\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 22.50893\n",
      "Epoch 237/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 163.5119 - mean_absolute_error: 10.2603 - val_loss: 31.3396 - val_mean_absolute_error: 3.7400\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 22.50893\n",
      "Epoch 238/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 132.6239 - mean_absolute_error: 9.0080 - val_loss: 29.4713 - val_mean_absolute_error: 3.5089\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 22.50893\n",
      "Epoch 239/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.4479 - mean_absolute_error: 9.4150 - val_loss: 32.5264 - val_mean_absolute_error: 4.5129\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 22.50893\n",
      "Epoch 240/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 183.5586 - mean_absolute_error: 10.5802 - val_loss: 29.9333 - val_mean_absolute_error: 3.6265\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 22.50893\n",
      "Epoch 241/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 169.5050 - mean_absolute_error: 10.1627 - val_loss: 28.4443 - val_mean_absolute_error: 3.7378\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 22.50893\n",
      "Epoch 242/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.3299 - mean_absolute_error: 9.5915 - val_loss: 23.8381 - val_mean_absolute_error: 3.3652\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 22.50893\n",
      "Epoch 243/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.0360 - mean_absolute_error: 10.2973 - val_loss: 33.5474 - val_mean_absolute_error: 4.7124\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 22.50893\n",
      "Epoch 244/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.7262 - mean_absolute_error: 10.0101 - val_loss: 22.9493 - val_mean_absolute_error: 3.2127\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 22.50893\n",
      "Epoch 245/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.6604 - mean_absolute_error: 9.2502 - val_loss: 31.2235 - val_mean_absolute_error: 4.1305\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 22.50893\n",
      "Epoch 246/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.2849 - mean_absolute_error: 9.6939 - val_loss: 30.1247 - val_mean_absolute_error: 3.8769\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 22.50893\n",
      "Epoch 247/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.4412 - mean_absolute_error: 9.9081 - val_loss: 26.2501 - val_mean_absolute_error: 3.3272\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 22.50893\n",
      "Epoch 248/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 152.5834 - mean_absolute_error: 10.0051 - val_loss: 26.2144 - val_mean_absolute_error: 3.2460\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 22.50893\n",
      "Epoch 249/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.9881 - mean_absolute_error: 9.4168 - val_loss: 39.9427 - val_mean_absolute_error: 4.9302\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 22.50893\n",
      "Epoch 250/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 146.5044 - mean_absolute_error: 9.6184 - val_loss: 26.5944 - val_mean_absolute_error: 3.8421\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 22.50893\n",
      "Epoch 251/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 165.1377 - mean_absolute_error: 10.1012 - val_loss: 36.1591 - val_mean_absolute_error: 4.3517\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 22.50893\n",
      "Epoch 252/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.4137 - mean_absolute_error: 10.0390 - val_loss: 23.8957 - val_mean_absolute_error: 2.9220\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 22.50893\n",
      "Epoch 253/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.3880 - mean_absolute_error: 9.8476 - val_loss: 21.7651 - val_mean_absolute_error: 3.1012\n",
      "\n",
      "Epoch 00253: val_loss improved from 22.50893 to 21.76511, saving model to RNN_best\n",
      "Epoch 254/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.8705 - mean_absolute_error: 9.5381 - val_loss: 78.8491 - val_mean_absolute_error: 7.1156\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 21.76511\n",
      "Epoch 255/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 206.6717 - mean_absolute_error: 11.0956 - val_loss: 84.1834 - val_mean_absolute_error: 5.3240\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 21.76511\n",
      "Epoch 256/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 209.0903 - mean_absolute_error: 10.6764 - val_loss: 46.5064 - val_mean_absolute_error: 5.7704\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 21.76511\n",
      "Epoch 257/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 154.1954 - mean_absolute_error: 9.3658 - val_loss: 38.9646 - val_mean_absolute_error: 4.8202\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 21.76511\n",
      "Epoch 258/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 171.9957 - mean_absolute_error: 10.1285 - val_loss: 32.8974 - val_mean_absolute_error: 3.8889\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 21.76511\n",
      "Epoch 259/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 175.5057 - mean_absolute_error: 10.3609 - val_loss: 39.4546 - val_mean_absolute_error: 4.4192\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 21.76511\n",
      "Epoch 260/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 142.6457 - mean_absolute_error: 9.3457 - val_loss: 32.3861 - val_mean_absolute_error: 3.5246\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 21.76511\n",
      "Epoch 261/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 141.1430 - mean_absolute_error: 9.4518 - val_loss: 29.9800 - val_mean_absolute_error: 3.3410\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 21.76511\n",
      "Epoch 262/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.6734 - mean_absolute_error: 9.7497 - val_loss: 30.1329 - val_mean_absolute_error: 3.6514\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 21.76511\n",
      "Epoch 263/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 159.4157 - mean_absolute_error: 9.9959 - val_loss: 28.3794 - val_mean_absolute_error: 3.5895\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 21.76511\n",
      "Epoch 264/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 175.4762 - mean_absolute_error: 10.5214 - val_loss: 39.1028 - val_mean_absolute_error: 4.7021\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 21.76511\n",
      "Epoch 265/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 152.2496 - mean_absolute_error: 9.7167 - val_loss: 28.8435 - val_mean_absolute_error: 3.1542\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 21.76511\n",
      "Epoch 266/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.7535 - mean_absolute_error: 9.8190 - val_loss: 29.8362 - val_mean_absolute_error: 3.6921\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 21.76511\n",
      "Epoch 267/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 171.3589 - mean_absolute_error: 10.2601 - val_loss: 42.3869 - val_mean_absolute_error: 4.6558\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 21.76511\n",
      "Epoch 268/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.7402 - mean_absolute_error: 9.6956 - val_loss: 33.6880 - val_mean_absolute_error: 3.3712\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 21.76511\n",
      "Epoch 269/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 170.5885 - mean_absolute_error: 10.5178 - val_loss: 34.4859 - val_mean_absolute_error: 3.7239\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 21.76511\n",
      "Epoch 270/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 179.4181 - mean_absolute_error: 10.3126 - val_loss: 30.9098 - val_mean_absolute_error: 3.9031\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 21.76511\n",
      "Epoch 271/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 146.5732 - mean_absolute_error: 9.6614 - val_loss: 39.2506 - val_mean_absolute_error: 4.7186\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 21.76511\n",
      "Epoch 272/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 154.9869 - mean_absolute_error: 9.9467 - val_loss: 33.3769 - val_mean_absolute_error: 3.7708\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 21.76511\n",
      "Epoch 273/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.6922 - mean_absolute_error: 9.4657 - val_loss: 27.6978 - val_mean_absolute_error: 3.1909\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 21.76511\n",
      "Epoch 274/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 159.8975 - mean_absolute_error: 10.0036 - val_loss: 46.6940 - val_mean_absolute_error: 5.1541\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 21.76511\n",
      "Epoch 275/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.4813 - mean_absolute_error: 9.3377 - val_loss: 28.5256 - val_mean_absolute_error: 3.3880\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 21.76511\n",
      "Epoch 276/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 136.5831 - mean_absolute_error: 9.2964 - val_loss: 41.4697 - val_mean_absolute_error: 4.8610\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 21.76511\n",
      "Epoch 277/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.4356 - mean_absolute_error: 9.4983 - val_loss: 26.7528 - val_mean_absolute_error: 3.3137\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 21.76511\n",
      "Epoch 278/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.8504 - mean_absolute_error: 9.6215 - val_loss: 37.4793 - val_mean_absolute_error: 4.2136\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 21.76511\n",
      "Epoch 279/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 157.5287 - mean_absolute_error: 9.8075 - val_loss: 29.9739 - val_mean_absolute_error: 3.8965\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 21.76511\n",
      "Epoch 280/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 167.3428 - mean_absolute_error: 10.1848 - val_loss: 33.1626 - val_mean_absolute_error: 4.0410\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 21.76511\n",
      "Epoch 281/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.3389 - mean_absolute_error: 10.0249 - val_loss: 28.2172 - val_mean_absolute_error: 3.8300\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 21.76511\n",
      "Epoch 282/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.6005 - mean_absolute_error: 9.5426 - val_loss: 30.7904 - val_mean_absolute_error: 3.7034\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 21.76511\n",
      "Epoch 283/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 136.1768 - mean_absolute_error: 9.0661 - val_loss: 27.1950 - val_mean_absolute_error: 3.6609\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 21.76511\n",
      "Epoch 284/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.0241 - mean_absolute_error: 9.4397 - val_loss: 28.1009 - val_mean_absolute_error: 3.9031\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 21.76511\n",
      "Epoch 285/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 156.4836 - mean_absolute_error: 9.8468 - val_loss: 28.8607 - val_mean_absolute_error: 3.3643\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 21.76511\n",
      "Epoch 286/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.5307 - mean_absolute_error: 9.8081 - val_loss: 35.8969 - val_mean_absolute_error: 4.2685\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 21.76511\n",
      "Epoch 287/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 136.7586 - mean_absolute_error: 9.2591 - val_loss: 36.4498 - val_mean_absolute_error: 4.5517\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 21.76511\n",
      "Epoch 288/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.5823 - mean_absolute_error: 9.5032 - val_loss: 28.3739 - val_mean_absolute_error: 3.8768\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 21.76511\n",
      "Epoch 289/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.4052 - mean_absolute_error: 9.2957 - val_loss: 25.3888 - val_mean_absolute_error: 3.3237\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 21.76511\n",
      "Epoch 290/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.2520 - mean_absolute_error: 9.9204 - val_loss: 29.2545 - val_mean_absolute_error: 3.5607\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 21.76511\n",
      "Epoch 291/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.6169 - mean_absolute_error: 9.6145 - val_loss: 22.8164 - val_mean_absolute_error: 3.1418\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 21.76511\n",
      "Epoch 292/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.3126 - mean_absolute_error: 9.8587 - val_loss: 28.6993 - val_mean_absolute_error: 3.6275\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 21.76511\n",
      "Epoch 293/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.4895 - mean_absolute_error: 9.7105 - val_loss: 28.2808 - val_mean_absolute_error: 3.5580\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 21.76511\n",
      "Epoch 294/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 141.1242 - mean_absolute_error: 9.3956 - val_loss: 29.5266 - val_mean_absolute_error: 3.7981\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 21.76511\n",
      "Epoch 295/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.5183 - mean_absolute_error: 9.8331 - val_loss: 33.8357 - val_mean_absolute_error: 4.2538\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 21.76511\n",
      "Epoch 296/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 157.0691 - mean_absolute_error: 9.6168 - val_loss: 31.7793 - val_mean_absolute_error: 3.5441\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 21.76511\n",
      "Epoch 297/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.6922 - mean_absolute_error: 9.4188 - val_loss: 31.4933 - val_mean_absolute_error: 3.8901\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 21.76511\n",
      "Epoch 298/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 155.9638 - mean_absolute_error: 9.9831 - val_loss: 31.5240 - val_mean_absolute_error: 3.7967\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 21.76511\n",
      "Epoch 299/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 150.3599 - mean_absolute_error: 9.5295 - val_loss: 34.8135 - val_mean_absolute_error: 4.2603\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 21.76511\n",
      "Epoch 300/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 161.5570 - mean_absolute_error: 9.8346 - val_loss: 25.6032 - val_mean_absolute_error: 3.2348\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 21.76511\n",
      "Epoch 301/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.9483 - mean_absolute_error: 9.2267 - val_loss: 24.8062 - val_mean_absolute_error: 3.3297\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 21.76511\n",
      "Epoch 302/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.7228 - mean_absolute_error: 9.5275 - val_loss: 33.6558 - val_mean_absolute_error: 3.5666\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 21.76511\n",
      "Epoch 303/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 144.2320 - mean_absolute_error: 9.6714 - val_loss: 26.2801 - val_mean_absolute_error: 3.4051\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 21.76511\n",
      "Epoch 304/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.1753 - mean_absolute_error: 9.5135 - val_loss: 41.9888 - val_mean_absolute_error: 4.9216\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 21.76511\n",
      "Epoch 305/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.1393 - mean_absolute_error: 9.5119 - val_loss: 27.2850 - val_mean_absolute_error: 3.5356\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 21.76511\n",
      "Epoch 306/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.7151 - mean_absolute_error: 9.6306 - val_loss: 25.3231 - val_mean_absolute_error: 3.0737\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 21.76511\n",
      "Epoch 307/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.6491 - mean_absolute_error: 9.3749 - val_loss: 30.5580 - val_mean_absolute_error: 4.0429\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 21.76511\n",
      "Epoch 308/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.9482 - mean_absolute_error: 9.4963 - val_loss: 29.6783 - val_mean_absolute_error: 3.8267\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 21.76511\n",
      "Epoch 309/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 150.6281 - mean_absolute_error: 9.7113 - val_loss: 24.6813 - val_mean_absolute_error: 2.9942\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 21.76511\n",
      "Epoch 310/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.8065 - mean_absolute_error: 9.5369 - val_loss: 28.8656 - val_mean_absolute_error: 3.8082\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 21.76511\n",
      "Epoch 311/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.8096 - mean_absolute_error: 9.3266 - val_loss: 31.6265 - val_mean_absolute_error: 4.0963\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 21.76511\n",
      "Epoch 312/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 142.7679 - mean_absolute_error: 9.5458 - val_loss: 30.9945 - val_mean_absolute_error: 4.0054\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 21.76511\n",
      "Epoch 313/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 162.9280 - mean_absolute_error: 10.0201 - val_loss: 30.4518 - val_mean_absolute_error: 3.4174\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 21.76511\n",
      "Epoch 314/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 149.1296 - mean_absolute_error: 9.9781 - val_loss: 41.3355 - val_mean_absolute_error: 4.6034\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 21.76511\n",
      "Epoch 315/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 154.1443 - mean_absolute_error: 9.7120 - val_loss: 31.7202 - val_mean_absolute_error: 3.7180\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 21.76511\n",
      "Epoch 316/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 131.6485 - mean_absolute_error: 9.0752 - val_loss: 25.5818 - val_mean_absolute_error: 3.4746\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 21.76511\n",
      "Epoch 317/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 173.6188 - mean_absolute_error: 10.3153 - val_loss: 30.1581 - val_mean_absolute_error: 3.6914\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 21.76511\n",
      "Epoch 318/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 164.2650 - mean_absolute_error: 9.9973 - val_loss: 29.7529 - val_mean_absolute_error: 3.2203\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 21.76511\n",
      "Epoch 319/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 133.6219 - mean_absolute_error: 9.0174 - val_loss: 30.5074 - val_mean_absolute_error: 3.8769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: val_loss did not improve from 21.76511\n",
      "Epoch 320/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 161.8636 - mean_absolute_error: 10.0138 - val_loss: 29.1219 - val_mean_absolute_error: 3.6036\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 21.76511\n",
      "Epoch 321/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.2894 - mean_absolute_error: 9.5911 - val_loss: 25.7514 - val_mean_absolute_error: 3.2375\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 21.76511\n",
      "Epoch 322/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.1001 - mean_absolute_error: 9.6740 - val_loss: 33.1222 - val_mean_absolute_error: 4.0915\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 21.76511\n",
      "Epoch 323/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.6538 - mean_absolute_error: 10.3086 - val_loss: 29.3434 - val_mean_absolute_error: 3.2371\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 21.76511\n",
      "Epoch 324/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 167.9736 - mean_absolute_error: 10.4128 - val_loss: 37.6893 - val_mean_absolute_error: 4.1633\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 21.76511\n",
      "Epoch 325/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 174.0709 - mean_absolute_error: 10.4006 - val_loss: 26.5023 - val_mean_absolute_error: 3.0376\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 21.76511\n",
      "Epoch 326/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 148.9799 - mean_absolute_error: 9.5457 - val_loss: 53.4266 - val_mean_absolute_error: 5.8260\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 21.76511\n",
      "Epoch 327/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 173.6457 - mean_absolute_error: 10.2816 - val_loss: 30.0306 - val_mean_absolute_error: 3.6184\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 21.76511\n",
      "Epoch 328/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 152.6681 - mean_absolute_error: 9.6761 - val_loss: 31.3453 - val_mean_absolute_error: 3.8304\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 21.76511\n",
      "Epoch 329/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 152.5747 - mean_absolute_error: 9.6716 - val_loss: 52.0358 - val_mean_absolute_error: 5.3439\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 21.76511\n",
      "Epoch 330/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 153.3936 - mean_absolute_error: 9.7750 - val_loss: 33.9622 - val_mean_absolute_error: 4.1098\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 21.76511\n",
      "Epoch 331/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 159.3368 - mean_absolute_error: 10.0363 - val_loss: 29.0945 - val_mean_absolute_error: 3.4203\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 21.76511\n",
      "Epoch 332/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 146.8279 - mean_absolute_error: 9.4096 - val_loss: 24.5066 - val_mean_absolute_error: 3.3835\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 21.76511\n",
      "Epoch 333/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 136.4030 - mean_absolute_error: 9.0837 - val_loss: 37.3745 - val_mean_absolute_error: 4.8830\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 21.76511\n",
      "Epoch 334/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 145.6731 - mean_absolute_error: 9.5097 - val_loss: 42.8398 - val_mean_absolute_error: 4.6812\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 21.76511\n",
      "Epoch 335/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.5376 - mean_absolute_error: 9.7966 - val_loss: 42.7538 - val_mean_absolute_error: 4.9584\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 21.76511\n",
      "Epoch 336/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 208.4130 - mean_absolute_error: 10.7411 - val_loss: 32.8834 - val_mean_absolute_error: 4.2053\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 21.76511\n",
      "Epoch 337/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 166.9949 - mean_absolute_error: 9.6474 - val_loss: 35.7370 - val_mean_absolute_error: 4.2821\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 21.76511\n",
      "Epoch 338/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 160.7725 - mean_absolute_error: 9.9219 - val_loss: 27.3302 - val_mean_absolute_error: 3.1757\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 21.76511\n",
      "Epoch 339/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 150.7063 - mean_absolute_error: 9.6731 - val_loss: 25.9996 - val_mean_absolute_error: 3.1319\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 21.76511\n",
      "Epoch 340/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.3539 - mean_absolute_error: 9.5455 - val_loss: 41.7546 - val_mean_absolute_error: 5.2036\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 21.76511\n",
      "Epoch 341/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 147.8915 - mean_absolute_error: 9.4173 - val_loss: 25.6521 - val_mean_absolute_error: 3.3500\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 21.76511\n",
      "Epoch 342/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.5391 - mean_absolute_error: 9.8725 - val_loss: 29.4149 - val_mean_absolute_error: 3.5776\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 21.76511\n",
      "Epoch 343/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 161.7196 - mean_absolute_error: 9.9713 - val_loss: 24.4375 - val_mean_absolute_error: 3.2965\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 21.76511\n",
      "Epoch 344/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 149.8057 - mean_absolute_error: 9.6410 - val_loss: 29.5476 - val_mean_absolute_error: 4.0892\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 21.76511\n",
      "Epoch 345/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 155.4396 - mean_absolute_error: 10.0162 - val_loss: 26.1772 - val_mean_absolute_error: 3.3410\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 21.76511\n",
      "Epoch 346/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 146.3805 - mean_absolute_error: 9.6695 - val_loss: 26.1970 - val_mean_absolute_error: 3.5986\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 21.76511\n",
      "Epoch 347/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 143.4865 - mean_absolute_error: 9.4414 - val_loss: 27.1460 - val_mean_absolute_error: 3.5908\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 21.76511\n",
      "Epoch 348/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 151.0553 - mean_absolute_error: 9.6257 - val_loss: 26.5880 - val_mean_absolute_error: 3.2266\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 21.76511\n",
      "Epoch 349/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 139.1134 - mean_absolute_error: 9.1107 - val_loss: 32.4212 - val_mean_absolute_error: 4.4023\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 21.76511\n",
      "Epoch 350/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 135.0236 - mean_absolute_error: 9.0911 - val_loss: 30.0460 - val_mean_absolute_error: 4.0557\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 21.76511\n",
      "Epoch 351/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 157.0183 - mean_absolute_error: 9.9706 - val_loss: 28.6576 - val_mean_absolute_error: 3.5280\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 21.76511\n",
      "Epoch 352/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 144.7984 - mean_absolute_error: 9.4952 - val_loss: 28.4480 - val_mean_absolute_error: 3.6607\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 21.76511\n",
      "Epoch 353/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 162.3854 - mean_absolute_error: 10.1244 - val_loss: 35.0066 - val_mean_absolute_error: 4.3406\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 21.76511\n",
      "Epoch 00353: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_VFI = lstm_baseline(X_train, y_train)\n",
    "outputs = [layer.output for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97.056282</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.747429</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.158716</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94.680809</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.711060</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89.609360</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96.843559</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91.935715</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89.532005</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96.473885</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>88.786278</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>95.863289</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>81.370003</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>53.362759</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>94.353874</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>97.069633</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>62.353527</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>96.785164</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96.652412</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>54.593185</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>96.756172</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>95.026314</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>93.962349</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>96.980431</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>96.231781</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>96.892006</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>88.361816</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>96.499443</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>96.821068</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>94.114670</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>96.345314</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>94.759552</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>62.685810</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>96.864159</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>84.829216</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>83.192574</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>93.343880</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>91.654015</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>51.639408</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>57.494701</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>94.775169</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>89.167198</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>96.307777</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>88.596214</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>55.410583</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>95.740585</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>75.552330</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>96.515587</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>96.873848</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>96.808708</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>89.717461</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>84.030273</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>77.163956</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>85.624847</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>95.811745</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>96.385612</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>96.715500</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>92.484322</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>92.757881</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>84.111214</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  actual\n",
       "0    97.056282   100.0\n",
       "1    95.747429    98.0\n",
       "2    27.158716    46.0\n",
       "3    94.680809    99.0\n",
       "4    95.711060    95.0\n",
       "5    89.609360    92.0\n",
       "6    96.843559    99.0\n",
       "7    91.935715    99.0\n",
       "8    89.532005    69.0\n",
       "9    96.473885    99.0\n",
       "10   88.786278    87.0\n",
       "11   95.863289    98.0\n",
       "12   81.370003    97.0\n",
       "13   53.362759    56.0\n",
       "14   94.353874    99.0\n",
       "15   97.069633    98.0\n",
       "16   62.353527    71.0\n",
       "17   96.785164    99.0\n",
       "18   96.652412    99.0\n",
       "19   54.593185    71.0\n",
       "20   96.756172   100.0\n",
       "21   95.026314   100.0\n",
       "22   93.962349    96.0\n",
       "23   96.980431    98.0\n",
       "24   96.231781    95.0\n",
       "25   96.892006   100.0\n",
       "26   88.361816    95.0\n",
       "27   96.499443   100.0\n",
       "28   96.821068    99.0\n",
       "29   94.114670    96.0\n",
       "..         ...     ...\n",
       "134  96.345314   100.0\n",
       "135  94.759552   100.0\n",
       "136  62.685810    80.0\n",
       "137  96.864159    99.0\n",
       "138  84.829216    92.0\n",
       "139  83.192574    76.0\n",
       "140  93.343880    88.0\n",
       "141  91.654015    85.0\n",
       "142  51.639408    62.0\n",
       "143  57.494701    69.0\n",
       "144  94.775169    98.0\n",
       "145  89.167198    91.0\n",
       "146  96.307777   100.0\n",
       "147  88.596214    76.0\n",
       "148  55.410583    62.0\n",
       "149  95.740585    98.0\n",
       "150  75.552330    78.0\n",
       "151  96.515587    99.0\n",
       "152  96.873848   100.0\n",
       "153  96.808708   100.0\n",
       "154  89.717461    91.0\n",
       "155  84.030273    96.0\n",
       "156  77.163956    85.0\n",
       "157  85.624847    92.0\n",
       "158  95.811745    98.0\n",
       "159  96.385612    98.0\n",
       "160  96.715500    99.0\n",
       "161  92.484322   100.0\n",
       "162  92.757881    94.0\n",
       "163  84.111214    85.0\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_VFI.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = y_pred.tolist()\n",
    "dictionary_DF = {'predicted':y_pred, 'actual':y_test}\n",
    "y_VFI_DF = pd.DataFrame(dictionary_DF)\n",
    "y_VFI_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    164.000000\n",
       "mean       5.645670\n",
       "std        7.838282\n",
       "min        0.000656\n",
       "25%        2.136494\n",
       "50%        3.224079\n",
       "75%        6.565681\n",
       "max       74.494698\n",
       "dtype: float64"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_VFI_DF\n",
    "difference_VFI = pd.Series(abs(y_test-y_pred))\n",
    "difference_VFI.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_GCA, test_size = 0.2, random_state = 200)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522 samples, validate on 131 samples\n",
      "Epoch 1/1000\n",
      "522/522 [==============================] - 14s 28ms/step - loss: 4754.4268 - mean_absolute_error: 68.2704 - val_loss: 4213.3220 - val_mean_absolute_error: 64.2058\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4213.32199, saving model to RNN_best\n",
      "Epoch 2/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3944.8978 - mean_absolute_error: 62.0172 - val_loss: 3373.4534 - val_mean_absolute_error: 57.2962\n",
      "\n",
      "Epoch 00002: val_loss improved from 4213.32199 to 3373.45343, saving model to RNN_best\n",
      "Epoch 3/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 3201.1428 - mean_absolute_error: 55.6783 - val_loss: 2721.3796 - val_mean_absolute_error: 51.2735\n",
      "\n",
      "Epoch 00003: val_loss improved from 3373.45343 to 2721.37963, saving model to RNN_best\n",
      "Epoch 4/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 2557.6460 - mean_absolute_error: 49.4950 - val_loss: 2059.3238 - val_mean_absolute_error: 44.3517\n",
      "\n",
      "Epoch 00004: val_loss improved from 2721.37963 to 2059.32382, saving model to RNN_best\n",
      "Epoch 5/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1875.6457 - mean_absolute_error: 42.0629 - val_loss: 1444.4958 - val_mean_absolute_error: 36.7731\n",
      "\n",
      "Epoch 00005: val_loss improved from 2059.32382 to 1444.49583, saving model to RNN_best\n",
      "Epoch 6/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 1303.3930 - mean_absolute_error: 34.4602 - val_loss: 933.7475 - val_mean_absolute_error: 29.0079\n",
      "\n",
      "Epoch 00006: val_loss improved from 1444.49583 to 933.74751, saving model to RNN_best\n",
      "Epoch 7/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 856.9847 - mean_absolute_error: 26.8584 - val_loss: 555.0677 - val_mean_absolute_error: 21.6044\n",
      "\n",
      "Epoch 00007: val_loss improved from 933.74751 to 555.06772, saving model to RNN_best\n",
      "Epoch 8/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 542.9755 - mean_absolute_error: 20.2781 - val_loss: 309.8492 - val_mean_absolute_error: 15.3833\n",
      "\n",
      "Epoch 00008: val_loss improved from 555.06772 to 309.84921, saving model to RNN_best\n",
      "Epoch 9/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 350.5994 - mean_absolute_error: 15.8021 - val_loss: 175.5092 - val_mean_absolute_error: 11.0180\n",
      "\n",
      "Epoch 00009: val_loss improved from 309.84921 to 175.50923, saving model to RNN_best\n",
      "Epoch 10/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 221.3703 - mean_absolute_error: 12.1280 - val_loss: 118.6796 - val_mean_absolute_error: 8.8028\n",
      "\n",
      "Epoch 00010: val_loss improved from 175.50923 to 118.67964, saving model to RNN_best\n",
      "Epoch 11/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 190.5199 - mean_absolute_error: 10.9282 - val_loss: 100.0521 - val_mean_absolute_error: 8.0173\n",
      "\n",
      "Epoch 00011: val_loss improved from 118.67964 to 100.05213, saving model to RNN_best\n",
      "Epoch 12/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 160.8533 - mean_absolute_error: 10.0515 - val_loss: 94.1454 - val_mean_absolute_error: 7.7391\n",
      "\n",
      "Epoch 00012: val_loss improved from 100.05213 to 94.14539, saving model to RNN_best\n",
      "Epoch 13/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 174.5111 - mean_absolute_error: 10.7154 - val_loss: 92.3390 - val_mean_absolute_error: 7.6321\n",
      "\n",
      "Epoch 00013: val_loss improved from 94.14539 to 92.33900, saving model to RNN_best\n",
      "Epoch 14/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 162.4266 - mean_absolute_error: 10.3251 - val_loss: 91.6123 - val_mean_absolute_error: 7.5750\n",
      "\n",
      "Epoch 00014: val_loss improved from 92.33900 to 91.61225, saving model to RNN_best\n",
      "Epoch 15/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 177.2410 - mean_absolute_error: 10.4476 - val_loss: 91.2969 - val_mean_absolute_error: 7.5596\n",
      "\n",
      "Epoch 00015: val_loss improved from 91.61225 to 91.29687, saving model to RNN_best\n",
      "Epoch 16/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 175.4072 - mean_absolute_error: 10.5811 - val_loss: 90.9640 - val_mean_absolute_error: 7.5424\n",
      "\n",
      "Epoch 00016: val_loss improved from 91.29687 to 90.96396, saving model to RNN_best\n",
      "Epoch 17/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 167.0621 - mean_absolute_error: 10.2409 - val_loss: 90.8177 - val_mean_absolute_error: 7.5348\n",
      "\n",
      "Epoch 00017: val_loss improved from 90.96396 to 90.81769, saving model to RNN_best\n",
      "Epoch 18/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 173.5183 - mean_absolute_error: 10.7407 - val_loss: 90.6212 - val_mean_absolute_error: 7.5286\n",
      "\n",
      "Epoch 00018: val_loss improved from 90.81769 to 90.62116, saving model to RNN_best\n",
      "Epoch 19/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 168.6663 - mean_absolute_error: 10.2623 - val_loss: 90.3193 - val_mean_absolute_error: 7.5226\n",
      "\n",
      "Epoch 00019: val_loss improved from 90.62116 to 90.31930, saving model to RNN_best\n",
      "Epoch 20/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 181.0773 - mean_absolute_error: 10.8095 - val_loss: 89.9992 - val_mean_absolute_error: 7.5106\n",
      "\n",
      "Epoch 00020: val_loss improved from 90.31930 to 89.99925, saving model to RNN_best\n",
      "Epoch 21/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 180.6990 - mean_absolute_error: 10.6831 - val_loss: 89.6607 - val_mean_absolute_error: 7.4978\n",
      "\n",
      "Epoch 00021: val_loss improved from 89.99925 to 89.66068, saving model to RNN_best\n",
      "Epoch 22/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 173.1140 - mean_absolute_error: 10.5762 - val_loss: 89.3847 - val_mean_absolute_error: 7.4831\n",
      "\n",
      "Epoch 00022: val_loss improved from 89.66068 to 89.38469, saving model to RNN_best\n",
      "Epoch 23/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 170.3204 - mean_absolute_error: 10.1271 - val_loss: 88.3487 - val_mean_absolute_error: 7.4446\n",
      "\n",
      "Epoch 00023: val_loss improved from 89.38469 to 88.34867, saving model to RNN_best\n",
      "Epoch 24/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 161.4471 - mean_absolute_error: 10.0711 - val_loss: 85.0779 - val_mean_absolute_error: 7.2880\n",
      "\n",
      "Epoch 00024: val_loss improved from 88.34867 to 85.07790, saving model to RNN_best\n",
      "Epoch 25/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 162.4840 - mean_absolute_error: 10.2288 - val_loss: 85.5151 - val_mean_absolute_error: 7.2893\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 85.07790\n",
      "Epoch 26/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 171.8445 - mean_absolute_error: 10.4267 - val_loss: 83.8000 - val_mean_absolute_error: 7.2182\n",
      "\n",
      "Epoch 00026: val_loss improved from 85.07790 to 83.80002, saving model to RNN_best\n",
      "Epoch 27/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 162.1196 - mean_absolute_error: 10.0376 - val_loss: 88.5853 - val_mean_absolute_error: 7.3780\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 83.80002\n",
      "Epoch 28/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 184.1543 - mean_absolute_error: 11.1020 - val_loss: 80.0349 - val_mean_absolute_error: 7.0174\n",
      "\n",
      "Epoch 00028: val_loss improved from 83.80002 to 80.03491, saving model to RNN_best\n",
      "Epoch 29/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 158.9815 - mean_absolute_error: 10.1743 - val_loss: 59.2207 - val_mean_absolute_error: 5.7489\n",
      "\n",
      "Epoch 00029: val_loss improved from 80.03491 to 59.22071, saving model to RNN_best\n",
      "Epoch 30/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 128.9903 - mean_absolute_error: 8.9858 - val_loss: 46.9537 - val_mean_absolute_error: 5.1236\n",
      "\n",
      "Epoch 00030: val_loss improved from 59.22071 to 46.95373, saving model to RNN_best\n",
      "Epoch 31/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 129.7786 - mean_absolute_error: 8.9894 - val_loss: 42.5638 - val_mean_absolute_error: 4.8502\n",
      "\n",
      "Epoch 00031: val_loss improved from 46.95373 to 42.56376, saving model to RNN_best\n",
      "Epoch 32/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 130.4436 - mean_absolute_error: 8.7752 - val_loss: 38.2100 - val_mean_absolute_error: 4.6110\n",
      "\n",
      "Epoch 00032: val_loss improved from 42.56376 to 38.20998, saving model to RNN_best\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 109.9587 - mean_absolute_error: 8.2176 - val_loss: 33.3889 - val_mean_absolute_error: 4.3426\n",
      "\n",
      "Epoch 00033: val_loss improved from 38.20998 to 33.38887, saving model to RNN_best\n",
      "Epoch 34/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 130.7605 - mean_absolute_error: 8.8876 - val_loss: 31.5125 - val_mean_absolute_error: 4.0492\n",
      "\n",
      "Epoch 00034: val_loss improved from 33.38887 to 31.51248, saving model to RNN_best\n",
      "Epoch 35/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 119.4262 - mean_absolute_error: 8.4228 - val_loss: 29.3453 - val_mean_absolute_error: 3.8816\n",
      "\n",
      "Epoch 00035: val_loss improved from 31.51248 to 29.34531, saving model to RNN_best\n",
      "Epoch 36/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 104.6377 - mean_absolute_error: 7.9908 - val_loss: 24.6887 - val_mean_absolute_error: 3.6433\n",
      "\n",
      "Epoch 00036: val_loss improved from 29.34531 to 24.68875, saving model to RNN_best\n",
      "Epoch 37/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.1584 - mean_absolute_error: 7.9727 - val_loss: 26.2194 - val_mean_absolute_error: 3.7147\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 24.68875\n",
      "Epoch 38/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 116.2176 - mean_absolute_error: 8.4542 - val_loss: 24.2641 - val_mean_absolute_error: 3.7835\n",
      "\n",
      "Epoch 00038: val_loss improved from 24.68875 to 24.26411, saving model to RNN_best\n",
      "Epoch 39/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.7585 - mean_absolute_error: 7.8500 - val_loss: 28.8235 - val_mean_absolute_error: 4.0312\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 24.26411\n",
      "Epoch 40/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.0039 - mean_absolute_error: 7.7659 - val_loss: 26.9731 - val_mean_absolute_error: 4.1517\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 24.26411\n",
      "Epoch 41/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 101.5536 - mean_absolute_error: 7.8583 - val_loss: 40.8298 - val_mean_absolute_error: 5.2193\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 24.26411\n",
      "Epoch 42/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 110.2384 - mean_absolute_error: 8.2283 - val_loss: 24.9467 - val_mean_absolute_error: 3.8968\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 24.26411\n",
      "Epoch 43/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.9667 - mean_absolute_error: 7.8955 - val_loss: 19.3261 - val_mean_absolute_error: 3.2318\n",
      "\n",
      "Epoch 00043: val_loss improved from 24.26411 to 19.32612, saving model to RNN_best\n",
      "Epoch 44/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.3089 - mean_absolute_error: 7.9456 - val_loss: 20.4650 - val_mean_absolute_error: 3.3637\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 19.32612\n",
      "Epoch 45/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.3627 - mean_absolute_error: 7.9082 - val_loss: 17.8103 - val_mean_absolute_error: 3.1295\n",
      "\n",
      "Epoch 00045: val_loss improved from 19.32612 to 17.81030, saving model to RNN_best\n",
      "Epoch 46/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 103.8597 - mean_absolute_error: 7.9295 - val_loss: 21.6385 - val_mean_absolute_error: 3.6704\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 17.81030\n",
      "Epoch 47/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 93.6940 - mean_absolute_error: 7.7063 - val_loss: 18.8513 - val_mean_absolute_error: 3.3183\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 17.81030\n",
      "Epoch 48/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.2039 - mean_absolute_error: 7.8132 - val_loss: 23.3233 - val_mean_absolute_error: 3.8151\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 17.81030\n",
      "Epoch 49/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 102.3472 - mean_absolute_error: 8.0963 - val_loss: 17.8572 - val_mean_absolute_error: 3.3312\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 17.81030\n",
      "Epoch 50/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 97.7971 - mean_absolute_error: 8.0262 - val_loss: 20.5766 - val_mean_absolute_error: 3.5944\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 17.81030\n",
      "Epoch 51/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.2889 - mean_absolute_error: 7.7969 - val_loss: 23.5188 - val_mean_absolute_error: 4.0656\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 17.81030\n",
      "Epoch 52/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.9549 - mean_absolute_error: 7.7547 - val_loss: 14.9714 - val_mean_absolute_error: 3.0437\n",
      "\n",
      "Epoch 00052: val_loss improved from 17.81030 to 14.97142, saving model to RNN_best\n",
      "Epoch 53/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.5235 - mean_absolute_error: 7.2742 - val_loss: 21.9164 - val_mean_absolute_error: 3.9995\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 14.97142\n",
      "Epoch 54/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 107.3786 - mean_absolute_error: 8.2600 - val_loss: 12.7039 - val_mean_absolute_error: 2.5504\n",
      "\n",
      "Epoch 00054: val_loss improved from 14.97142 to 12.70388, saving model to RNN_best\n",
      "Epoch 55/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 88.6079 - mean_absolute_error: 7.3759 - val_loss: 14.0945 - val_mean_absolute_error: 2.7498\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 12.70388\n",
      "Epoch 56/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.4964 - mean_absolute_error: 7.7749 - val_loss: 15.9399 - val_mean_absolute_error: 3.0504\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 12.70388\n",
      "Epoch 57/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 105.5072 - mean_absolute_error: 7.8741 - val_loss: 10.7963 - val_mean_absolute_error: 2.3795\n",
      "\n",
      "Epoch 00057: val_loss improved from 12.70388 to 10.79627, saving model to RNN_best\n",
      "Epoch 58/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 92.9820 - mean_absolute_error: 7.5889 - val_loss: 11.7289 - val_mean_absolute_error: 2.4415\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 10.79627\n",
      "Epoch 59/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 97.2779 - mean_absolute_error: 7.9965 - val_loss: 18.6415 - val_mean_absolute_error: 3.5993\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 10.79627\n",
      "Epoch 60/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 94.1713 - mean_absolute_error: 7.7757 - val_loss: 11.1758 - val_mean_absolute_error: 2.4692\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 10.79627\n",
      "Epoch 61/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.6606 - mean_absolute_error: 7.3759 - val_loss: 10.9044 - val_mean_absolute_error: 2.5366\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 10.79627\n",
      "Epoch 62/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.6126 - mean_absolute_error: 7.8176 - val_loss: 11.9761 - val_mean_absolute_error: 2.6419\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 10.79627\n",
      "Epoch 63/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 102.6754 - mean_absolute_error: 8.1194 - val_loss: 14.0374 - val_mean_absolute_error: 3.0437\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 10.79627\n",
      "Epoch 64/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.3454 - mean_absolute_error: 7.1206 - val_loss: 18.4644 - val_mean_absolute_error: 3.6341\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 10.79627\n",
      "Epoch 65/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.8826 - mean_absolute_error: 7.1411 - val_loss: 12.4665 - val_mean_absolute_error: 2.6921\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 10.79627\n",
      "Epoch 66/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 90.2568 - mean_absolute_error: 7.5120 - val_loss: 12.1847 - val_mean_absolute_error: 2.7021\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 10.79627\n",
      "Epoch 67/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.3610 - mean_absolute_error: 7.2501 - val_loss: 14.0902 - val_mean_absolute_error: 3.0978\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 10.79627\n",
      "Epoch 68/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.4427 - mean_absolute_error: 7.1558 - val_loss: 13.8366 - val_mean_absolute_error: 2.6656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss did not improve from 10.79627\n",
      "Epoch 69/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 90.1876 - mean_absolute_error: 7.4328 - val_loss: 14.2918 - val_mean_absolute_error: 3.0910\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 10.79627\n",
      "Epoch 70/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 94.5433 - mean_absolute_error: 7.7328 - val_loss: 12.0704 - val_mean_absolute_error: 2.7059\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 10.79627\n",
      "Epoch 71/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 97.2666 - mean_absolute_error: 7.6506 - val_loss: 10.8933 - val_mean_absolute_error: 2.5137\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 10.79627\n",
      "Epoch 72/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.5834 - mean_absolute_error: 7.5726 - val_loss: 13.5788 - val_mean_absolute_error: 3.0096\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 10.79627\n",
      "Epoch 73/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 92.3470 - mean_absolute_error: 7.6208 - val_loss: 11.0148 - val_mean_absolute_error: 2.5032\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 10.79627\n",
      "Epoch 74/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 89.8734 - mean_absolute_error: 7.4938 - val_loss: 9.2909 - val_mean_absolute_error: 2.1438\n",
      "\n",
      "Epoch 00074: val_loss improved from 10.79627 to 9.29092, saving model to RNN_best\n",
      "Epoch 75/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.6149 - mean_absolute_error: 7.3104 - val_loss: 9.8775 - val_mean_absolute_error: 2.3516\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 9.29092\n",
      "Epoch 76/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 88.4273 - mean_absolute_error: 7.3817 - val_loss: 11.5755 - val_mean_absolute_error: 2.7455\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 9.29092\n",
      "Epoch 77/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.0633 - mean_absolute_error: 6.7880 - val_loss: 9.4050 - val_mean_absolute_error: 2.1913\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 9.29092\n",
      "Epoch 78/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.3672 - mean_absolute_error: 7.1593 - val_loss: 16.1603 - val_mean_absolute_error: 3.2969\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 9.29092\n",
      "Epoch 79/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 89.5010 - mean_absolute_error: 7.5333 - val_loss: 12.4247 - val_mean_absolute_error: 2.7887\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 9.29092\n",
      "Epoch 80/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 97.0411 - mean_absolute_error: 7.7893 - val_loss: 12.6235 - val_mean_absolute_error: 2.5070\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 9.29092\n",
      "Epoch 81/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 94.1316 - mean_absolute_error: 7.6539 - val_loss: 13.8295 - val_mean_absolute_error: 2.8286\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 9.29092\n",
      "Epoch 82/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 86.3314 - mean_absolute_error: 7.3339 - val_loss: 11.7416 - val_mean_absolute_error: 2.6821\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 9.29092\n",
      "Epoch 83/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.7570 - mean_absolute_error: 7.4565 - val_loss: 15.3133 - val_mean_absolute_error: 3.2951\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 9.29092\n",
      "Epoch 84/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.2354 - mean_absolute_error: 7.4965 - val_loss: 10.9676 - val_mean_absolute_error: 2.5762\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 9.29092\n",
      "Epoch 85/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.6391 - mean_absolute_error: 7.7066 - val_loss: 10.2991 - val_mean_absolute_error: 2.4170\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 9.29092\n",
      "Epoch 86/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.8725 - mean_absolute_error: 7.1817 - val_loss: 11.7231 - val_mean_absolute_error: 2.6053\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 9.29092\n",
      "Epoch 87/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.4700 - mean_absolute_error: 7.0713 - val_loss: 10.4506 - val_mean_absolute_error: 2.3660\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 9.29092\n",
      "Epoch 88/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.0263 - mean_absolute_error: 7.2101 - val_loss: 13.9013 - val_mean_absolute_error: 2.9727\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 9.29092\n",
      "Epoch 89/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.1546 - mean_absolute_error: 7.2986 - val_loss: 15.5557 - val_mean_absolute_error: 3.2117\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 9.29092\n",
      "Epoch 90/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.3745 - mean_absolute_error: 7.9946 - val_loss: 10.8542 - val_mean_absolute_error: 2.3762\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 9.29092\n",
      "Epoch 91/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 93.8441 - mean_absolute_error: 7.6023 - val_loss: 13.3332 - val_mean_absolute_error: 2.7814\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 9.29092\n",
      "Epoch 92/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 88.3447 - mean_absolute_error: 7.5166 - val_loss: 11.2321 - val_mean_absolute_error: 2.6600\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 9.29092\n",
      "Epoch 93/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.5365 - mean_absolute_error: 7.3520 - val_loss: 12.0355 - val_mean_absolute_error: 2.6562\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 9.29092\n",
      "Epoch 94/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 94.2904 - mean_absolute_error: 7.7458 - val_loss: 12.0024 - val_mean_absolute_error: 2.6555\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 9.29092\n",
      "Epoch 95/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.9441 - mean_absolute_error: 7.3199 - val_loss: 11.7536 - val_mean_absolute_error: 2.5613\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 9.29092\n",
      "Epoch 96/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.5133 - mean_absolute_error: 7.1158 - val_loss: 14.3129 - val_mean_absolute_error: 2.9606\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 9.29092\n",
      "Epoch 97/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.8466 - mean_absolute_error: 7.4255 - val_loss: 15.5417 - val_mean_absolute_error: 3.2251\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 9.29092\n",
      "Epoch 98/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.8298 - mean_absolute_error: 7.8214 - val_loss: 12.3660 - val_mean_absolute_error: 2.7510\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 9.29092\n",
      "Epoch 99/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.6879 - mean_absolute_error: 7.2791 - val_loss: 11.6601 - val_mean_absolute_error: 2.4415\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 9.29092\n",
      "Epoch 100/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.2735 - mean_absolute_error: 7.2099 - val_loss: 13.2396 - val_mean_absolute_error: 2.8899\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 9.29092\n",
      "Epoch 101/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 94.6676 - mean_absolute_error: 7.6488 - val_loss: 12.5550 - val_mean_absolute_error: 2.6711\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 9.29092\n",
      "Epoch 102/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 95.7634 - mean_absolute_error: 7.6459 - val_loss: 11.5093 - val_mean_absolute_error: 2.7271\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 9.29092\n",
      "Epoch 103/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 94.7774 - mean_absolute_error: 7.6453 - val_loss: 14.9648 - val_mean_absolute_error: 3.1529\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 9.29092\n",
      "Epoch 104/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.2910 - mean_absolute_error: 7.5314 - val_loss: 11.5333 - val_mean_absolute_error: 2.5403\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 9.29092\n",
      "Epoch 105/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.2646 - mean_absolute_error: 7.5194 - val_loss: 9.6810 - val_mean_absolute_error: 2.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_loss did not improve from 9.29092\n",
      "Epoch 106/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 89.5416 - mean_absolute_error: 7.6127 - val_loss: 20.1641 - val_mean_absolute_error: 3.8083\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 9.29092\n",
      "Epoch 107/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.9564 - mean_absolute_error: 7.9096 - val_loss: 9.0757 - val_mean_absolute_error: 2.2239\n",
      "\n",
      "Epoch 00107: val_loss improved from 9.29092 to 9.07568, saving model to RNN_best\n",
      "Epoch 108/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.2380 - mean_absolute_error: 7.2593 - val_loss: 9.2648 - val_mean_absolute_error: 2.2994\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 9.07568\n",
      "Epoch 109/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.9054 - mean_absolute_error: 7.3816 - val_loss: 9.1681 - val_mean_absolute_error: 2.2452\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 9.07568\n",
      "Epoch 110/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.8198 - mean_absolute_error: 7.2184 - val_loss: 11.0775 - val_mean_absolute_error: 2.5912\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 9.07568\n",
      "Epoch 111/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 100.7319 - mean_absolute_error: 7.8049 - val_loss: 10.1302 - val_mean_absolute_error: 2.2499\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 9.07568\n",
      "Epoch 112/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 97.7012 - mean_absolute_error: 7.7237 - val_loss: 10.0651 - val_mean_absolute_error: 2.3633\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 9.07568\n",
      "Epoch 113/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.2404 - mean_absolute_error: 7.0324 - val_loss: 13.0809 - val_mean_absolute_error: 2.8549\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 9.07568\n",
      "Epoch 114/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 90.9829 - mean_absolute_error: 7.5887 - val_loss: 10.2695 - val_mean_absolute_error: 2.3863\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 9.07568\n",
      "Epoch 115/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 88.8828 - mean_absolute_error: 7.4990 - val_loss: 12.2629 - val_mean_absolute_error: 2.7482\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 9.07568\n",
      "Epoch 116/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.3937 - mean_absolute_error: 7.1774 - val_loss: 10.8957 - val_mean_absolute_error: 2.4790\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 9.07568\n",
      "Epoch 117/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.0240 - mean_absolute_error: 7.4392 - val_loss: 10.0514 - val_mean_absolute_error: 2.3012\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 9.07568\n",
      "Epoch 118/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 88.2290 - mean_absolute_error: 7.3967 - val_loss: 13.8706 - val_mean_absolute_error: 3.0561\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 9.07568\n",
      "Epoch 119/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 93.3125 - mean_absolute_error: 7.5937 - val_loss: 9.8902 - val_mean_absolute_error: 2.3048\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 9.07568\n",
      "Epoch 120/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 88.2954 - mean_absolute_error: 7.3029 - val_loss: 12.0612 - val_mean_absolute_error: 2.7187\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 9.07568\n",
      "Epoch 121/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.5506 - mean_absolute_error: 7.5020 - val_loss: 9.7089 - val_mean_absolute_error: 2.2657\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 9.07568\n",
      "Epoch 122/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.4358 - mean_absolute_error: 7.2302 - val_loss: 12.3262 - val_mean_absolute_error: 2.7733\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 9.07568\n",
      "Epoch 123/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.9746 - mean_absolute_error: 7.2818 - val_loss: 8.7643 - val_mean_absolute_error: 2.1339\n",
      "\n",
      "Epoch 00123: val_loss improved from 9.07568 to 8.76432, saving model to RNN_best\n",
      "Epoch 124/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.2941 - mean_absolute_error: 7.1488 - val_loss: 14.1571 - val_mean_absolute_error: 3.0904\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 8.76432\n",
      "Epoch 125/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.9031 - mean_absolute_error: 7.4144 - val_loss: 14.9806 - val_mean_absolute_error: 3.1537\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 8.76432\n",
      "Epoch 126/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.5941 - mean_absolute_error: 7.2536 - val_loss: 8.4936 - val_mean_absolute_error: 2.0172\n",
      "\n",
      "Epoch 00126: val_loss improved from 8.76432 to 8.49363, saving model to RNN_best\n",
      "Epoch 127/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 99.1033 - mean_absolute_error: 7.9375 - val_loss: 22.3426 - val_mean_absolute_error: 4.1397\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 8.49363\n",
      "Epoch 128/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 86.7492 - mean_absolute_error: 7.3160 - val_loss: 14.0838 - val_mean_absolute_error: 3.0397\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 8.49363\n",
      "Epoch 129/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 96.0336 - mean_absolute_error: 7.8523 - val_loss: 18.7721 - val_mean_absolute_error: 3.6570\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 8.49363\n",
      "Epoch 130/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 98.3722 - mean_absolute_error: 7.7873 - val_loss: 11.1448 - val_mean_absolute_error: 2.5263\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 8.49363\n",
      "Epoch 131/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.4063 - mean_absolute_error: 7.2394 - val_loss: 11.5206 - val_mean_absolute_error: 2.4947\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 8.49363\n",
      "Epoch 132/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 94.5601 - mean_absolute_error: 7.6686 - val_loss: 13.5646 - val_mean_absolute_error: 2.9958\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 8.49363\n",
      "Epoch 133/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.1816 - mean_absolute_error: 6.9995 - val_loss: 11.4885 - val_mean_absolute_error: 2.6483\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 8.49363\n",
      "Epoch 134/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.5365 - mean_absolute_error: 6.7129 - val_loss: 9.1801 - val_mean_absolute_error: 2.2704\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 8.49363\n",
      "Epoch 135/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.9770 - mean_absolute_error: 6.8837 - val_loss: 8.4769 - val_mean_absolute_error: 1.9550\n",
      "\n",
      "Epoch 00135: val_loss improved from 8.49363 to 8.47695, saving model to RNN_best\n",
      "Epoch 136/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.9479 - mean_absolute_error: 7.4176 - val_loss: 11.3067 - val_mean_absolute_error: 2.6827\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 8.47695\n",
      "Epoch 137/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.5878 - mean_absolute_error: 7.1417 - val_loss: 12.8964 - val_mean_absolute_error: 3.0168\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 8.47695\n",
      "Epoch 138/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.0315 - mean_absolute_error: 7.0904 - val_loss: 10.3697 - val_mean_absolute_error: 2.4951\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 8.47695\n",
      "Epoch 139/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.1022 - mean_absolute_error: 6.8257 - val_loss: 10.3960 - val_mean_absolute_error: 2.3743\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 8.47695\n",
      "Epoch 140/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 88.6217 - mean_absolute_error: 7.5589 - val_loss: 8.9694 - val_mean_absolute_error: 2.2577\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 8.47695\n",
      "Epoch 141/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.8359 - mean_absolute_error: 7.0796 - val_loss: 14.2195 - val_mean_absolute_error: 3.1246\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 8.47695\n",
      "Epoch 142/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 94.2544 - mean_absolute_error: 7.6417 - val_loss: 11.2454 - val_mean_absolute_error: 2.7118\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 8.47695\n",
      "Epoch 143/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.8763 - mean_absolute_error: 7.1474 - val_loss: 12.4454 - val_mean_absolute_error: 2.8967\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 8.47695\n",
      "Epoch 144/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.9256 - mean_absolute_error: 7.1325 - val_loss: 8.6835 - val_mean_absolute_error: 2.1807\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 8.47695\n",
      "Epoch 145/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.1957 - mean_absolute_error: 7.0914 - val_loss: 8.7131 - val_mean_absolute_error: 2.2830\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 8.47695\n",
      "Epoch 146/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.9607 - mean_absolute_error: 7.0693 - val_loss: 9.0727 - val_mean_absolute_error: 2.3321\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 8.47695\n",
      "Epoch 147/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.4573 - mean_absolute_error: 7.1798 - val_loss: 8.7123 - val_mean_absolute_error: 2.1817\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 8.47695\n",
      "Epoch 148/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.6752 - mean_absolute_error: 7.2064 - val_loss: 12.0483 - val_mean_absolute_error: 2.7654\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 8.47695\n",
      "Epoch 149/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.6565 - mean_absolute_error: 7.3738 - val_loss: 9.7067 - val_mean_absolute_error: 2.3968\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 8.47695\n",
      "Epoch 150/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 86.6773 - mean_absolute_error: 7.3780 - val_loss: 13.7293 - val_mean_absolute_error: 3.0996\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 8.47695\n",
      "Epoch 151/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.1816 - mean_absolute_error: 6.7534 - val_loss: 7.8080 - val_mean_absolute_error: 1.8885\n",
      "\n",
      "Epoch 00151: val_loss improved from 8.47695 to 7.80805, saving model to RNN_best\n",
      "Epoch 152/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 68.8703 - mean_absolute_error: 6.5955 - val_loss: 9.9930 - val_mean_absolute_error: 2.4605\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 7.80805\n",
      "Epoch 153/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.6548 - mean_absolute_error: 7.1233 - val_loss: 12.0312 - val_mean_absolute_error: 2.8113\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 7.80805\n",
      "Epoch 154/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.9754 - mean_absolute_error: 6.9306 - val_loss: 9.3388 - val_mean_absolute_error: 2.3959\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 7.80805\n",
      "Epoch 155/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.4365 - mean_absolute_error: 6.9037 - val_loss: 14.3954 - val_mean_absolute_error: 3.2510\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 7.80805\n",
      "Epoch 156/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.9541 - mean_absolute_error: 7.0516 - val_loss: 9.3071 - val_mean_absolute_error: 2.3174\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 7.80805\n",
      "Epoch 157/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.4541 - mean_absolute_error: 7.1798 - val_loss: 10.2781 - val_mean_absolute_error: 2.4876\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 7.80805\n",
      "Epoch 158/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.1985 - mean_absolute_error: 6.5196 - val_loss: 9.6385 - val_mean_absolute_error: 2.3698\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 7.80805\n",
      "Epoch 159/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 76.8753 - mean_absolute_error: 6.8200 - val_loss: 7.1754 - val_mean_absolute_error: 1.8988\n",
      "\n",
      "Epoch 00159: val_loss improved from 7.80805 to 7.17537, saving model to RNN_best\n",
      "Epoch 160/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 90.0026 - mean_absolute_error: 7.3429 - val_loss: 12.1829 - val_mean_absolute_error: 2.6890\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 7.17537\n",
      "Epoch 161/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.5118 - mean_absolute_error: 7.4001 - val_loss: 14.8145 - val_mean_absolute_error: 3.0326\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 7.17537\n",
      "Epoch 162/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.3890 - mean_absolute_error: 7.1555 - val_loss: 27.0234 - val_mean_absolute_error: 4.4318\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 7.17537\n",
      "Epoch 163/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.1939 - mean_absolute_error: 7.2455 - val_loss: 12.0120 - val_mean_absolute_error: 2.8873\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 7.17537\n",
      "Epoch 164/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.4202 - mean_absolute_error: 6.9953 - val_loss: 8.1745 - val_mean_absolute_error: 2.0069\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 7.17537\n",
      "Epoch 165/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 89.1676 - mean_absolute_error: 7.4550 - val_loss: 8.1857 - val_mean_absolute_error: 1.9458\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 7.17537\n",
      "Epoch 166/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.3903 - mean_absolute_error: 6.8611 - val_loss: 12.6465 - val_mean_absolute_error: 2.9518\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 7.17537\n",
      "Epoch 167/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.5397 - mean_absolute_error: 7.0835 - val_loss: 10.7368 - val_mean_absolute_error: 2.6389\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 7.17537\n",
      "Epoch 168/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.3702 - mean_absolute_error: 6.9375 - val_loss: 8.5830 - val_mean_absolute_error: 2.0833\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 7.17537\n",
      "Epoch 169/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 73.2458 - mean_absolute_error: 6.8683 - val_loss: 9.4398 - val_mean_absolute_error: 2.3516\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 7.17537\n",
      "Epoch 170/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 89.6340 - mean_absolute_error: 7.3891 - val_loss: 11.4975 - val_mean_absolute_error: 2.7734\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 7.17537\n",
      "Epoch 171/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.8741 - mean_absolute_error: 7.1803 - val_loss: 7.4290 - val_mean_absolute_error: 2.0201\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 7.17537\n",
      "Epoch 172/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.8848 - mean_absolute_error: 7.0391 - val_loss: 12.9256 - val_mean_absolute_error: 2.9560\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 7.17537\n",
      "Epoch 173/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.5533 - mean_absolute_error: 7.2398 - val_loss: 7.5692 - val_mean_absolute_error: 1.9768\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 7.17537\n",
      "Epoch 174/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.6121 - mean_absolute_error: 7.0556 - val_loss: 11.7610 - val_mean_absolute_error: 2.8230\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 7.17537\n",
      "Epoch 175/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.0749 - mean_absolute_error: 7.0691 - val_loss: 9.7945 - val_mean_absolute_error: 2.4770\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 7.17537\n",
      "Epoch 176/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.9566 - mean_absolute_error: 7.2203 - val_loss: 8.4477 - val_mean_absolute_error: 2.1463\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 7.17537\n",
      "Epoch 177/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 76.7665 - mean_absolute_error: 6.6800 - val_loss: 10.6993 - val_mean_absolute_error: 2.6172\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 7.17537\n",
      "Epoch 178/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.2533 - mean_absolute_error: 6.9288 - val_loss: 7.9258 - val_mean_absolute_error: 2.1900\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 7.17537\n",
      "Epoch 179/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 75.3393 - mean_absolute_error: 6.8526 - val_loss: 15.0400 - val_mean_absolute_error: 3.1825\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 7.17537\n",
      "Epoch 180/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.9223 - mean_absolute_error: 7.2394 - val_loss: 13.6330 - val_mean_absolute_error: 3.0572\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 7.17537\n",
      "Epoch 181/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.9701 - mean_absolute_error: 6.8412 - val_loss: 8.5471 - val_mean_absolute_error: 2.2626\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 7.17537\n",
      "Epoch 182/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.3212 - mean_absolute_error: 6.7124 - val_loss: 9.5521 - val_mean_absolute_error: 2.2715\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 7.17537\n",
      "Epoch 183/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.0611 - mean_absolute_error: 7.4017 - val_loss: 8.6819 - val_mean_absolute_error: 2.2585\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 7.17537\n",
      "Epoch 184/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.5340 - mean_absolute_error: 7.4378 - val_loss: 14.9363 - val_mean_absolute_error: 3.2714\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 7.17537\n",
      "Epoch 185/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.2051 - mean_absolute_error: 7.0990 - val_loss: 9.8279 - val_mean_absolute_error: 2.3932\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 7.17537\n",
      "Epoch 186/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 77.4417 - mean_absolute_error: 6.9160 - val_loss: 12.2316 - val_mean_absolute_error: 2.8411\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 7.17537\n",
      "Epoch 187/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 89.6175 - mean_absolute_error: 7.4422 - val_loss: 10.9654 - val_mean_absolute_error: 2.6305\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 7.17537\n",
      "Epoch 188/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.8187 - mean_absolute_error: 7.0741 - val_loss: 7.5531 - val_mean_absolute_error: 1.9150\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 7.17537\n",
      "Epoch 189/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.4576 - mean_absolute_error: 6.7447 - val_loss: 8.6747 - val_mean_absolute_error: 2.0671\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 7.17537\n",
      "Epoch 190/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.8118 - mean_absolute_error: 7.3932 - val_loss: 14.0142 - val_mean_absolute_error: 3.1299\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 7.17537\n",
      "Epoch 191/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.2780 - mean_absolute_error: 7.1314 - val_loss: 7.4698 - val_mean_absolute_error: 1.9127\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 7.17537\n",
      "Epoch 192/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.0809 - mean_absolute_error: 7.1843 - val_loss: 9.7036 - val_mean_absolute_error: 2.4586\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 7.17537\n",
      "Epoch 193/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.6196 - mean_absolute_error: 7.0706 - val_loss: 9.0460 - val_mean_absolute_error: 2.2269\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 7.17537\n",
      "Epoch 194/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.4243 - mean_absolute_error: 6.9237 - val_loss: 12.3461 - val_mean_absolute_error: 2.8592\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 7.17537\n",
      "Epoch 195/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.6145 - mean_absolute_error: 7.0874 - val_loss: 8.3437 - val_mean_absolute_error: 2.1183\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 7.17537\n",
      "Epoch 196/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.1470 - mean_absolute_error: 7.3443 - val_loss: 11.7237 - val_mean_absolute_error: 2.7225\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 7.17537\n",
      "Epoch 197/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.3246 - mean_absolute_error: 7.1544 - val_loss: 7.0953 - val_mean_absolute_error: 1.8595\n",
      "\n",
      "Epoch 00197: val_loss improved from 7.17537 to 7.09528, saving model to RNN_best\n",
      "Epoch 198/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.4008 - mean_absolute_error: 6.9360 - val_loss: 6.8492 - val_mean_absolute_error: 1.9071\n",
      "\n",
      "Epoch 00198: val_loss improved from 7.09528 to 6.84923, saving model to RNN_best\n",
      "Epoch 199/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 73.5144 - mean_absolute_error: 6.8350 - val_loss: 8.9951 - val_mean_absolute_error: 2.2935\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 6.84923\n",
      "Epoch 200/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 92.8135 - mean_absolute_error: 7.5590 - val_loss: 9.6151 - val_mean_absolute_error: 2.4040\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 6.84923\n",
      "Epoch 201/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.6143 - mean_absolute_error: 6.5902 - val_loss: 7.2942 - val_mean_absolute_error: 1.9355\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 6.84923\n",
      "Epoch 202/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.1196 - mean_absolute_error: 7.1467 - val_loss: 18.6065 - val_mean_absolute_error: 3.7855\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 6.84923\n",
      "Epoch 203/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.0753 - mean_absolute_error: 7.1110 - val_loss: 10.5352 - val_mean_absolute_error: 2.5770\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 6.84923\n",
      "Epoch 204/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 89.1725 - mean_absolute_error: 7.4926 - val_loss: 8.5738 - val_mean_absolute_error: 2.2375\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 6.84923\n",
      "Epoch 205/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 64.3106 - mean_absolute_error: 6.2294 - val_loss: 9.7099 - val_mean_absolute_error: 2.4541\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 6.84923\n",
      "Epoch 206/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.5727 - mean_absolute_error: 6.6328 - val_loss: 9.6266 - val_mean_absolute_error: 2.4350\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 6.84923\n",
      "Epoch 207/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.2573 - mean_absolute_error: 7.1028 - val_loss: 10.7241 - val_mean_absolute_error: 2.5859\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 6.84923\n",
      "Epoch 208/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.0353 - mean_absolute_error: 7.0062 - val_loss: 9.5971 - val_mean_absolute_error: 2.4926\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 6.84923\n",
      "Epoch 209/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 74.3428 - mean_absolute_error: 7.0268 - val_loss: 9.1222 - val_mean_absolute_error: 2.3517\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 6.84923\n",
      "Epoch 210/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.6883 - mean_absolute_error: 7.2182 - val_loss: 8.1295 - val_mean_absolute_error: 2.0597\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 6.84923\n",
      "Epoch 211/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.6932 - mean_absolute_error: 6.9062 - val_loss: 6.5670 - val_mean_absolute_error: 1.8083\n",
      "\n",
      "Epoch 00211: val_loss improved from 6.84923 to 6.56699, saving model to RNN_best\n",
      "Epoch 212/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.3510 - mean_absolute_error: 6.8472 - val_loss: 7.0348 - val_mean_absolute_error: 1.8549\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 6.56699\n",
      "Epoch 213/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 74.8882 - mean_absolute_error: 6.7834 - val_loss: 7.1487 - val_mean_absolute_error: 1.9381\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 6.56699\n",
      "Epoch 214/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 86.6969 - mean_absolute_error: 7.5350 - val_loss: 7.4228 - val_mean_absolute_error: 2.0758\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 6.56699\n",
      "Epoch 215/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 85.3159 - mean_absolute_error: 7.2403 - val_loss: 9.0442 - val_mean_absolute_error: 2.4220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00215: val_loss did not improve from 6.56699\n",
      "Epoch 216/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 96.2375 - mean_absolute_error: 7.6231 - val_loss: 18.8497 - val_mean_absolute_error: 3.8264\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 6.56699\n",
      "Epoch 217/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.3658 - mean_absolute_error: 7.0306 - val_loss: 9.6923 - val_mean_absolute_error: 2.0843\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 6.56699\n",
      "Epoch 218/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.2189 - mean_absolute_error: 6.8946 - val_loss: 9.0284 - val_mean_absolute_error: 2.1709\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 6.56699\n",
      "Epoch 219/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 90.5760 - mean_absolute_error: 7.4709 - val_loss: 8.9590 - val_mean_absolute_error: 2.1839\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 6.56699\n",
      "Epoch 220/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 86.8024 - mean_absolute_error: 7.0500 - val_loss: 10.1707 - val_mean_absolute_error: 2.3610\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 6.56699\n",
      "Epoch 221/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.8786 - mean_absolute_error: 6.9159 - val_loss: 9.2383 - val_mean_absolute_error: 2.2745\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 6.56699\n",
      "Epoch 222/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.2180 - mean_absolute_error: 6.9191 - val_loss: 7.3177 - val_mean_absolute_error: 1.9549\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 6.56699\n",
      "Epoch 223/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 89.3616 - mean_absolute_error: 7.4973 - val_loss: 16.4156 - val_mean_absolute_error: 3.4852\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 6.56699\n",
      "Epoch 224/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.7324 - mean_absolute_error: 7.0124 - val_loss: 8.2898 - val_mean_absolute_error: 2.0399\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 6.56699\n",
      "Epoch 225/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.4851 - mean_absolute_error: 7.0148 - val_loss: 9.7483 - val_mean_absolute_error: 2.4264\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 6.56699\n",
      "Epoch 226/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 87.5994 - mean_absolute_error: 7.3209 - val_loss: 8.3067 - val_mean_absolute_error: 2.1437\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 6.56699\n",
      "Epoch 227/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 76.9915 - mean_absolute_error: 6.9553 - val_loss: 10.5958 - val_mean_absolute_error: 2.6136\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 6.56699\n",
      "Epoch 228/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.9384 - mean_absolute_error: 7.2817 - val_loss: 10.1096 - val_mean_absolute_error: 2.4516\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 6.56699\n",
      "Epoch 229/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.3411 - mean_absolute_error: 6.6928 - val_loss: 11.6705 - val_mean_absolute_error: 2.8001\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 6.56699\n",
      "Epoch 230/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.6541 - mean_absolute_error: 7.1345 - val_loss: 8.0880 - val_mean_absolute_error: 2.0975\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 6.56699\n",
      "Epoch 231/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.1598 - mean_absolute_error: 6.6740 - val_loss: 11.1176 - val_mean_absolute_error: 2.6640\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 6.56699\n",
      "Epoch 232/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.7486 - mean_absolute_error: 7.0895 - val_loss: 8.4229 - val_mean_absolute_error: 2.2322\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 6.56699\n",
      "Epoch 233/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.3934 - mean_absolute_error: 7.2283 - val_loss: 8.5479 - val_mean_absolute_error: 2.1177\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 6.56699\n",
      "Epoch 234/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.0448 - mean_absolute_error: 7.0157 - val_loss: 7.1448 - val_mean_absolute_error: 1.9551\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 6.56699\n",
      "Epoch 235/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.1441 - mean_absolute_error: 7.1187 - val_loss: 7.4352 - val_mean_absolute_error: 2.0495\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 6.56699\n",
      "Epoch 236/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.5289 - mean_absolute_error: 6.5866 - val_loss: 7.3807 - val_mean_absolute_error: 2.0662\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 6.56699\n",
      "Epoch 237/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.6928 - mean_absolute_error: 6.5358 - val_loss: 10.3782 - val_mean_absolute_error: 2.5907\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 6.56699\n",
      "Epoch 238/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.4719 - mean_absolute_error: 7.2218 - val_loss: 8.6974 - val_mean_absolute_error: 2.2109\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 6.56699\n",
      "Epoch 239/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.1476 - mean_absolute_error: 7.2032 - val_loss: 8.3358 - val_mean_absolute_error: 2.1484\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 6.56699\n",
      "Epoch 240/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.7916 - mean_absolute_error: 7.0729 - val_loss: 9.5929 - val_mean_absolute_error: 2.3024\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 6.56699\n",
      "Epoch 241/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.0517 - mean_absolute_error: 6.9091 - val_loss: 13.2094 - val_mean_absolute_error: 3.0643\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 6.56699\n",
      "Epoch 242/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.7061 - mean_absolute_error: 7.0473 - val_loss: 9.4420 - val_mean_absolute_error: 2.2473\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 6.56699\n",
      "Epoch 243/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.4734 - mean_absolute_error: 7.0247 - val_loss: 8.8701 - val_mean_absolute_error: 2.2490\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 6.56699\n",
      "Epoch 244/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.0945 - mean_absolute_error: 7.1437 - val_loss: 17.3724 - val_mean_absolute_error: 3.4816\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 6.56699\n",
      "Epoch 245/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.0789 - mean_absolute_error: 6.7829 - val_loss: 8.8122 - val_mean_absolute_error: 2.3614\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 6.56699\n",
      "Epoch 246/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.8909 - mean_absolute_error: 7.1542 - val_loss: 8.1592 - val_mean_absolute_error: 2.3082\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 6.56699\n",
      "Epoch 247/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.0050 - mean_absolute_error: 6.8953 - val_loss: 9.6524 - val_mean_absolute_error: 2.5647\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 6.56699\n",
      "Epoch 248/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 76.3137 - mean_absolute_error: 6.7744 - val_loss: 7.2001 - val_mean_absolute_error: 2.0655\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 6.56699\n",
      "Epoch 249/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.9503 - mean_absolute_error: 6.7772 - val_loss: 7.9866 - val_mean_absolute_error: 2.1490\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 6.56699\n",
      "Epoch 250/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.7667 - mean_absolute_error: 7.1579 - val_loss: 7.6221 - val_mean_absolute_error: 2.0025\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 6.56699\n",
      "Epoch 251/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.8771 - mean_absolute_error: 7.1204 - val_loss: 7.4917 - val_mean_absolute_error: 1.9919\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 6.56699\n",
      "Epoch 252/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.1490 - mean_absolute_error: 6.5875 - val_loss: 15.8464 - val_mean_absolute_error: 3.3741\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 6.56699\n",
      "Epoch 253/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 85.3840 - mean_absolute_error: 7.1743 - val_loss: 7.9392 - val_mean_absolute_error: 2.0444\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 6.56699\n",
      "Epoch 254/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 74.7378 - mean_absolute_error: 6.8263 - val_loss: 7.6345 - val_mean_absolute_error: 1.9671\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 6.56699\n",
      "Epoch 255/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.0731 - mean_absolute_error: 6.7598 - val_loss: 8.9351 - val_mean_absolute_error: 2.1913\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 6.56699\n",
      "Epoch 256/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.1230 - mean_absolute_error: 7.2427 - val_loss: 6.8174 - val_mean_absolute_error: 1.9777\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 6.56699\n",
      "Epoch 257/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.1047 - mean_absolute_error: 6.8490 - val_loss: 18.6591 - val_mean_absolute_error: 3.7270\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 6.56699\n",
      "Epoch 258/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.1297 - mean_absolute_error: 7.3015 - val_loss: 8.3499 - val_mean_absolute_error: 2.2465\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 6.56699\n",
      "Epoch 259/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.1644 - mean_absolute_error: 6.9708 - val_loss: 6.8583 - val_mean_absolute_error: 1.9020\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 6.56699\n",
      "Epoch 260/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.8075 - mean_absolute_error: 6.7392 - val_loss: 6.8464 - val_mean_absolute_error: 1.8950\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 6.56699\n",
      "Epoch 261/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.4620 - mean_absolute_error: 7.0090 - val_loss: 6.5954 - val_mean_absolute_error: 1.9076\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 6.56699\n",
      "Epoch 262/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.7616 - mean_absolute_error: 6.9060 - val_loss: 7.3689 - val_mean_absolute_error: 2.0280\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 6.56699\n",
      "Epoch 263/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.2899 - mean_absolute_error: 6.8010 - val_loss: 8.1607 - val_mean_absolute_error: 2.2184\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 6.56699\n",
      "Epoch 264/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.6051 - mean_absolute_error: 7.2586 - val_loss: 7.9056 - val_mean_absolute_error: 2.0864\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 6.56699\n",
      "Epoch 265/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.9264 - mean_absolute_error: 6.9399 - val_loss: 9.5347 - val_mean_absolute_error: 2.4709\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 6.56699\n",
      "Epoch 266/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.7462 - mean_absolute_error: 6.9702 - val_loss: 11.8716 - val_mean_absolute_error: 2.7715\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 6.56699\n",
      "Epoch 267/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.2358 - mean_absolute_error: 7.1841 - val_loss: 6.3254 - val_mean_absolute_error: 1.7593\n",
      "\n",
      "Epoch 00267: val_loss improved from 6.56699 to 6.32535, saving model to RNN_best\n",
      "Epoch 268/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.1040 - mean_absolute_error: 6.8685 - val_loss: 13.7645 - val_mean_absolute_error: 3.1620\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 6.32535\n",
      "Epoch 269/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.2316 - mean_absolute_error: 7.1536 - val_loss: 7.3298 - val_mean_absolute_error: 1.9329\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 6.32535\n",
      "Epoch 270/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 92.2274 - mean_absolute_error: 7.5889 - val_loss: 9.4002 - val_mean_absolute_error: 2.4358\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 6.32535\n",
      "Epoch 271/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 67.4746 - mean_absolute_error: 6.5087 - val_loss: 7.6218 - val_mean_absolute_error: 2.0334\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 6.32535\n",
      "Epoch 272/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.4466 - mean_absolute_error: 6.7775 - val_loss: 8.9966 - val_mean_absolute_error: 2.0849\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 6.32535\n",
      "Epoch 273/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 74.0257 - mean_absolute_error: 6.8960 - val_loss: 8.2055 - val_mean_absolute_error: 2.0812\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 6.32535\n",
      "Epoch 274/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.5717 - mean_absolute_error: 7.0326 - val_loss: 16.2888 - val_mean_absolute_error: 3.4050\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 6.32535\n",
      "Epoch 275/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.9779 - mean_absolute_error: 7.0976 - val_loss: 6.7605 - val_mean_absolute_error: 1.8983\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 6.32535\n",
      "Epoch 276/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.5673 - mean_absolute_error: 7.0010 - val_loss: 7.7764 - val_mean_absolute_error: 2.0061\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 6.32535\n",
      "Epoch 277/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.0549 - mean_absolute_error: 7.0320 - val_loss: 7.8126 - val_mean_absolute_error: 2.0675\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 6.32535\n",
      "Epoch 278/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.0554 - mean_absolute_error: 7.1264 - val_loss: 8.4096 - val_mean_absolute_error: 2.2243\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 6.32535\n",
      "Epoch 279/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.3353 - mean_absolute_error: 7.1878 - val_loss: 8.9777 - val_mean_absolute_error: 2.2929\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 6.32535\n",
      "Epoch 280/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 73.7514 - mean_absolute_error: 6.6687 - val_loss: 6.7257 - val_mean_absolute_error: 1.9384\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 6.32535\n",
      "Epoch 281/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 77.2311 - mean_absolute_error: 6.8966 - val_loss: 8.4648 - val_mean_absolute_error: 2.2339\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 6.32535\n",
      "Epoch 282/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.6075 - mean_absolute_error: 6.6535 - val_loss: 10.6013 - val_mean_absolute_error: 2.5836\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 6.32535\n",
      "Epoch 283/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.2088 - mean_absolute_error: 6.8894 - val_loss: 12.9766 - val_mean_absolute_error: 2.9576\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 6.32535\n",
      "Epoch 284/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.6369 - mean_absolute_error: 6.8055 - val_loss: 7.6040 - val_mean_absolute_error: 1.9751\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 6.32535\n",
      "Epoch 285/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 94.7570 - mean_absolute_error: 7.7194 - val_loss: 11.6277 - val_mean_absolute_error: 2.7195\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 6.32535\n",
      "Epoch 286/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 66.8411 - mean_absolute_error: 6.5069 - val_loss: 8.1376 - val_mean_absolute_error: 2.1423\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 6.32535\n",
      "Epoch 287/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.9028 - mean_absolute_error: 7.2081 - val_loss: 6.6724 - val_mean_absolute_error: 1.8876\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 6.32535\n",
      "Epoch 288/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.7509 - mean_absolute_error: 7.0749 - val_loss: 8.7329 - val_mean_absolute_error: 2.2916\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 6.32535\n",
      "Epoch 289/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 87.5230 - mean_absolute_error: 7.3825 - val_loss: 12.7334 - val_mean_absolute_error: 2.9723\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 6.32535\n",
      "Epoch 290/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 2ms/step - loss: 77.2313 - mean_absolute_error: 6.9781 - val_loss: 7.3699 - val_mean_absolute_error: 2.1231\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 6.32535\n",
      "Epoch 291/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.7927 - mean_absolute_error: 7.0160 - val_loss: 6.7474 - val_mean_absolute_error: 1.8496\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 6.32535\n",
      "Epoch 292/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.2354 - mean_absolute_error: 7.0470 - val_loss: 10.0625 - val_mean_absolute_error: 2.5253\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 6.32535\n",
      "Epoch 293/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.0865 - mean_absolute_error: 7.0037 - val_loss: 7.2488 - val_mean_absolute_error: 2.0482\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 6.32535\n",
      "Epoch 294/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.0493 - mean_absolute_error: 7.0269 - val_loss: 9.2067 - val_mean_absolute_error: 2.4030\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 6.32535\n",
      "Epoch 295/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.9656 - mean_absolute_error: 6.5951 - val_loss: 6.7639 - val_mean_absolute_error: 1.9575\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 6.32535\n",
      "Epoch 296/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.7958 - mean_absolute_error: 6.7609 - val_loss: 6.6263 - val_mean_absolute_error: 1.9260\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 6.32535\n",
      "Epoch 297/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.6000 - mean_absolute_error: 7.0644 - val_loss: 10.3913 - val_mean_absolute_error: 2.6435\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 6.32535\n",
      "Epoch 298/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.9536 - mean_absolute_error: 6.8513 - val_loss: 8.0780 - val_mean_absolute_error: 2.1270\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 6.32535\n",
      "Epoch 299/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.1391 - mean_absolute_error: 6.7383 - val_loss: 9.2178 - val_mean_absolute_error: 2.3341\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 6.32535\n",
      "Epoch 300/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.5259 - mean_absolute_error: 7.3788 - val_loss: 6.6395 - val_mean_absolute_error: 1.8781\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 6.32535\n",
      "Epoch 301/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.2045 - mean_absolute_error: 6.8151 - val_loss: 7.5041 - val_mean_absolute_error: 2.1201\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 6.32535\n",
      "Epoch 302/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.1630 - mean_absolute_error: 6.8790 - val_loss: 10.5176 - val_mean_absolute_error: 2.6401\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 6.32535\n",
      "Epoch 303/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.5946 - mean_absolute_error: 6.9266 - val_loss: 6.5827 - val_mean_absolute_error: 1.8417\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 6.32535\n",
      "Epoch 304/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.7533 - mean_absolute_error: 6.7969 - val_loss: 8.6958 - val_mean_absolute_error: 2.2924\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 6.32535\n",
      "Epoch 305/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.9795 - mean_absolute_error: 6.9531 - val_loss: 12.5744 - val_mean_absolute_error: 2.9975\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 6.32535\n",
      "Epoch 306/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 91.3585 - mean_absolute_error: 7.5392 - val_loss: 10.3710 - val_mean_absolute_error: 2.6266\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 6.32535\n",
      "Epoch 307/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.8756 - mean_absolute_error: 6.9758 - val_loss: 6.7986 - val_mean_absolute_error: 1.9044\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 6.32535\n",
      "Epoch 308/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.1487 - mean_absolute_error: 6.9123 - val_loss: 8.8729 - val_mean_absolute_error: 2.2777\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 6.32535\n",
      "Epoch 309/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.8104 - mean_absolute_error: 6.6891 - val_loss: 10.1182 - val_mean_absolute_error: 2.5517\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 6.32535\n",
      "Epoch 310/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.6627 - mean_absolute_error: 7.4181 - val_loss: 6.2394 - val_mean_absolute_error: 1.7768\n",
      "\n",
      "Epoch 00310: val_loss improved from 6.32535 to 6.23945, saving model to RNN_best\n",
      "Epoch 311/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.7600 - mean_absolute_error: 7.1478 - val_loss: 8.1626 - val_mean_absolute_error: 2.1689\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 6.23945\n",
      "Epoch 312/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.6293 - mean_absolute_error: 6.6647 - val_loss: 11.3741 - val_mean_absolute_error: 2.8240\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 6.23945\n",
      "Epoch 313/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.7089 - mean_absolute_error: 7.2574 - val_loss: 7.2335 - val_mean_absolute_error: 1.9719\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 6.23945\n",
      "Epoch 314/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.6533 - mean_absolute_error: 6.9116 - val_loss: 7.5160 - val_mean_absolute_error: 1.9964\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 6.23945\n",
      "Epoch 315/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.9615 - mean_absolute_error: 6.6244 - val_loss: 7.3818 - val_mean_absolute_error: 1.9882\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 6.23945\n",
      "Epoch 316/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.5577 - mean_absolute_error: 6.6966 - val_loss: 8.6915 - val_mean_absolute_error: 2.2886\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 6.23945\n",
      "Epoch 317/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.5318 - mean_absolute_error: 6.9173 - val_loss: 9.6550 - val_mean_absolute_error: 2.4984\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 6.23945\n",
      "Epoch 318/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.1783 - mean_absolute_error: 6.6163 - val_loss: 8.2992 - val_mean_absolute_error: 2.3066\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 6.23945\n",
      "Epoch 319/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.9235 - mean_absolute_error: 6.8331 - val_loss: 6.6773 - val_mean_absolute_error: 1.9200\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 6.23945\n",
      "Epoch 320/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.6862 - mean_absolute_error: 6.6460 - val_loss: 7.6741 - val_mean_absolute_error: 2.0935\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 6.23945\n",
      "Epoch 321/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.1938 - mean_absolute_error: 7.0919 - val_loss: 11.8053 - val_mean_absolute_error: 2.7876\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 6.23945\n",
      "Epoch 322/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.2819 - mean_absolute_error: 6.9438 - val_loss: 6.2650 - val_mean_absolute_error: 1.7640\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 6.23945\n",
      "Epoch 323/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 77.1581 - mean_absolute_error: 6.8824 - val_loss: 8.5341 - val_mean_absolute_error: 2.2679\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 6.23945\n",
      "Epoch 324/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.9973 - mean_absolute_error: 6.7930 - val_loss: 9.6087 - val_mean_absolute_error: 2.5109\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 6.23945\n",
      "Epoch 325/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.9856 - mean_absolute_error: 7.1512 - val_loss: 8.0024 - val_mean_absolute_error: 2.0902\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 6.23945\n",
      "Epoch 326/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.8569 - mean_absolute_error: 7.2059 - val_loss: 9.1069 - val_mean_absolute_error: 2.3415\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 6.23945\n",
      "Epoch 327/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 79.8840 - mean_absolute_error: 6.9294 - val_loss: 7.0756 - val_mean_absolute_error: 1.9596\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 6.23945\n",
      "Epoch 328/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.8572 - mean_absolute_error: 7.0680 - val_loss: 7.5670 - val_mean_absolute_error: 2.0723\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 6.23945\n",
      "Epoch 329/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 95.0077 - mean_absolute_error: 7.6465 - val_loss: 6.4825 - val_mean_absolute_error: 1.9062\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 6.23945\n",
      "Epoch 330/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.6150 - mean_absolute_error: 6.9029 - val_loss: 6.9302 - val_mean_absolute_error: 1.8908\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 6.23945\n",
      "Epoch 331/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 66.8682 - mean_absolute_error: 6.4857 - val_loss: 8.7712 - val_mean_absolute_error: 2.2619\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 6.23945\n",
      "Epoch 332/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.2000 - mean_absolute_error: 6.6583 - val_loss: 7.0993 - val_mean_absolute_error: 1.9125\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 6.23945\n",
      "Epoch 333/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.2256 - mean_absolute_error: 6.8496 - val_loss: 7.3889 - val_mean_absolute_error: 2.0702\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 6.23945\n",
      "Epoch 334/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.6774 - mean_absolute_error: 6.5344 - val_loss: 9.0522 - val_mean_absolute_error: 2.3656\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 6.23945\n",
      "Epoch 335/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.6272 - mean_absolute_error: 6.5970 - val_loss: 6.9547 - val_mean_absolute_error: 1.9257\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 6.23945\n",
      "Epoch 336/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.5399 - mean_absolute_error: 6.9597 - val_loss: 12.4933 - val_mean_absolute_error: 2.9175\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 6.23945\n",
      "Epoch 337/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.7179 - mean_absolute_error: 6.6373 - val_loss: 8.8741 - val_mean_absolute_error: 2.2993\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 6.23945\n",
      "Epoch 338/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.7633 - mean_absolute_error: 6.9948 - val_loss: 6.9361 - val_mean_absolute_error: 1.9145\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 6.23945\n",
      "Epoch 339/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.9124 - mean_absolute_error: 6.9482 - val_loss: 6.8125 - val_mean_absolute_error: 1.9265\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 6.23945\n",
      "Epoch 340/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.2859 - mean_absolute_error: 6.6598 - val_loss: 8.0420 - val_mean_absolute_error: 2.1127\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 6.23945\n",
      "Epoch 341/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.6263 - mean_absolute_error: 6.8865 - val_loss: 8.8379 - val_mean_absolute_error: 2.2875\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 6.23945\n",
      "Epoch 342/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.9530 - mean_absolute_error: 7.0695 - val_loss: 6.7364 - val_mean_absolute_error: 1.9029\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 6.23945\n",
      "Epoch 343/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.1665 - mean_absolute_error: 6.6625 - val_loss: 9.9614 - val_mean_absolute_error: 2.5180\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 6.23945\n",
      "Epoch 344/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 85.3322 - mean_absolute_error: 7.3591 - val_loss: 8.8405 - val_mean_absolute_error: 2.2580\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 6.23945\n",
      "Epoch 345/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 86.2546 - mean_absolute_error: 7.3445 - val_loss: 9.2130 - val_mean_absolute_error: 2.3973\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 6.23945\n",
      "Epoch 346/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.2098 - mean_absolute_error: 6.7171 - val_loss: 11.0610 - val_mean_absolute_error: 2.6772\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 6.23945\n",
      "Epoch 347/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.7799 - mean_absolute_error: 6.8279 - val_loss: 11.4145 - val_mean_absolute_error: 2.7711\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 6.23945\n",
      "Epoch 348/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.0371 - mean_absolute_error: 6.9646 - val_loss: 6.2244 - val_mean_absolute_error: 1.8669\n",
      "\n",
      "Epoch 00348: val_loss improved from 6.23945 to 6.22439, saving model to RNN_best\n",
      "Epoch 349/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.7565 - mean_absolute_error: 6.6601 - val_loss: 6.7622 - val_mean_absolute_error: 2.0215\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 6.22439\n",
      "Epoch 350/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.2899 - mean_absolute_error: 6.4804 - val_loss: 8.4034 - val_mean_absolute_error: 2.2588\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 6.22439\n",
      "Epoch 351/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.1238 - mean_absolute_error: 6.6579 - val_loss: 5.9152 - val_mean_absolute_error: 1.7967\n",
      "\n",
      "Epoch 00351: val_loss improved from 6.22439 to 5.91518, saving model to RNN_best\n",
      "Epoch 352/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.8211 - mean_absolute_error: 6.9615 - val_loss: 7.0557 - val_mean_absolute_error: 2.0257\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 5.91518\n",
      "Epoch 353/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.0790 - mean_absolute_error: 6.9142 - val_loss: 10.8582 - val_mean_absolute_error: 2.6629\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 5.91518\n",
      "Epoch 354/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.7696 - mean_absolute_error: 6.8469 - val_loss: 6.8020 - val_mean_absolute_error: 1.9042\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 5.91518\n",
      "Epoch 355/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 76.0316 - mean_absolute_error: 6.8418 - val_loss: 7.4827 - val_mean_absolute_error: 2.0271\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 5.91518\n",
      "Epoch 356/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.5057 - mean_absolute_error: 7.1840 - val_loss: 9.2962 - val_mean_absolute_error: 2.3934\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 5.91518\n",
      "Epoch 357/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.3387 - mean_absolute_error: 6.6344 - val_loss: 6.3209 - val_mean_absolute_error: 1.7180\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 5.91518\n",
      "Epoch 358/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.9092 - mean_absolute_error: 6.8202 - val_loss: 6.6382 - val_mean_absolute_error: 1.8419\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 5.91518\n",
      "Epoch 359/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.9387 - mean_absolute_error: 6.8871 - val_loss: 8.2424 - val_mean_absolute_error: 2.2571\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 5.91518\n",
      "Epoch 360/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.6229 - mean_absolute_error: 7.2252 - val_loss: 6.4516 - val_mean_absolute_error: 1.8335\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 5.91518\n",
      "Epoch 361/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.1007 - mean_absolute_error: 6.7877 - val_loss: 6.0884 - val_mean_absolute_error: 1.7655\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 5.91518\n",
      "Epoch 362/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 68.0201 - mean_absolute_error: 6.5746 - val_loss: 8.7991 - val_mean_absolute_error: 2.2932\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 5.91518\n",
      "Epoch 363/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.3409 - mean_absolute_error: 6.9572 - val_loss: 7.0410 - val_mean_absolute_error: 1.9071\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 5.91518\n",
      "Epoch 364/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 76.7445 - mean_absolute_error: 6.9056 - val_loss: 7.1803 - val_mean_absolute_error: 1.9257\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 5.91518\n",
      "Epoch 365/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.1282 - mean_absolute_error: 7.1528 - val_loss: 9.8750 - val_mean_absolute_error: 2.4600\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 5.91518\n",
      "Epoch 366/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.4928 - mean_absolute_error: 7.2517 - val_loss: 6.2818 - val_mean_absolute_error: 1.8377\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 5.91518\n",
      "Epoch 367/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.7328 - mean_absolute_error: 6.5987 - val_loss: 7.8276 - val_mean_absolute_error: 2.1038\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 5.91518\n",
      "Epoch 368/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.3544 - mean_absolute_error: 6.7977 - val_loss: 6.5239 - val_mean_absolute_error: 1.9442\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 5.91518\n",
      "Epoch 369/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.7801 - mean_absolute_error: 6.7687 - val_loss: 10.9759 - val_mean_absolute_error: 2.7004\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 5.91518\n",
      "Epoch 370/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.8290 - mean_absolute_error: 7.2497 - val_loss: 7.9190 - val_mean_absolute_error: 2.2193\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 5.91518\n",
      "Epoch 371/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.2424 - mean_absolute_error: 6.9195 - val_loss: 11.9979 - val_mean_absolute_error: 2.9209\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 5.91518\n",
      "Epoch 372/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.9674 - mean_absolute_error: 6.8758 - val_loss: 7.0745 - val_mean_absolute_error: 1.9789\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 5.91518\n",
      "Epoch 373/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.4897 - mean_absolute_error: 7.0739 - val_loss: 7.3473 - val_mean_absolute_error: 1.9804\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 5.91518\n",
      "Epoch 374/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.2589 - mean_absolute_error: 7.1372 - val_loss: 6.5496 - val_mean_absolute_error: 1.8772\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 5.91518\n",
      "Epoch 375/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 67.5792 - mean_absolute_error: 6.5201 - val_loss: 7.1390 - val_mean_absolute_error: 1.9904\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 5.91518\n",
      "Epoch 376/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 83.3566 - mean_absolute_error: 7.2468 - val_loss: 9.6683 - val_mean_absolute_error: 2.5166\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 5.91518\n",
      "Epoch 377/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 79.4877 - mean_absolute_error: 7.0332 - val_loss: 7.3983 - val_mean_absolute_error: 2.1127\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 5.91518\n",
      "Epoch 378/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.7883 - mean_absolute_error: 6.8385 - val_loss: 7.0389 - val_mean_absolute_error: 2.0708\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 5.91518\n",
      "Epoch 379/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.9312 - mean_absolute_error: 6.8786 - val_loss: 12.9157 - val_mean_absolute_error: 3.0010\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 5.91518\n",
      "Epoch 380/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.5955 - mean_absolute_error: 7.0504 - val_loss: 5.2433 - val_mean_absolute_error: 1.7032\n",
      "\n",
      "Epoch 00380: val_loss improved from 5.91518 to 5.24330, saving model to RNN_best\n",
      "Epoch 381/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.1147 - mean_absolute_error: 6.7839 - val_loss: 5.2304 - val_mean_absolute_error: 1.7065\n",
      "\n",
      "Epoch 00381: val_loss improved from 5.24330 to 5.23042, saving model to RNN_best\n",
      "Epoch 382/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.3778 - mean_absolute_error: 6.5480 - val_loss: 5.3006 - val_mean_absolute_error: 1.6761\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 5.23042\n",
      "Epoch 383/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.5636 - mean_absolute_error: 7.0746 - val_loss: 6.5283 - val_mean_absolute_error: 1.8853\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 5.23042\n",
      "Epoch 384/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.5608 - mean_absolute_error: 6.8181 - val_loss: 7.3144 - val_mean_absolute_error: 2.0827\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 5.23042\n",
      "Epoch 385/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.0259 - mean_absolute_error: 6.7315 - val_loss: 6.9261 - val_mean_absolute_error: 2.0216\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 5.23042\n",
      "Epoch 386/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.0965 - mean_absolute_error: 6.6882 - val_loss: 6.8724 - val_mean_absolute_error: 1.9819\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 5.23042\n",
      "Epoch 387/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.1369 - mean_absolute_error: 6.9572 - val_loss: 10.9679 - val_mean_absolute_error: 2.7637\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 5.23042\n",
      "Epoch 388/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.9365 - mean_absolute_error: 6.7052 - val_loss: 7.4093 - val_mean_absolute_error: 2.0663\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 5.23042\n",
      "Epoch 389/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.3728 - mean_absolute_error: 6.7874 - val_loss: 9.4295 - val_mean_absolute_error: 2.4537\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 5.23042\n",
      "Epoch 390/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.4848 - mean_absolute_error: 6.4625 - val_loss: 9.8438 - val_mean_absolute_error: 2.5554\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 5.23042\n",
      "Epoch 391/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.2512 - mean_absolute_error: 6.6980 - val_loss: 5.6426 - val_mean_absolute_error: 1.7332\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 5.23042\n",
      "Epoch 392/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.2616 - mean_absolute_error: 6.7869 - val_loss: 7.0711 - val_mean_absolute_error: 2.0818\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 5.23042\n",
      "Epoch 393/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.8686 - mean_absolute_error: 6.8376 - val_loss: 5.7966 - val_mean_absolute_error: 1.7969\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 5.23042\n",
      "Epoch 394/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.9378 - mean_absolute_error: 7.1913 - val_loss: 5.6997 - val_mean_absolute_error: 1.8163\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 5.23042\n",
      "Epoch 395/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 81.4693 - mean_absolute_error: 7.1234 - val_loss: 9.2694 - val_mean_absolute_error: 2.4190\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 5.23042\n",
      "Epoch 396/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 77.1512 - mean_absolute_error: 6.9471 - val_loss: 8.1307 - val_mean_absolute_error: 2.2536\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 5.23042\n",
      "Epoch 397/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.5107 - mean_absolute_error: 6.8648 - val_loss: 7.2833 - val_mean_absolute_error: 2.0499\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 5.23042\n",
      "Epoch 398/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 83.0279 - mean_absolute_error: 7.1710 - val_loss: 6.3814 - val_mean_absolute_error: 1.7768\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 5.23042\n",
      "Epoch 399/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.3863 - mean_absolute_error: 6.7797 - val_loss: 7.6379 - val_mean_absolute_error: 2.1311\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 5.23042\n",
      "Epoch 400/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.8172 - mean_absolute_error: 6.6305 - val_loss: 9.4045 - val_mean_absolute_error: 2.4455\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 5.23042\n",
      "Epoch 401/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 79.6787 - mean_absolute_error: 7.0539 - val_loss: 9.5157 - val_mean_absolute_error: 2.5123\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 5.23042\n",
      "Epoch 402/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.5614 - mean_absolute_error: 6.7424 - val_loss: 11.3921 - val_mean_absolute_error: 2.8562\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 5.23042\n",
      "Epoch 403/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.2247 - mean_absolute_error: 6.5603 - val_loss: 8.3632 - val_mean_absolute_error: 2.3359\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 5.23042\n",
      "Epoch 404/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.7936 - mean_absolute_error: 6.8475 - val_loss: 7.6674 - val_mean_absolute_error: 2.2011\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 5.23042\n",
      "Epoch 405/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 84.2130 - mean_absolute_error: 7.4471 - val_loss: 7.9028 - val_mean_absolute_error: 2.2116\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 5.23042\n",
      "Epoch 406/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.4266 - mean_absolute_error: 6.6985 - val_loss: 8.3660 - val_mean_absolute_error: 2.2249\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 5.23042\n",
      "Epoch 407/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.8350 - mean_absolute_error: 7.0176 - val_loss: 5.7214 - val_mean_absolute_error: 1.7465\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 5.23042\n",
      "Epoch 408/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.2901 - mean_absolute_error: 6.8994 - val_loss: 7.7015 - val_mean_absolute_error: 2.1561\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 5.23042\n",
      "Epoch 409/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.8696 - mean_absolute_error: 7.0781 - val_loss: 5.8643 - val_mean_absolute_error: 1.7511\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 5.23042\n",
      "Epoch 410/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.5005 - mean_absolute_error: 7.0137 - val_loss: 7.8582 - val_mean_absolute_error: 2.1026\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 5.23042\n",
      "Epoch 411/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 80.2452 - mean_absolute_error: 6.9984 - val_loss: 7.3589 - val_mean_absolute_error: 2.0483\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 5.23042\n",
      "Epoch 412/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.6748 - mean_absolute_error: 6.8919 - val_loss: 5.9741 - val_mean_absolute_error: 1.7961\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 5.23042\n",
      "Epoch 413/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.6613 - mean_absolute_error: 6.7082 - val_loss: 9.9334 - val_mean_absolute_error: 2.5537\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 5.23042\n",
      "Epoch 414/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.5223 - mean_absolute_error: 6.7819 - val_loss: 7.2770 - val_mean_absolute_error: 2.0725\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 5.23042\n",
      "Epoch 415/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.0076 - mean_absolute_error: 6.8942 - val_loss: 7.1668 - val_mean_absolute_error: 2.0289\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 5.23042\n",
      "Epoch 416/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.2058 - mean_absolute_error: 6.4938 - val_loss: 7.9290 - val_mean_absolute_error: 2.1565\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 5.23042\n",
      "Epoch 417/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.2198 - mean_absolute_error: 6.7800 - val_loss: 7.9026 - val_mean_absolute_error: 2.1735\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 5.23042\n",
      "Epoch 418/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.3263 - mean_absolute_error: 7.0908 - val_loss: 6.1421 - val_mean_absolute_error: 1.8425\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 5.23042\n",
      "Epoch 419/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 77.5411 - mean_absolute_error: 6.8210 - val_loss: 7.0559 - val_mean_absolute_error: 1.9652\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 5.23042\n",
      "Epoch 420/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.9486 - mean_absolute_error: 6.6695 - val_loss: 14.5480 - val_mean_absolute_error: 3.2293\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 5.23042\n",
      "Epoch 421/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.2276 - mean_absolute_error: 7.1212 - val_loss: 8.9684 - val_mean_absolute_error: 2.3794\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 5.23042\n",
      "Epoch 422/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.9548 - mean_absolute_error: 6.8298 - val_loss: 7.1745 - val_mean_absolute_error: 2.0246\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 5.23042\n",
      "Epoch 423/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.7045 - mean_absolute_error: 6.6048 - val_loss: 10.3875 - val_mean_absolute_error: 2.5259\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 5.23042\n",
      "Epoch 424/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 79.8685 - mean_absolute_error: 7.0248 - val_loss: 6.0168 - val_mean_absolute_error: 1.8157\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 5.23042\n",
      "Epoch 425/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.5403 - mean_absolute_error: 6.7865 - val_loss: 5.6405 - val_mean_absolute_error: 1.7907\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 5.23042\n",
      "Epoch 426/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.7164 - mean_absolute_error: 6.7105 - val_loss: 7.6722 - val_mean_absolute_error: 2.1557\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 5.23042\n",
      "Epoch 427/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 86.3593 - mean_absolute_error: 7.3559 - val_loss: 6.9535 - val_mean_absolute_error: 1.9796\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 5.23042\n",
      "Epoch 428/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.1999 - mean_absolute_error: 6.6347 - val_loss: 11.2744 - val_mean_absolute_error: 2.8157\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 5.23042\n",
      "Epoch 429/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.5910 - mean_absolute_error: 6.7564 - val_loss: 6.5934 - val_mean_absolute_error: 1.9201\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 5.23042\n",
      "Epoch 430/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.8003 - mean_absolute_error: 6.6656 - val_loss: 9.8357 - val_mean_absolute_error: 2.4546\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 5.23042\n",
      "Epoch 431/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.5644 - mean_absolute_error: 6.9620 - val_loss: 6.0601 - val_mean_absolute_error: 1.7543\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 5.23042\n",
      "Epoch 432/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.4373 - mean_absolute_error: 6.8793 - val_loss: 11.1390 - val_mean_absolute_error: 2.7341\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 5.23042\n",
      "Epoch 433/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 81.5245 - mean_absolute_error: 7.1425 - val_loss: 9.0779 - val_mean_absolute_error: 2.3935\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 5.23042\n",
      "Epoch 434/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.7519 - mean_absolute_error: 6.6224 - val_loss: 6.0722 - val_mean_absolute_error: 1.8331\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 5.23042\n",
      "Epoch 435/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.5150 - mean_absolute_error: 6.6647 - val_loss: 8.5259 - val_mean_absolute_error: 2.2950\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 5.23042\n",
      "Epoch 436/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.6942 - mean_absolute_error: 6.7554 - val_loss: 7.9592 - val_mean_absolute_error: 2.1879\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 5.23042\n",
      "Epoch 437/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.2932 - mean_absolute_error: 6.8936 - val_loss: 6.5930 - val_mean_absolute_error: 1.9498\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 5.23042\n",
      "Epoch 438/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.7934 - mean_absolute_error: 6.7762 - val_loss: 6.3577 - val_mean_absolute_error: 1.8664\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 5.23042\n",
      "Epoch 439/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.5303 - mean_absolute_error: 6.9534 - val_loss: 9.2339 - val_mean_absolute_error: 2.4406\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 5.23042\n",
      "Epoch 440/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 78.2544 - mean_absolute_error: 6.9662 - val_loss: 8.6064 - val_mean_absolute_error: 2.3220\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 5.23042\n",
      "Epoch 441/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.5995 - mean_absolute_error: 6.9399 - val_loss: 6.4026 - val_mean_absolute_error: 1.8228\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 5.23042\n",
      "Epoch 442/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.0894 - mean_absolute_error: 6.8391 - val_loss: 8.0427 - val_mean_absolute_error: 2.1683\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 5.23042\n",
      "Epoch 443/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 72.3122 - mean_absolute_error: 6.7172 - val_loss: 5.8538 - val_mean_absolute_error: 1.7622\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 5.23042\n",
      "Epoch 444/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.2964 - mean_absolute_error: 7.2575 - val_loss: 16.5494 - val_mean_absolute_error: 3.5546\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 5.23042\n",
      "Epoch 445/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 84.3124 - mean_absolute_error: 7.1554 - val_loss: 7.4276 - val_mean_absolute_error: 2.1078\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 5.23042\n",
      "Epoch 446/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.1761 - mean_absolute_error: 6.8042 - val_loss: 5.6982 - val_mean_absolute_error: 1.7805\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 5.23042\n",
      "Epoch 447/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.8144 - mean_absolute_error: 6.6455 - val_loss: 7.3270 - val_mean_absolute_error: 2.0860\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 5.23042\n",
      "Epoch 448/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 74.1341 - mean_absolute_error: 6.6426 - val_loss: 7.9705 - val_mean_absolute_error: 2.2080\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 5.23042\n",
      "Epoch 449/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.0758 - mean_absolute_error: 6.4076 - val_loss: 8.2069 - val_mean_absolute_error: 2.2406\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 5.23042\n",
      "Epoch 450/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 80.4299 - mean_absolute_error: 7.0335 - val_loss: 5.7956 - val_mean_absolute_error: 1.8029\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 5.23042\n",
      "Epoch 451/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.4457 - mean_absolute_error: 6.7086 - val_loss: 6.6140 - val_mean_absolute_error: 1.8872\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 5.23042\n",
      "Epoch 452/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.1551 - mean_absolute_error: 6.8682 - val_loss: 9.2183 - val_mean_absolute_error: 2.3620\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 5.23042\n",
      "Epoch 453/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 77.3214 - mean_absolute_error: 6.9605 - val_loss: 7.7441 - val_mean_absolute_error: 2.1670\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 5.23042\n",
      "Epoch 454/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 76.8572 - mean_absolute_error: 6.8752 - val_loss: 6.0116 - val_mean_absolute_error: 1.7778\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 5.23042\n",
      "Epoch 455/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.6274 - mean_absolute_error: 6.7066 - val_loss: 6.8782 - val_mean_absolute_error: 1.9459\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 5.23042\n",
      "Epoch 456/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.5048 - mean_absolute_error: 6.6711 - val_loss: 6.0696 - val_mean_absolute_error: 1.8239\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 5.23042\n",
      "Epoch 457/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.0296 - mean_absolute_error: 6.6626 - val_loss: 6.3625 - val_mean_absolute_error: 1.8969\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 5.23042\n",
      "Epoch 458/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 73.0253 - mean_absolute_error: 6.6090 - val_loss: 6.2819 - val_mean_absolute_error: 1.8432\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 5.23042\n",
      "Epoch 459/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.6912 - mean_absolute_error: 7.1767 - val_loss: 7.2682 - val_mean_absolute_error: 2.0405\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 5.23042\n",
      "Epoch 460/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.3455 - mean_absolute_error: 7.0096 - val_loss: 12.3737 - val_mean_absolute_error: 2.9012\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 5.23042\n",
      "Epoch 461/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 65.6875 - mean_absolute_error: 6.4055 - val_loss: 5.4740 - val_mean_absolute_error: 1.7338\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 5.23042\n",
      "Epoch 462/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.2226 - mean_absolute_error: 6.9501 - val_loss: 8.1233 - val_mean_absolute_error: 2.2712\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 5.23042\n",
      "Epoch 463/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.3827 - mean_absolute_error: 6.8334 - val_loss: 6.1874 - val_mean_absolute_error: 1.8639\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 5.23042\n",
      "Epoch 464/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.5105 - mean_absolute_error: 6.8038 - val_loss: 6.3513 - val_mean_absolute_error: 1.9182\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 5.23042\n",
      "Epoch 465/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 74.9067 - mean_absolute_error: 6.8834 - val_loss: 5.4585 - val_mean_absolute_error: 1.7624\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 5.23042\n",
      "Epoch 466/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 66.3268 - mean_absolute_error: 6.3553 - val_loss: 6.6894 - val_mean_absolute_error: 2.0310\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 5.23042\n",
      "Epoch 467/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 69.9348 - mean_absolute_error: 6.4992 - val_loss: 7.3328 - val_mean_absolute_error: 2.1086\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 5.23042\n",
      "Epoch 468/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 72.9214 - mean_absolute_error: 6.6830 - val_loss: 7.7832 - val_mean_absolute_error: 2.1388\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 5.23042\n",
      "Epoch 469/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 75.9924 - mean_absolute_error: 7.0051 - val_loss: 6.4363 - val_mean_absolute_error: 1.8655\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 5.23042\n",
      "Epoch 470/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 71.0397 - mean_absolute_error: 6.5419 - val_loss: 7.3378 - val_mean_absolute_error: 1.9857\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 5.23042\n",
      "Epoch 471/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.6315 - mean_absolute_error: 7.0312 - val_loss: 6.9414 - val_mean_absolute_error: 1.9928\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 5.23042\n",
      "Epoch 472/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 82.3020 - mean_absolute_error: 7.0699 - val_loss: 11.7108 - val_mean_absolute_error: 2.8596\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 5.23042\n",
      "Epoch 473/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 70.8946 - mean_absolute_error: 6.5637 - val_loss: 6.8929 - val_mean_absolute_error: 2.0577\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 5.23042\n",
      "Epoch 474/1000\n",
      "522/522 [==============================] - 1s 1ms/step - loss: 67.9881 - mean_absolute_error: 6.4524 - val_loss: 6.2138 - val_mean_absolute_error: 1.8373\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 5.23042\n",
      "Epoch 475/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 1s 1ms/step - loss: 71.4000 - mean_absolute_error: 6.6299 - val_loss: 6.7014 - val_mean_absolute_error: 1.9128\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 5.23042\n",
      "Epoch 476/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 75.1251 - mean_absolute_error: 6.8159 - val_loss: 7.3790 - val_mean_absolute_error: 2.0909\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 5.23042\n",
      "Epoch 477/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 78.5064 - mean_absolute_error: 6.8907 - val_loss: 7.5792 - val_mean_absolute_error: 2.1823\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 5.23042\n",
      "Epoch 478/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 70.0527 - mean_absolute_error: 6.5190 - val_loss: 10.2366 - val_mean_absolute_error: 2.6364\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 5.23042\n",
      "Epoch 479/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 71.4145 - mean_absolute_error: 6.5818 - val_loss: 6.4409 - val_mean_absolute_error: 1.8225\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 5.23042\n",
      "Epoch 480/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 69.4140 - mean_absolute_error: 6.6275 - val_loss: 7.7387 - val_mean_absolute_error: 2.0799\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 5.23042\n",
      "Epoch 481/1000\n",
      "522/522 [==============================] - 1s 2ms/step - loss: 82.3939 - mean_absolute_error: 7.0513 - val_loss: 6.5271 - val_mean_absolute_error: 1.9401\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 5.23042\n",
      "Epoch 00481: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_GCA = lstm_baseline(X_train, y_train)\n",
    "outputs = [layer.output for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.590385</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.393562</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.214680</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.294174</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.389214</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64.956833</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>74.759544</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66.081177</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73.022781</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74.578262</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>68.184662</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70.380142</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40.550026</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>74.398163</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>63.530014</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>77.854874</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>52.647858</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>88.022461</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>81.522095</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>59.738972</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>81.161957</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.634178</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>59.040577</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>72.739540</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>82.929306</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>77.616585</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>61.766689</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>82.212044</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76.694023</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>67.717857</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>86.609680</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>53.003258</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>62.063362</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>79.675079</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>69.251228</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>70.203506</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>67.008408</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>71.596466</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>55.144680</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>64.288124</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>76.015755</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>83.812271</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>67.826500</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>65.633232</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>51.544281</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>80.010506</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>55.649071</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>66.573730</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>67.846748</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>64.514946</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>67.825058</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>70.416443</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>55.916389</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>68.874481</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>81.817627</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>85.378616</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>72.073898</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>92.515656</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>53.301689</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>64.084663</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted  actual\n",
       "0    80.590385      81\n",
       "1    88.393562      84\n",
       "2    52.214680      50\n",
       "3    63.294174      61\n",
       "4    70.389214      73\n",
       "5    64.956833      65\n",
       "6    74.759544      76\n",
       "7    66.081177      65\n",
       "8    73.022781      74\n",
       "9    74.578262      75\n",
       "10   68.184662      66\n",
       "11   70.380142      71\n",
       "12   40.550026      28\n",
       "13   74.398163      79\n",
       "14   63.530014      60\n",
       "15   77.854874      79\n",
       "16   52.647858      54\n",
       "17   88.022461      84\n",
       "18   81.522095      83\n",
       "19   59.738972      60\n",
       "20   81.161957      85\n",
       "21   50.634178      44\n",
       "22   59.040577      55\n",
       "23   72.739540      75\n",
       "24   82.929306      83\n",
       "25   77.616585      78\n",
       "26   61.766689      64\n",
       "27   82.212044      81\n",
       "28   76.694023      77\n",
       "29   67.717857      72\n",
       "..         ...     ...\n",
       "134  86.609680      86\n",
       "135  53.003258      55\n",
       "136  62.063362      64\n",
       "137  79.675079      82\n",
       "138  69.251228      69\n",
       "139  70.203506      76\n",
       "140  67.008408      68\n",
       "141  71.596466      68\n",
       "142  55.144680      56\n",
       "143  64.288124      66\n",
       "144  76.015755      75\n",
       "145  83.812271      83\n",
       "146  67.826500      69\n",
       "147  65.633232      68\n",
       "148  51.544281      52\n",
       "149  80.010506      78\n",
       "150  55.649071      55\n",
       "151  66.573730      69\n",
       "152  67.846748      70\n",
       "153  64.514946      61\n",
       "154  67.825058      69\n",
       "155  70.416443      73\n",
       "156  55.916389      52\n",
       "157  68.874481      72\n",
       "158  81.817627      80\n",
       "159  85.378616      80\n",
       "160  72.073898      71\n",
       "161  92.515656      92\n",
       "162  53.301689      55\n",
       "163  64.084663      65\n",
       "\n",
       "[164 rows x 2 columns]"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_GCA.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred = y_pred.tolist()\n",
    "dictionary_DF = {'predicted':y_pred, 'actual':y_test}\n",
    "y_GCA_DF = pd.DataFrame(dictionary_DF)\n",
    "y_GCA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    164.000000\n",
       "mean       2.369138\n",
       "std        3.398933\n",
       "min        0.001236\n",
       "25%        0.692579\n",
       "50%        1.495090\n",
       "75%        2.637930\n",
       "max       32.579651\n",
       "dtype: float64"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_GCA_DF\n",
    "difference_GCA = pd.Series(abs(y_test-y_pred))\n",
    "difference_GCA.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
